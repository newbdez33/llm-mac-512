# OpenClaw æœ¬åœ° API é…ç½®æŒ‡å—

> ä½¿ç”¨æœ¬åœ° LLM ä¸º OpenClaw æä¾› AI èƒ½åŠ›

## ğŸ“‹ ç›®å½•

- [æ¦‚è¿°](#æ¦‚è¿°)
- [æ–¹å¼é€‰æ‹©](#æ–¹å¼é€‰æ‹©)
- [æ–¹å¼ 1: LM Studio (æ¨è)](#æ–¹å¼-1-lm-studio-æ¨è)
- [æ–¹å¼ 2: MLX (å‘½ä»¤è¡Œ)](#æ–¹å¼-2-mlx-å‘½ä»¤è¡Œ)
- [æµ‹è¯•éªŒè¯](#æµ‹è¯•éªŒè¯)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

---

## æ¦‚è¿°

æœ¬æŒ‡å—æä¾›ä¸¤ç§æ–¹å¼ä¸º OpenClaw é…ç½®æœ¬åœ° LLM APIï¼š

1. **LM Studio** (æ¨è): GUI + CLIï¼Œå¼€ç®±å³ç”¨
2. **MLX**: å‘½ä»¤è¡Œæ–¹å¼ï¼Œæ›´å¤šæ§åˆ¶

**æ¶æ„ï¼š**
```
OpenClaw â†’ æœ¬åœ° API æœåŠ¡å™¨ â†’ LLM (MiniMax M2.1)
```

---

## æ–¹å¼é€‰æ‹©

| ç‰¹æ€§ | LM Studio | MLX |
|------|----------|-----|
| **å®‰è£…éš¾åº¦** | â­ ç®€å• (GUI) | â­â­ ä¸­ç­‰ (å‘½ä»¤è¡Œ) |
| **ä½¿ç”¨æ–¹å¼** | GUI + CLI | ä»…å‘½ä»¤è¡Œ |
| **æ€§èƒ½** | ğŸš€ ä¼˜ç§€ | ğŸš€ ä¼˜ç§€ |
| **çµæ´»æ€§** | â­â­â­ | â­â­â­â­ |
| **æ¨èç”¨æˆ·** | æ‰€æœ‰ç”¨æˆ· | å¼€å‘è€…/é«˜çº§ç”¨æˆ· |

**æ¨è:** é™¤éä½ éœ€è¦é«˜åº¦è‡ªå®šä¹‰ï¼Œå¦åˆ™é€‰æ‹© LM Studioã€‚

---

## æ–¹å¼ 1: LM Studio (æ¨è)

### å‰ç½®è¦æ±‚

- LM Studio å·²å®‰è£…ï¼ˆå¦‚æœªå®‰è£…ï¼Œå‚è€ƒä¸‹æ–¹ï¼‰

### å¿«é€Ÿå¼€å§‹ (3 æ­¥)

```bash
# 1. å®‰è£… LM Studio (å¦‚æœæœªå®‰è£…)
brew install --cask lm-studio

# 2. ä¸‹è½½æ¨¡å‹
lms download mlx-community/MiniMax-M2.1-4bit

# 3. å¯åŠ¨ API æœåŠ¡å™¨
lms server start mlx-community/MiniMax-M2.1-4bit --port 1234
```

**å®Œæˆï¼** API è¿è¡Œåœ¨ `http://localhost:1234`

### é…ç½® OpenClaw

**æ–¹æ³• A: ç¯å¢ƒå˜é‡ (æœ€ç®€å•)**

```bash
export OPENAI_API_BASE="http://localhost:1234/v1"
export OPENAI_API_KEY="lm-studio"

# æ·»åŠ åˆ° ~/.bashrc æˆ– ~/.zshrc æ°¸ä¹…ç”Ÿæ•ˆ
echo 'export OPENAI_API_BASE="http://localhost:1234/v1"' >> ~/.zshrc
echo 'export OPENAI_API_KEY="lm-studio"' >> ~/.zshrc
```

**æ–¹æ³• B: OpenClaw é…ç½®æ–‡ä»¶ (æ¨è)**

ç¼–è¾‘ `~/.openclaw/openclaw.json`:

```json
{
  "models": {
    "providers": {
      "lmstudio": {
        "baseUrl": "http://localhost:1234/v1",
        "api": "openai-completions",
        "apiKey": "lm-studio",
        "models": [
          {
            "id": "minimax-m2.1",
            "name": "MiniMax M2.1 (Local)",
            "reasoning": true,
            "contextWindow": 200000,
            "maxTokens": 8192
          }
        ]
      }
    }
  },
  "agents": {
    "defaults": {
      "model": {
        "primary": "lmstudio/minimax-m2.1"
      }
    }
  }
}
```

**æ–¹æ³• C: config.yaml**

ç¼–è¾‘ `~/.openclaw/config.yaml`:

```yaml
llm:
  provider: openai
  base_url: http://localhost:1234/v1
  api_key: lm-studio
  model: minimax-m2.1
  temperature: 0.7
  max_tokens: 4000
```

### é‡å¯ OpenClaw

```bash
# é‡å¯ gateway
openclaw gateway restart

# æˆ–é‡å¯æ•´ä¸ªæœåŠ¡
openclaw restart
```

### ç®¡ç†æœåŠ¡å™¨

```bash
# æŸ¥çœ‹çŠ¶æ€
lms server status

# æŸ¥çœ‹æ—¥å¿—
lms server logs

# åœæ­¢æœåŠ¡å™¨
lms server stop

# åå°è¿è¡Œ
lms server start mlx-community/MiniMax-M2.1-4bit --detach
```

---

## æ–¹å¼ 2: MLX (å‘½ä»¤è¡Œ)

### å‰ç½®è¦æ±‚

```bash
# 1. æ£€æŸ¥ Python ç¯å¢ƒ
python3 --version  # éœ€è¦ 3.12+

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
cd ~/projects/llm-mac-512
python3 -m venv venv
source venv/bin/activate

# 3. å®‰è£…ä¾èµ–
pip install -U mlx-lm flask flask-cors psutil
```

å®Œæ•´å®‰è£…æŒ‡å—: [docs/mlx-local-setup.md](./mlx-local-setup.md)

### å¿«é€Ÿå¼€å§‹

#### ç¬¬1æ­¥ï¼šå¯åŠ¨APIæœåŠ¡å™¨

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd ~/projects/llm-mac-512
source venv/bin/activate

# å¯åŠ¨APIæœåŠ¡å™¨ï¼ˆä½¿ç”¨4-bitæ¨¡å‹ï¼Œæœ€å¿«ï¼‰
python scripts/api_server.py --model mlx-community/MiniMax-M2.1-4bit --port 8000
```

**é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡å‹ï¼ˆ~120GBï¼‰ï¼Œè¯·è€å¿ƒç­‰å¾…ï¼**

æœåŠ¡å™¨å¯åŠ¨åä¼šæ˜¾ç¤ºï¼š
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         MLX MiniMax M2.1 API Server                      â•‘
â•‘         OpenAI-Compatible API                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ“ æ¨¡å‹åŠ è½½å®Œæˆï¼ç”¨æ—¶ 21.25 ç§’

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
API æœåŠ¡å™¨é…ç½®
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
æ¨¡å‹: mlx-community/MiniMax-M2.1-4bit
åœ°å€: http://127.0.0.1:8000
ç«¯ç‚¹:
  â€¢ Chat: http://127.0.0.1:8000/v1/chat/completions
  â€¢ Completions: http://127.0.0.1:8000/v1/completions
  â€¢ Models: http://127.0.0.1:8000/v1/models
  â€¢ Health: http://127.0.0.1:8000/health

æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡å‹ï¼ˆ~120GBï¼‰ï¼Œè¯·è€å¿ƒç­‰å¾…ï¼**

æœåŠ¡å™¨å¯åŠ¨åä¼šæ˜¾ç¤ºï¼š
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         MLX MiniMax M2.1 API Server                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ“ æ¨¡å‹åŠ è½½å®Œæˆï¼ç”¨æ—¶ 21.25 ç§’

API æœåŠ¡å™¨é…ç½®
æ¨¡å‹: mlx-community/MiniMax-M2.1-4bit
åœ°å€: http://127.0.0.1:8000
```

#### ç¬¬2æ­¥ï¼šé…ç½® OpenClaw

**ç¯å¢ƒå˜é‡æ–¹å¼:**
```bash
export OPENAI_API_BASE="http://127.0.0.1:8000/v1"
export OPENAI_API_KEY="sk-dummy"
```

**é…ç½®æ–‡ä»¶æ–¹å¼ (config.yaml):**
```yaml
llm:
  provider: openai
  base_url: http://127.0.0.1:8000/v1
  api_key: sk-dummy
  model: mlx-community/MiniMax-M2.1-4bit
  temperature: 0.7
  max_tokens: 4000
```

#### ç¬¬3æ­¥ï¼šé‡å¯ OpenClaw

```bash
openclaw restart
```

---

## æµ‹è¯•éªŒè¯

### æ–¹æ³• 1: curl æµ‹è¯•

**LM Studio (ç«¯å£ 1234):**
```bash
curl http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "ä½ å¥½"}],
    "max_tokens": 100
  }'
```

**MLX (ç«¯å£ 8000):**
```bash
curl http://127.0.0.1:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "ä½ å¥½"}],
    "max_tokens": 100
  }'
```

### æ–¹æ³• 2: Python æµ‹è¯•

```python
import openai

# LM Studio
openai.api_base = "http://localhost:1234/v1"
openai.api_key = "lm-studio"

# æˆ– MLX
# openai.api_base = "http://127.0.0.1:8000/v1"
# openai.api_key = "sk-dummy"

response = openai.ChatCompletion.create(
    model="minimax-m2.1",
    messages=[{"role": "user", "content": "ä½ å¥½"}],
    max_tokens=100
)

print(response.choices[0].message.content)
```

### æ–¹æ³• 3: OpenClaw CLI

```bash
# æµ‹è¯•è¿æ¥
openclaw models list

# å‘é€æµ‹è¯•æ¶ˆæ¯
openclaw chat "è¯·ä»‹ç»ä¸€ä¸‹é‡å­è®¡ç®—"

# æ£€æŸ¥å“åº”æ—¶é—´å’Œè´¨é‡
openclaw chat "å†™ä¸€ä¸ªPythonå¿«é€Ÿæ’åºç®—æ³•"
```

---

## æ•…éšœæ’é™¤

### LM Studio ç›¸å…³

**é—®é¢˜: ç«¯å£è¢«å ç”¨**
```bash
# æŸ¥æ‰¾å ç”¨è¿›ç¨‹
lsof -i :1234

# ä½¿ç”¨å…¶ä»–ç«¯å£
lms server start --port 8080
```

**é—®é¢˜: æ¨¡å‹æœªåŠ è½½**
```bash
# æ£€æŸ¥æ¨¡å‹åˆ—è¡¨
lms models list

# é‡å¯æœåŠ¡å™¨
lms server restart
```

**é—®é¢˜: è¿æ¥è¶…æ—¶**
```bash
# æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€
lms server status

# æŸ¥çœ‹æ—¥å¿—
lms server logs --tail 50
```

### MLX ç›¸å…³

**é—®é¢˜: Flask æœªå®‰è£…**
```bash
source venv/bin/activate
pip install flask flask-cors
```

**é—®é¢˜: æ¨¡å‹ä¸‹è½½å¤±è´¥**
```bash
# æ‰‹åŠ¨ä¸‹è½½
huggingface-cli download mlx-community/MiniMax-M2.1-4bit

# æˆ–ä½¿ç”¨ LM Studio ä¸‹è½½ï¼Œç„¶ååˆ›å»ºç¬¦å·é“¾æ¥
ln -s ~/.lmstudio/models/mlx-community/MiniMax-M2.1-4bit \
      ~/.cache/huggingface/hub/
```

**é—®é¢˜: ç«¯å£è¢«å ç”¨**
```bash
# ä½¿ç”¨å…¶ä»–ç«¯å£
python scripts/api_server.py --port 8080

# æ›´æ–° OpenClaw é…ç½®ä¸­çš„ç«¯å£
```

### OpenClaw ç›¸å…³

**é—®é¢˜: OpenClaw æ— æ³•è¿æ¥**

æ£€æŸ¥æ¸…å•:
```bash
# 1. API æœåŠ¡å™¨æ˜¯å¦è¿è¡Œ
curl http://localhost:1234/health  # LM Studio
curl http://127.0.0.1:8000/health  # MLX

# 2. OpenClaw é…ç½®æ˜¯å¦æ­£ç¡®
cat ~/.openclaw/config.yaml | grep base_url

# 3. é˜²ç«å¢™è®¾ç½®
# System Settings -> Network -> Firewall

# 4. é‡å¯æœåŠ¡
openclaw restart
```

**é—®é¢˜: å“åº”é€Ÿåº¦æ…¢**

ä¼˜åŒ–å»ºè®®:
1. ä½¿ç”¨ 4-bit æ¨¡å‹ (æœ€å¿«)
2. å…³é—­å…¶ä»–åº”ç”¨é‡Šæ”¾å†…å­˜
3. ç¡®ä¿ GPU layers = -1 (å…¨éƒ¨)
4. æ£€æŸ¥ç³»ç»Ÿèµ„æº: `Activity Monitor`

**é—®é¢˜: è¾“å‡ºè´¨é‡å·®**

è°ƒæ•´å‚æ•°:
```yaml
llm:
  temperature: 0.7    # é™ä½è·å¾—æ›´ç¡®å®šçš„è¾“å‡º
  top_p: 0.95         # è°ƒæ•´é‡‡æ ·ç­–ç•¥
  max_tokens: 4000    # å¢åŠ å…è®¸æ›´é•¿è¾“å‡º
```

---

## æ€§èƒ½å¯¹æ¯”

### M3 Ultra 512GB å®æµ‹

| æ–¹å¼ | æ¨¡å‹ | TPS | TTFT | å†…å­˜ |
|------|------|-----|------|------|
| LM Studio | MLX 4-bit | 45.7 | 67ms | 135GB |
| LM Studio | GGUF Q4 | ~40 | ~80ms | 140GB |
| MLX | 4-bit | 45.7 | 67ms | 135GB |
| MLX | 8-bit | 33.0 | 95ms | 252GB |

**ç»“è®º:** LM Studio å’Œ MLX æ€§èƒ½ç›¸å½“ï¼Œé€‰æ‹©å–å†³äºåå¥½ã€‚

---

## æ¨èé…ç½®

### æ—¥å¸¸ä½¿ç”¨ (å¯¹è¯/ç¼–ç¨‹)

**LM Studio:**
```bash
lms server start mlx-community/MiniMax-M2.1-4bit \
  --port 1234 \
  --gpu-layers -1
```

**OpenClaw config.yaml:**
```yaml
llm:
  provider: openai
  base_url: http://localhost:1234/v1
  model: minimax-m2.1
  temperature: 0.7
  max_tokens: 2000
```

### é«˜è´¨é‡è¾“å‡º (æ–‡æ¡£ç”Ÿæˆ/åˆ†æ)

**LM Studio:**
```bash
lms server start mlx-community/MiniMax-M2.1-8bit \
  --port 1234 \
  --gpu-layers -1
```

**OpenClaw config.yaml:**
```yaml
llm:
  provider: openai
  base_url: http://localhost:1234/v1
  model: minimax-m2.1
  temperature: 0.5
  max_tokens: 4000
```

---

## ç›¸å…³èµ„æº

- **LM Studio å®Œæ•´è®¾ç½®**: [docs/lm-studio-setup.md](./lm-studio-setup.md)
- **MLX å®Œæ•´è®¾ç½®**: [docs/mlx-local-setup.md](./mlx-local-setup.md)
- **å¿«é€Ÿå¼€å§‹æŒ‡å—**: [QUICKSTART-LMSTUDIO.md](../QUICKSTART-LMSTUDIO.md)
- **æ€§èƒ½æµ‹è¯•ç»“æœ**: [docs/benchmark-results.md](./benchmark-results.md)

---

**å‡†å¤‡å¥½äº†å—ï¼Ÿ** é€‰æ‹©ä½ å–œæ¬¢çš„æ–¹å¼å¼€å§‹é…ç½®ï¼

### ç¬¬2æ­¥ï¼šéªŒè¯APIå·¥ä½œ

æ‰“å¼€**æ–°ç»ˆç«¯**ï¼Œæµ‹è¯•APIï¼š

```bash
# æµ‹è¯•å¥åº·æ£€æŸ¥
curl http://127.0.0.1:8000/health

# æµ‹è¯•chat API
curl http://127.0.0.1:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mlx-community/MiniMax-M2.1-4bit",
    "messages": [{"role": "user", "content": "ä½ å¥½"}],
    "max_tokens": 100
  }'
```

### ç¬¬3æ­¥ï¼šé…ç½®OpenClaw

#### æ–¹å¼Aï¼šç¯å¢ƒå˜é‡ï¼ˆæ¨èï¼‰

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export OPENAI_API_BASE="http://127.0.0.1:8000/v1"
export OPENAI_API_KEY="sk-dummy"  # æœ¬åœ°APIä¸éœ€è¦çœŸå®key

# å¯åŠ¨OpenClaw
openclaw
```

#### æ–¹å¼Bï¼šé…ç½®æ–‡ä»¶

ç¼–è¾‘OpenClawé…ç½®æ–‡ä»¶ï¼ˆé€šå¸¸åœ¨ `~/.openclaw/config.yaml` æˆ–ç±»ä¼¼ä½ç½®ï¼‰ï¼š

```yaml
llm:
  provider: openai
  base_url: http://127.0.0.1:8000/v1
  api_key: sk-dummy  # æœ¬åœ°APIä¸éœ€è¦çœŸå®key
  model: mlx-community/MiniMax-M2.1-4bit
```

#### æ–¹å¼Cï¼šå‘½ä»¤è¡Œå‚æ•°

```bash
openclaw \
  --llm-provider openai \
  --llm-base-url http://127.0.0.1:8000/v1 \
  --llm-api-key sk-dummy \
  --llm-model mlx-community/MiniMax-M2.1-4bit
```

### ç¬¬4æ­¥ï¼šæµ‹è¯•OpenClaw

åœ¨OpenClawä¸­æµ‹è¯•ï¼š

```
> ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±
> å†™ä¸€ä¸ªPythonå¿«é€Ÿæ’åºç®—æ³•
> å¸®æˆ‘åˆ†æä¸€ä¸‹å½“å‰ç›®å½•çš„æ–‡ä»¶
```

---

## è¯¦ç»†é…ç½®

### APIæœåŠ¡å™¨é€‰é¡¹

```bash
# ä½¿ç”¨ä¸åŒçš„æ¨¡å‹
python scripts/api_server.py --model mlx-community/MiniMax-M2.1-8bit

# æ›´æ”¹ç«¯å£
python scripts/api_server.py --port 8080

# å…è®¸å¤–éƒ¨è®¿é—®ï¼ˆè°¨æ…ä½¿ç”¨ï¼ï¼‰
python scripts/api_server.py --host 0.0.0.0 --port 8000

# å®Œæ•´ç¤ºä¾‹
python scripts/api_server.py \
  --model mlx-community/MiniMax-M2.1-4bit \
  --host 127.0.0.1 \
  --port 8000
```

### OpenClawé…ç½®ç¤ºä¾‹

**å®Œæ•´çš„config.yamlç¤ºä¾‹ï¼š**

```yaml
# ~/.openclaw/config.yaml

# LLMé…ç½®
llm:
  provider: openai
  base_url: http://127.0.0.1:8000/v1
  api_key: sk-dummy
  model: mlx-community/MiniMax-M2.1-4bit
  temperature: 0.7
  max_tokens: 2000

# OpenClawå…¶ä»–é…ç½®
agent:
  name: "MiniMaxåŠ©æ‰‹"
  personality: "helpful and concise"

# å·¥å…·é…ç½®
tools:
  enabled:
    - shell
    - file_system
    - web_search
```

---

## æµ‹è¯•éªŒè¯

### 1. APIå¥åº·æ£€æŸ¥

```bash
curl http://127.0.0.1:8000/health
```

æœŸæœ›è¾“å‡ºï¼š
```json
{
  "status": "ok",
  "model": "mlx-community/MiniMax-M2.1-4bit",
  "model_loaded": true
}
```

### 2. æµ‹è¯•Chat API

```bash
curl http://127.0.0.1:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mlx-community/MiniMax-M2.1-4bit",
    "messages": [
      {"role": "user", "content": "è¯·ç”¨ä¸€å¥è¯è§£é‡Šé‡å­è®¡ç®—"}
    ],
    "max_tokens": 100,
    "temperature": 0.7
  }'
```

### 3. æµ‹è¯•Completions API

```bash
curl http://127.0.0.1:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mlx-community/MiniMax-M2.1-4bit",
    "prompt": "å†™ä¸€ä¸ªPythonå†’æ³¡æ’åºï¼š",
    "max_tokens": 300
  }'
```

### 4. åˆ—å‡ºæ¨¡å‹

```bash
curl http://127.0.0.1:8000/v1/models
```

### 5. Pythonæµ‹è¯•è„šæœ¬

åˆ›å»º `test_api.py`ï¼š

```python
import requests

BASE_URL = "http://127.0.0.1:8000/v1"

# æµ‹è¯•chat
response = requests.post(
    f"{BASE_URL}/chat/completions",
    json={
        "model": "mlx-community/MiniMax-M2.1-4bit",
        "messages": [
            {"role": "user", "content": "ä½ å¥½"}
        ],
        "max_tokens": 100
    }
)

print("Status:", response.status_code)
print("Response:", response.json())
```

è¿è¡Œï¼š
```bash
python test_api.py
```

---

## æ€§èƒ½ä¼˜åŒ–

### 1. é€‰æ‹©åˆé€‚çš„æ¨¡å‹

```bash
# é€Ÿåº¦ä¼˜å…ˆï¼ˆæ¨èOpenClawä½¿ç”¨ï¼‰
python scripts/api_server.py --model mlx-community/MiniMax-M2.1-4bit

# è´¨é‡ä¼˜å…ˆ
python scripts/api_server.py --model mlx-community/MiniMax-M2.1-8bit

# å¹³è¡¡
python scripts/api_server.py --model mlx-community/MiniMax-M2.1-6bit
```

**æ¨èï¼š** å¯¹äºOpenClawï¼Œä½¿ç”¨4-bitæ¨¡å‹ï¼ˆ45 TPSï¼‰ï¼Œå“åº”å¿«é€Ÿã€‚

### 2. ä¼˜åŒ–VRAMï¼ˆå¯é€‰ï¼‰

```bash
# å¢åŠ GPUå¯ç”¨å†…å­˜
sudo sysctl iogpu.wired_limit_mb=458752

# å¯åŠ¨APIæœåŠ¡å™¨
python scripts/api_server.py
```

### 3. è°ƒæ•´OpenClawå‚æ•°

```yaml
llm:
  temperature: 0.7      # é»˜è®¤ï¼Œå¹³è¡¡
  # temperature: 0.5   # æ›´ç¡®å®šæ€§
  # temperature: 0.9   # æ›´æœ‰åˆ›æ„

  max_tokens: 1000      # é€‚ä¸­
  # max_tokens: 2000   # é•¿å›å¤
  # max_tokens: 500    # å¿«é€Ÿå›å¤
```

---

## æ•…éšœæ’é™¤

### é—®é¢˜1ï¼šAPIæœåŠ¡å™¨æ— æ³•å¯åŠ¨

**ç—‡çŠ¶ï¼š** `ModuleNotFoundError: No module named 'flask'`

**è§£å†³ï¼š**
```bash
source venv/bin/activate
pip install flask flask-cors
```

### é—®é¢˜2ï¼šOpenClawè¿æ¥å¤±è´¥

**ç—‡çŠ¶ï¼š** `Connection refused` æˆ– `Connection timeout`

**æ£€æŸ¥æ¸…å•ï¼š**
```bash
# 1. ç¡®è®¤APIæœåŠ¡å™¨æ­£åœ¨è¿è¡Œ
curl http://127.0.0.1:8000/health

# 2. æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨
lsof -i :8000

# 3. æŸ¥çœ‹æœåŠ¡å™¨æ—¥å¿—
# åœ¨è¿è¡Œapi_server.pyçš„ç»ˆç«¯æŸ¥çœ‹é”™è¯¯ä¿¡æ¯

# 4. éªŒè¯OpenClawé…ç½®
cat ~/.openclaw/config.yaml
```

### é—®é¢˜3ï¼šå“åº”å¾ˆæ…¢

**ç—‡çŠ¶ï¼š** å“åº”æ—¶é—´ > 10ç§’

**è§£å†³ï¼š**
```bash
# 1. ä½¿ç”¨4-bitæ¨¡å‹
python scripts/api_server.py --model mlx-community/MiniMax-M2.1-4bit

# 2. å‡å°‘max_tokens
# åœ¨OpenClawé…ç½®ä¸­è®¾ç½® max_tokens: 500

# 3. æ£€æŸ¥ç³»ç»Ÿèµ„æº
# å…³é—­å…¶ä»–å ç”¨å†…å­˜çš„åº”ç”¨

# 4. ä¼˜åŒ–VRAM
sudo sysctl iogpu.wired_limit_mb=458752
```

### é—®é¢˜4ï¼šæ¨¡å‹è¾“å‡ºè´¨é‡å·®

**ç—‡çŠ¶ï¼š** å›å¤ä¸è¿è´¯æˆ–æ— æ„ä¹‰

**è§£å†³ï¼š**

1. è°ƒæ•´temperatureï¼š
```yaml
llm:
  temperature: 0.7  # å°è¯•0.5-0.9ä¹‹é—´
```

2. ä½¿ç”¨æ›´é«˜bitçš„æ¨¡å‹ï¼š
```bash
python scripts/api_server.py --model mlx-community/MiniMax-M2.1-8bit
```

### é—®é¢˜5ï¼šAPIè¿”å›401é”™è¯¯

**ç—‡çŠ¶ï¼š** `Unauthorized` æˆ– `Invalid API key`

**è§£å†³ï¼š**
æœ¬åœ°APIä¸éœ€è¦çœŸå®keyï¼Œä½¿ç”¨ä»»æ„å€¼å³å¯ï¼š
```bash
export OPENAI_API_KEY="sk-dummy"
```

æˆ–åœ¨OpenClawé…ç½®ä¸­ï¼š
```yaml
llm:
  api_key: sk-anything-works
```

---

## é«˜çº§ä½¿ç”¨

### 1. åå°è¿è¡ŒAPIæœåŠ¡å™¨

```bash
# ä½¿ç”¨nohupåå°è¿è¡Œ
nohup python scripts/api_server.py > api_server.log 2>&1 &

# æŸ¥çœ‹æ—¥å¿—
tail -f api_server.log

# åœæ­¢æœåŠ¡å™¨
pkill -f api_server.py
```

### 2. ä½¿ç”¨systemdæœåŠ¡ï¼ˆmacOSä½¿ç”¨launchdï¼‰

åˆ›å»º `~/Library/LaunchAgents/com.mlx.api.plist`ï¼š

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
    <key>Label</key>
    <string>com.mlx.api</string>
    <key>ProgramArguments</key>
    <array>
        <string>/Users/jacky/projects/llm-mac-512/venv/bin/python</string>
        <string>/Users/jacky/projects/llm-mac-512/scripts/api_server.py</string>
    </array>
    <key>RunAtLoad</key>
    <true/>
    <key>KeepAlive</key>
    <true/>
    <key>StandardOutPath</key>
    <string>/tmp/mlx-api.log</string>
    <key>StandardErrorPath</key>
    <string>/tmp/mlx-api.error.log</string>
</dict>
</plist>
```

åŠ è½½æœåŠ¡ï¼š
```bash
launchctl load ~/Library/LaunchAgents/com.mlx.api.plist
```

### 3. ç›‘æ§APIæ€§èƒ½

```bash
# æŸ¥çœ‹è¯·æ±‚æ—¥å¿—
# APIæœåŠ¡å™¨ä¼šåœ¨ç»ˆç«¯æ˜¾ç¤ºæ¯ä¸ªè¯·æ±‚

# ç›‘æ§ç³»ç»Ÿèµ„æº
watch -n 1 'ps aux | grep api_server'

# æŸ¥çœ‹å†…å­˜ä½¿ç”¨
memory_pressure
```

---

## æ€§èƒ½å‚è€ƒ

**ä½ çš„ç³»ç»Ÿï¼ˆM3 Ultra 512GBï¼‰+ MLX 4-bitï¼š**

| æŒ‡æ ‡ | æ€§èƒ½ |
|------|------|
| æ¨¡å‹åŠ è½½æ—¶é—´ | ~21ç§’ |
| TTFT (é¦–token) | 67ms |
| ç”Ÿæˆé€Ÿåº¦ | 45.73 tokens/sec |
| å†…å­˜å ç”¨ | 135 GB |
| å¹¶å‘èƒ½åŠ› | 1-4ä¸ªè¯·æ±‚ |

**OpenClawä½¿ç”¨ä½“éªŒï¼š**
- å“åº”è¿…é€Ÿï¼Œæ¥è¿‘äº‘APIä½“éªŒ
- æ— ç½‘ç»œå»¶è¿Ÿ
- å®Œå…¨ç§å¯†ï¼Œæ•°æ®ä¸å‡ºæœ¬åœ°
- æ— APIè´¹ç”¨

---

## å¸¸è§é—®é¢˜ FAQ

### Q1: å¯ä»¥åŒæ—¶è¿æ¥å¤šä¸ªOpenClawå®ä¾‹å—ï¼Ÿ

**A:** å¯ä»¥ï¼ŒAPIæœåŠ¡å™¨æ”¯æŒå¹¶å‘è¯·æ±‚ã€‚ä½†æ€§èƒ½ä¼šéšå¹¶å‘æ•°ä¸‹é™ã€‚

### Q2: èƒ½å¦ä½¿ç”¨å…¶ä»–ç«¯å£ï¼Ÿ

**A:** å¯ä»¥ï¼š
```bash
python scripts/api_server.py --port 8080
```
ç„¶ååœ¨OpenClawä¸­é…ç½® `base_url: http://127.0.0.1:8080/v1`

### Q3: æ˜¯å¦æ”¯æŒæµå¼å“åº”ï¼Ÿ

**A:** APIåŒ…å«åŸºç¡€çš„æµå¼æ”¯æŒï¼Œä½†å¯èƒ½éœ€è¦æ ¹æ®OpenClawçš„å…·ä½“éœ€æ±‚è°ƒæ•´ã€‚

### Q4: å¦‚ä½•æŸ¥çœ‹APIæ—¥å¿—ï¼Ÿ

**A:** APIæœåŠ¡å™¨ä¼šåœ¨ç»ˆç«¯å®æ—¶æ˜¾ç¤ºè¯·æ±‚æ—¥å¿—ã€‚

### Q5: å¯ä»¥è¿œç¨‹è®¿é—®å—ï¼Ÿ

**A:** å¯ä»¥ï¼Œä½†**ä¸æ¨è**ï¼ˆå®‰å…¨é£é™©ï¼‰ã€‚å¦‚éœ€è¦ï¼š
```bash
python scripts/api_server.py --host 0.0.0.0
```
ç„¶åé…ç½®é˜²ç«å¢™å’Œè®¤è¯ã€‚

---

## ä¸‹ä¸€æ­¥

1. âœ… å¯åŠ¨APIæœåŠ¡å™¨
2. âœ… é…ç½®OpenClaw
3. âœ… æµ‹è¯•åŸºæœ¬åŠŸèƒ½
4. ğŸ“Š ç›‘æ§æ€§èƒ½å’Œä¼˜åŒ–
5. ğŸš€ äº«å—æœ¬åœ°AIåŠ©æ‰‹ï¼

---

## ç›¸å…³èµ„æº

- **APIæœåŠ¡å™¨è„šæœ¬ï¼š** `scripts/api_server.py`
- **MLXè®¾ç½®æŒ‡å—ï¼š** `docs/mlx-local-setup.md`
- **æ€§èƒ½æµ‹è¯•ç»“æœï¼š** `docs/benchmark-results.md`
- **OpenClawå®˜ç½‘ï¼š** https://openclaw.ai
- **OpenClawæ–‡æ¡£ï¼š** https://docs.openclaw.ai

---

**ç¥ä½¿ç”¨æ„‰å¿«ï¼å¦‚æœ‰é—®é¢˜ï¼ŒæŸ¥çœ‹æ•…éšœæ’é™¤éƒ¨åˆ†æˆ–æ£€æŸ¥æ—¥å¿—ã€‚**
