# Automated Test Results Summary
**Date:** 2026-02-05
**Session:** Automated MLX vs GGUF Comparison

## Test Results

### Test 1: GGUF Q4_K_S (130GB)
**Model:** unsloth/minimax-m2.1
**Backend:** llama.cpp (GGUF)
**Status:** ✓ Complete

| Metric | Value |
|--------|-------|
| **Average TPS** | **40.31** |
| Max TPS | 46.05 |
| Min TPS | 28.36 |
| Tests Passed | 5/5 |
| Duration | 90 seconds |
| Model Size | 130GB |

**Detailed Results:**
- 短文本生成: 28.36 TPS (100 tokens, 3.53s)
- 代码生成: 43.44 TPS (500 tokens, 11.51s)
- 长文本生成: 46.05 TPS (2000 tokens, 43.43s)
- 推理能力: 42.34 TPS (500 tokens, 11.81s)
- 指令跟随: 41.38 TPS (400 tokens, 9.67s)

**Result File:** `lmstudio-benchmark-20260205-152620.json`

---

### Test 2: MLX 8-bit (243GB)
**Model:** mlx-community/minimax-m2.1
**Backend:** MLX
**Status:** ✓ Complete

| Metric | Value |
|--------|-------|
| **Average TPS** | **25.98** |
| Max TPS | 32.92 |
| Min TPS | 14.47 |
| Tests Passed | 4/5 |
| Duration | 129 seconds |
| Model Size | 243GB |
| Load Time | 41.4 seconds |

**Detailed Results:**
- 短文本生成: 14.47 TPS (99 tokens, 6.84s)
- 代码生成: 28.33 TPS (499 tokens, 17.61s)
- 长文本生成: 32.92 TPS (1999 tokens, 60.73s)
- 推理能力: 28.22 TPS (499 tokens, 17.68s)
- 指令跟随: Failed/Incomplete

**Result File:** `lmstudio-benchmark-20260205-152959.json`

---

## Performance Comparison

### TPS Comparison
| Test Type | Q4_K_S (GGUF) | MLX 8-bit | Difference |
|-----------|---------------|-----------|------------|
| 短文本生成 | 28.36 | 14.47 | +96% faster |
| 代码生成 | 43.44 | 28.33 | +53% faster |
| 长文本生成 | 46.05 | 32.92 | +40% faster |
| 推理能力 | 42.34 | 28.22 | +50% faster |
| 指令跟随 | 41.38 | N/A | - |
| **Average** | **40.31** | **25.98** | **+55% faster** |

### Key Findings

1. **GGUF Q4_K_S significantly outperforms MLX 8-bit**
   - 55% faster on average
   - Consistently faster across all test types
   - Despite being half the size (130GB vs 243GB)

2. **Model Size vs Performance**
   - Q4_K_S: 130GB, 40.31 TPS → **0.31 TPS per GB**
   - MLX 8-bit: 243GB, 25.98 TPS → **0.11 TPS per GB**
   - GGUF is 2.8x more efficient per GB

3. **Load Time**
   - MLX 8-bit: 41.4 seconds for 243GB
   - Load speed: ~5.9 GB/s

4. **Test Reliability**
   - Q4_K_S: 5/5 tests passed
   - MLX 8-bit: 4/5 tests passed

## Recommendations

### For Production Use
**Winner: GGUF Q4_K_S**
- Best performance (40.31 TPS)
- Smaller size (130GB)
- More reliable (5/5 tests)
- Better efficiency per GB

### Next Steps

1. **Complete GGUF Testing Matrix** (if desired):
   - Q4_K_M (138GB) - Expected: ~38-42 TPS
   - Q6_K (188GB) - Expected: ~35-40 TPS
   - Q8_0 (243GB) - Expected: ~30-35 TPS

2. **MLX 4-bit Testing** (if model download completes):
   - Expected performance: Better than 8-bit
   - Smaller size (~120GB)
   - May compete with Q4_K_S

3. **Consider Testing Qwen3-Coder-Next**:
   - Smaller model (80B parameters)
   - Faster inference expected
   - Good for code-heavy tasks

## Technical Notes

### Why GGUF Outperforms MLX
Possible reasons:
1. llama.cpp's GGUF backend is highly optimized for MoE models
2. Q4_K_S quantization is well-tuned for this architecture
3. MLX 8-bit may have suboptimal quantization for this specific model
4. Memory bandwidth utilization differences

### Testing Environment
- **Hardware:** Mac with 512GB unified memory
- **LM Studio:** CLI version
- **Context Length:** 131,072 tokens
- **API:** http://localhost:1234
- **Test Framework:** benchmark_lmstudio.py

### Test Methodology
- 5 standardized prompts (short text, code, long text, reasoning, instruction following)
- Token counts: 100, 500, 2000, 500, 400
- 30-second cooldown between tests
- Automated model loading/unloading

## Conclusion

For MiniMax M2.1 on Mac with 512GB memory:
- **Best choice: GGUF Q4_K_S** (130GB, 40.31 TPS)
- MLX 8-bit is 55% slower despite being 87% larger
- GGUF backend provides better performance-to-size ratio
- Q4_K_S offers the best balance of speed, size, and reliability

---

*Generated by automated testing script v4*
*Session: 2026-02-05 15:24 - 15:30*
