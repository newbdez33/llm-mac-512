# æµ‹è¯•æ¨¡å‹æ¸…å•

> æ›´æ–°æ—¶é—´: 2026-02-04
> æ‰€æœ‰æ¨¡å‹é€šè¿‡ LM Studio åŠ è½½æµ‹è¯•

---

## MiniMax M2.1 (230B/10B MoE)

### MLX ç‰ˆæœ¬ (mlx-community)

| é‡åŒ– | æ¨¡å‹ä»“åº“ | å¤§å° | å†…å­˜ | çŠ¶æ€ | å¤‡æ³¨ |
|------|----------|------|------|------|------|
| **4-bit** | [mlx-community/MiniMax-M2.1-4bit](https://huggingface.co/mlx-community/MiniMax-M2.1-4bit) | ~120GB | ~135GB | âœ… å¯ç”¨ | æ¨è |
| **3-bit** | [mlx-community/MiniMax-M2.1-3bit](https://huggingface.co/mlx-community/MiniMax-M2.1-3bit) | ~90GB | ~105GB | ğŸ” å¤‡é€‰ | æ›´å°ä½†è´¨é‡å¯èƒ½ä¸‹é™ |
| **8-bit** | [mlx-community/MiniMax-M2.1-8bit-gs32](https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32) | ~240GB | ~252GB | âœ… å¯ç”¨ | é«˜è´¨é‡ |
| **6-bit** | âŒ ä¸å­˜åœ¨ | - | - | âŒ | mlx-community æ— æ­¤ç‰ˆæœ¬ |

**ä¸‹è½½å‘½ä»¤**:
```bash
# LM Studio CLI
lms download mlx-community/MiniMax-M2.1-4bit
lms download mlx-community/MiniMax-M2.1-8bit-gs32

# æˆ– mlx-lm
mlx_lm.convert --hf-path mlx-community/MiniMax-M2.1-4bit
```

---

### GGUF ç‰ˆæœ¬ (unsloth)

| é‡åŒ– | æ¨¡å‹ä»“åº“ | æ–‡ä»¶ | å¤§å° | å†…å­˜ | çŠ¶æ€ | å¤‡æ³¨ |
|------|----------|------|------|------|------|------|
| **Q4_K_S** | [unsloth/MiniMax-M2.1-GGUF](https://huggingface.co/unsloth/MiniMax-M2.1-GGUF) | Q4_K_S | 130GB | ~135GB | ğŸ”„ å·²åŠ è½½ | å°4-bit |
| **Q4_K_M** | [unsloth/MiniMax-M2.1-GGUF](https://huggingface.co/unsloth/MiniMax-M2.1-GGUF) | Q4_K_M | 138GB | ~143GB | âœ… å¯ç”¨ | æ ‡å‡†4-bit |
| **Q6_K** | [unsloth/MiniMax-M2.1-GGUF](https://huggingface.co/unsloth/MiniMax-M2.1-GGUF) | Q6_K | 188GB | ~193GB | âœ… å¯ç”¨ | 6-bit |
| **Q8_0** | [unsloth/MiniMax-M2.1-GGUF](https://huggingface.co/unsloth/MiniMax-M2.1-GGUF) | Q8_0 | 243GB | ~248GB | âœ… å¯ç”¨ | 8-bit |
| **BF16** | [unsloth/MiniMax-M2.1-GGUF](https://huggingface.co/unsloth/MiniMax-M2.1-GGUF) | BF16 | 457GB | ~462GB | âŒ å¤±è´¥ | OOM (å·²æµ‹) |

**ä¸‹è½½å‘½ä»¤**:
```bash
# LM Studio
lms download unsloth/MiniMax-M2.1-GGUF:Q4_K_S
lms download unsloth/MiniMax-M2.1-GGUF:Q4_K_M
lms download unsloth/MiniMax-M2.1-GGUF:Q6_K
lms download unsloth/MiniMax-M2.1-GGUF:Q8_0
```

---

### MiniMax M2.1 æµ‹è¯•é…å¯¹

| é‡åŒ–çº§åˆ« | MLX ç‰ˆæœ¬ | GGUF ç‰ˆæœ¬ | å¯¹æ¯”ç›®çš„ |
|----------|----------|-----------|----------|
| **4-bit** | mlx-4bit (120GB) | Q4_K_S (130GB) âœ… | ç›¸ä¼¼å¤§å°ï¼Œæ¡†æ¶å¯¹æ¯” |
| **4-bit+** | - | Q4_K_M (138GB) | æ›´å¤§çš„ Q4 ç‰ˆæœ¬ |
| **8-bit** | mlx-8bit-gs32 (240GB) | Q8_0 (243GB) | é«˜ç²¾åº¦å¯¹æ¯” |

**æ³¨æ„**:
- âŒ æ—  6-bit MLX ç‰ˆæœ¬ï¼ŒQ6_K åªèƒ½å•ç‹¬æµ‹è¯•
- âœ… æœ‰ 3-bit MLX ç‰ˆæœ¬ï¼Œå¯ä½œä¸ºå¤‡é€‰

---

## Qwen3-Coder-Next (80B/3B MoE)

### MLX ç‰ˆæœ¬

| çŠ¶æ€ | è¯´æ˜ |
|------|------|
| âŒ **æ— é¢„é‡åŒ– MLX ç‰ˆæœ¬** | mlx-community æš‚æ— è½¬æ¢ç‰ˆæœ¬ |
| âš ï¸ **å¯æ‰‹åŠ¨è½¬æ¢** | å®˜æ–¹æ”¯æŒ MLX-LMï¼Œå¯è‡ªè¡Œè½¬æ¢åŸå§‹æ¨¡å‹ |
| ğŸ“¦ **åŸå§‹æ¨¡å‹** | [Qwen/Qwen3-Coder-Next](https://huggingface.co/Qwen/Qwen3-Coder-Next) (BF16, 159GB) |

**æ‰‹åŠ¨è½¬æ¢** (å¦‚éœ€è¦):
```bash
# ä¸‹è½½åŸå§‹æ¨¡å‹
huggingface-cli download Qwen/Qwen3-Coder-Next

# ä½¿ç”¨ mlx-lm è½¬æ¢ä¸º 4-bit
mlx_lm.convert \
  --hf-path Qwen/Qwen3-Coder-Next \
  --quantize \
  --q-bits 4 \
  --mlx-path ./qwen3-coder-next-4bit
```

**æ³¨æ„**:
- åŸå§‹æ¨¡å‹ 159GBï¼Œè½¬æ¢ä¸º 4-bit çº¦ 45GB
- è½¬æ¢éœ€è¦çº¦ 200GB+ ä¸´æ—¶ç©ºé—´
- è½¬æ¢æ—¶é—´çº¦ 1-2 å°æ—¶

---

### GGUF ç‰ˆæœ¬ (unsloth) âœ…

| é‡åŒ– | æ¨¡å‹ä»“åº“ | æ–‡ä»¶ | å¤§å° | å†…å­˜ | çŠ¶æ€ | ä¼˜å…ˆçº§ |
|------|----------|------|------|------|------|--------|
| **Q4_K_M** | [unsloth/Qwen3-Coder-Next-GGUF](https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF) | Q4_K_M | 48.5GB | ~53GB | âœ… æ¨è | ğŸ”¥ 1 |
| **Q4_0** | [unsloth/Qwen3-Coder-Next-GGUF](https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF) | Q4_0 | 45.3GB | ~50GB | âœ… å¯ç”¨ | 2 |
| **Q6_K** | [unsloth/Qwen3-Coder-Next-GGUF](https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF) | Q6_K | 65.5GB | ~70GB | âœ… æ¨è | ğŸ”¥ 3 |
| **Q8_0** | [unsloth/Qwen3-Coder-Next-GGUF](https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF) | Q8_0 | 84.8GB | ~90GB | âœ… æ¨è | ğŸ”¥ 4 |
| **Q2_K** | [unsloth/Qwen3-Coder-Next-GGUF](https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF) | Q2_K | 29.2GB | ~34GB | âœ… å¯é€‰ | 5 |
| **BF16** | [unsloth/Qwen3-Coder-Next-GGUF](https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF) | BF16 | 159GB | ~164GB | âœ… å¯æµ‹ | 6 |

**ä¸‹è½½å‘½ä»¤**:
```bash
# LM Studio
lms download unsloth/Qwen3-Coder-Next-GGUF:Q4_K_M
lms download unsloth/Qwen3-Coder-Next-GGUF:Q6_K
lms download unsloth/Qwen3-Coder-Next-GGUF:Q8_0
lms download unsloth/Qwen3-Coder-Next-GGUF:BF16
```

---

### Qwen3-Coder-Next æµ‹è¯•è®¡åˆ’

| é‡åŒ–çº§åˆ« | MLX ç‰ˆæœ¬ | GGUF ç‰ˆæœ¬ | æµ‹è¯•è®¡åˆ’ |
|----------|----------|-----------|----------|
| **4-bit** | âŒ ä¸å­˜åœ¨ | Q4_K_M (48.5GB) âœ… | ä»… GGUF |
| **6-bit** | âŒ ä¸å­˜åœ¨ | Q6_K (65.5GB) âœ… | ä»… GGUF |
| **8-bit** | âŒ ä¸å­˜åœ¨ | Q8_0 (84.8GB) âœ… | ä»… GGUF |
| **BF16** | âŒ ä¸å­˜åœ¨ | BF16 (159GB) âœ… | ä»… GGUFï¼Œå¯é€‰ |

**ç»“è®º**: Qwen3-Coder-Next åªèƒ½æµ‹è¯• GGUF ç‰ˆæœ¬ï¼Œæ— æ³•è¿›è¡Œ MLX vs llama.cpp æ¡†æ¶å¯¹æ¯”

**å¤‡é€‰æ–¹æ¡ˆ**:
1. æ‰‹åŠ¨è½¬æ¢ MLX 4-bit ç‰ˆæœ¬ (éœ€è¦ 1-2 å°æ—¶ + 200GB ç©ºé—´)
2. åªæµ‹è¯• GGUF ç‰ˆæœ¬çš„æ€§èƒ½
3. ä½¿ç”¨ Qwen3-Next (ä¸æ˜¯ Coder) çš„ MLX ç‰ˆæœ¬è¿›è¡Œæ¡†æ¶å¯¹æ¯”

---

## æ¨èæµ‹è¯•çŸ©é˜µ

### Phase 1: MiniMax M2.1 (å®Œæ•´æµ‹è¯•)

| Day | MLX Backend (LM Studio) | llama.cpp Backend (LM Studio) | å¯¹æ¯” |
|-----|-------------------------|-------------------------------|------|
| 1 | mlx-4bit (120GB) | Q4_K_S (130GB) | âœ… æ¡†æ¶å¯¹æ¯” |
| 2 | - | Q4_K_M (138GB) | Q4 å˜ä½“å¯¹æ¯” |
| 3 | mlx-8bit (240GB) | Q8_0 (243GB) | âœ… æ¡†æ¶å¯¹æ¯” |
| 4 | - | Q6_K (188GB) | å•ç‹¬æµ‹è¯• |
| 5 | (å¯é€‰) mlx-3bit (90GB) | - | æ›´å°ç‰ˆæœ¬ |

**é¢„è®¡ç£ç›˜å ç”¨**: æœ€å¤§çº¦ 400GB (åŒæ—¶ä¿ç•™ 2-3 ä¸ªæ¨¡å‹)

---

### Phase 2: Qwen3-Coder-Next (GGUF only)

| Day | llama.cpp Backend (LM Studio) | è¯´æ˜ |
|-----|-------------------------------|------|
| 1 | Q4_K_M (48.5GB) | 4-bit åŸºå‡† |
| 2 | Q6_K (65.5GB) | 6-bit |
| 3 | Q8_0 (84.8GB) | 8-bit |
| 4 | Q2_K (29.2GB) + Q4_0 (45.3GB) | è½»é‡çº§ç‰ˆæœ¬ |
| 5 | (å¯é€‰) BF16 (159GB) | å®Œæ•´ç²¾åº¦ |

**é¢„è®¡ç£ç›˜å ç”¨**: æœ€å¤§çº¦ 250GB

---

## ç£ç›˜ç©ºé—´è§„åˆ’

### å¹¶è¡Œä¿ç•™ç­–ç•¥

**MiniMax M2.1 æµ‹è¯•æ—¶**:
```
æ´»è·ƒæ¨¡å‹:
- MLX 4-bit: 120GB
- GGUF Q4_K_S: 130GB
æ€»è®¡: ~250GB

å¯é€‰ä¿ç•™:
+ MLX 8-bit: 240GB
+ GGUF Q8_0: 243GB
å¦‚æœç£ç›˜å……è¶³: ~730GB
```

**Qwen3-Coder-Next æµ‹è¯•æ—¶**:
```
æ´»è·ƒæ¨¡å‹:
- Q4_K_M: 48.5GB
- Q6_K: 65.5GB
- Q8_0: 84.8GB
æ€»è®¡: ~200GB (å¯åŒæ—¶ä¿ç•™)
```

### åˆ é™¤ç­–ç•¥

**æµ‹è¯•å®Œæ¯•å³åˆ é™¤**:
```bash
# æµ‹è¯•å®Œæˆååˆ é™¤æ¨¡å‹
rm -rf ~/.lmstudio/models/{model-name}

# æˆ–é€šè¿‡ LM Studio GUI åˆ é™¤
```

**ä¿ç•™æ ¸å¿ƒç‰ˆæœ¬**:
- MiniMax M2.1 4-bit (120GB) - æ€§ä»·æ¯”æœ€é«˜
- Qwen3-Coder-Next Q4_K_M (48.5GB) - æ¨èç‰ˆæœ¬

æ€»è®¡: ~170GB (é•¿æœŸä¿ç•™)

---

## ä¸‹è½½æ¸…å•

### Week 1 å‡†å¤‡ (MiniMax M2.1)

```bash
# MLX ç‰ˆæœ¬
lms download mlx-community/MiniMax-M2.1-4bit
lms download mlx-community/MiniMax-M2.1-8bit-gs32

# GGUF ç‰ˆæœ¬ (Q4_K_S å·²æœ‰)
lms download unsloth/MiniMax-M2.1-GGUF:Q4_K_M
lms download unsloth/MiniMax-M2.1-GGUF:Q6_K
lms download unsloth/MiniMax-M2.1-GGUF:Q8_0
```

**ä¸‹è½½å¤§å°**: ~850GB
**å»ºè®®**: è¾¹æµ‹è¾¹ä¸‹ï¼Œæµ‹å®Œåˆ é™¤

---

### Week 2 å‡†å¤‡ (Qwen3-Coder-Next)

```bash
# GGUF ç‰ˆæœ¬ (ä»…æ­¤ä¸€å¥—)
lms download unsloth/Qwen3-Coder-Next-GGUF:Q4_K_M
lms download unsloth/Qwen3-Coder-Next-GGUF:Q6_K
lms download unsloth/Qwen3-Coder-Next-GGUF:Q8_0

# å¯é€‰
lms download unsloth/Qwen3-Coder-Next-GGUF:Q2_K
lms download unsloth/Qwen3-Coder-Next-GGUF:Q4_0
lms download unsloth/Qwen3-Coder-Next-GGUF:BF16
```

**ä¸‹è½½å¤§å°**: ~200GB (æ ¸å¿ƒç‰ˆæœ¬)
**å»ºè®®**: å¯åŒæ—¶ä¸‹è½½ï¼Œå ç”¨ç©ºé—´è¾ƒå°

---

## æ¨¡å‹éªŒè¯æ¸…å•

ä¸‹è½½å®ŒæˆåéªŒè¯ï¼š

### MiniMax M2.1
- [ ] MLX 4-bit: å¯åœ¨ LM Studio ä¸­åŠ è½½ï¼Œæ˜¾ç¤º "MLX" æ ‡ç­¾
- [ ] MLX 8-bit: å¯åœ¨ LM Studio ä¸­åŠ è½½ï¼Œæ˜¾ç¤º "MLX" æ ‡ç­¾
- [ ] GGUF Q4_K_S: å·²åŠ è½½ âœ…
- [ ] GGUF Q4_K_M: å¯åŠ è½½ï¼Œæ˜¾ç¤º "llama.cpp" æ ‡ç­¾
- [ ] GGUF Q6_K: å¯åŠ è½½ï¼Œæ˜¾ç¤º "llama.cpp" æ ‡ç­¾
- [ ] GGUF Q8_0: å¯åŠ è½½ï¼Œæ˜¾ç¤º "llama.cpp" æ ‡ç­¾

### Qwen3-Coder-Next
- [ ] GGUF Q4_K_M: å¯åŠ è½½ï¼Œæ˜¾ç¤º "llama.cpp" æ ‡ç­¾
- [ ] GGUF Q6_K: å¯åŠ è½½ï¼Œæ˜¾ç¤º "llama.cpp" æ ‡ç­¾
- [ ] GGUF Q8_0: å¯åŠ è½½ï¼Œæ˜¾ç¤º "llama.cpp" æ ‡ç­¾

---

## å‚è€ƒé“¾æ¥

### MiniMax M2.1
- [MLX 4-bit](https://huggingface.co/mlx-community/MiniMax-M2.1-4bit)
- [MLX 8-bit-gs32](https://huggingface.co/mlx-community/MiniMax-M2.1-8bit-gs32)
- [Unsloth GGUF](https://huggingface.co/unsloth/MiniMax-M2.1-GGUF)

### Qwen3-Coder-Next
- [å®˜æ–¹æ¨¡å‹](https://huggingface.co/Qwen/Qwen3-Coder-Next)
- [Unsloth GGUF](https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF)
- [å®˜æ–¹åšå®¢](https://qwen.ai/blog?id=qwen3-coder-next)

---

## æ›´æ–°æ—¥å¿—

- **2026-02-04**: åˆå§‹ç‰ˆæœ¬ï¼Œç¡®è®¤æ‰€æœ‰æµ‹è¯•æ¨¡å‹
  - MiniMax M2.1: MLX 4bit/8bit + GGUF Q4/Q6/Q8
  - Qwen3-Coder-Next: GGUF only (æ—  MLX é¢„é‡åŒ–ç‰ˆæœ¬)
  - æ³¨æ„: æ—  MLX 6-bit MiniMax ç‰ˆæœ¬
