# LM Studio å¿«é€Ÿå¼€å§‹ - 5åˆ†é’Ÿè¿è¡Œ MiniMax M2.1

> æœ€ç®€å•çš„æ–¹å¼åœ¨ Mac ä¸Šè¿è¡Œå¤§å‹è¯­è¨€æ¨¡å‹

## ğŸš€ 3 æ­¥å¼€å§‹

> **é€‰æ‹©ä½ å–œæ¬¢çš„æ–¹å¼:** [GUI å›¾å½¢ç•Œé¢](#gui-æ–¹å¼) | [CLI å‘½ä»¤è¡Œ](#cli-æ–¹å¼-æ¨è)

---

## CLI æ–¹å¼ â­ æ¨è

### 3 ä¸ªå‘½ä»¤å®Œæˆ

```bash
# 1. å®‰è£… LM Studio
brew install --cask lm-studio

# 2. ä¸‹è½½æ¨¡å‹ (è‡ªåŠ¨ä¸‹è½½ ~120GB)
lms download mlx-community/MiniMax-M2.1-4bit

# 3. å¯åŠ¨æœåŠ¡å™¨
lms server start mlx-community/MiniMax-M2.1-4bit --port 1234
```

**å®Œæˆï¼**æœåŠ¡å™¨è¿è¡Œåœ¨ `http://localhost:1234`

### å¿«é€Ÿæµ‹è¯•

```bash
# æµ‹è¯• API
curl http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"messages": [{"role": "user", "content": "ä½ å¥½"}], "max_tokens": 100}'

# æŸ¥çœ‹æœåŠ¡å™¨çŠ¶æ€
lms server status

# æŸ¥çœ‹æ—¥å¿—
lms server logs
```

---

## GUI æ–¹å¼

### æ­¥éª¤ 1: ä¸‹è½½ LM Studio (2 åˆ†é’Ÿ)

```bash
# æ‰“å¼€æµè§ˆå™¨ä¸‹è½½
open https://lmstudio.ai/download

# æˆ–ç›´æ¥ä¸‹è½½ dmg
curl -L https://lmstudio.ai/download -o LMStudio.dmg
```

**å®‰è£…:**
1. æ‰“å¼€ä¸‹è½½çš„ `.dmg` æ–‡ä»¶
2. æ‹–æ‹½ LM Studio åˆ° Applications æ–‡ä»¶å¤¹
3. ä» Launchpad æˆ– Applications æ‰“å¼€ LM Studio

### æ­¥éª¤ 2: ä¸‹è½½æ¨¡å‹ (30-60 åˆ†é’Ÿ)

**åœ¨ LM Studio ç•Œé¢ä¸­:**

1. ç‚¹å‡»å·¦ä¾§ **ğŸ” Search** å›¾æ ‡
2. æœç´¢æ¡†è¾“å…¥: `MiniMax-M2.1`
3. æ‰¾åˆ°å¹¶ä¸‹è½½ **æ¨èæ¨¡å‹**:

   **æ¨è - MLX 4-bit (æœ€å¿«):**
   ```
   mlx-community/MiniMax-M2.1-4bit
   å¤§å°: ~120GB
   é€Ÿåº¦: 45 TPS
   ```

   **å¤‡é€‰ - GGUF Q4 (é€šç”¨):**
   ```
   unsloth/MiniMax-M2.1-GGUF:Q4_K_M
   å¤§å°: ~138GB
   é€Ÿåº¦: 40 TPS
   ```

4. ç‚¹å‡» **Download** æŒ‰é’®
5. ç­‰å¾…ä¸‹è½½å®Œæˆï¼ˆè¿›åº¦æ¡ä¼šæ˜¾ç¤ºï¼‰

**é¦–æ¬¡ä¸‹è½½éœ€è¦æ—¶é—´ï¼š**
- 120GB æ¨¡å‹çº¦éœ€ 30-60 åˆ†é’Ÿ
- å–å†³äºç½‘ç»œé€Ÿåº¦
- å¯ä»¥æš‚åœåç»§ç»­ä¸‹è½½

### æ­¥éª¤ 3: å¯åŠ¨å¹¶æµ‹è¯• (2 åˆ†é’Ÿ)

**æ–¹å¼ A: èŠå¤©ç•Œé¢ (æœ€ç®€å•)**

1. ç‚¹å‡»å·¦ä¾§ **ğŸ’¬ Chat** å›¾æ ‡
2. é¡¶éƒ¨é€‰æ‹©åˆšä¸‹è½½çš„æ¨¡å‹
3. ç­‰å¾…æ¨¡å‹åŠ è½½ (çº¦ 20 ç§’)
4. åœ¨åº•éƒ¨è¾“å…¥æ¡†è¾“å…¥æ¶ˆæ¯ï¼Œæ¯”å¦‚:
   ```
   è¯·ç”¨ä¸€å¥è¯è§£é‡Šé‡å­è®¡ç®—
   ```
5. æŒ‰ Enterï¼ŒæŸ¥çœ‹å›å¤ï¼

**æ–¹å¼ B: API æœåŠ¡å™¨ (ç”¨äº OpenClaw)**

1. ç‚¹å‡»å·¦ä¾§ **âš¡ Local Server** å›¾æ ‡
2. é€‰æ‹©æ¨¡å‹ (å¦‚æœæœªåŠ è½½)
3. ç‚¹å‡» **Start Server** ğŸš€
4. ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨ï¼Œçœ‹åˆ°:
   ```
   âœ… Server running on http://localhost:1234
   ```

**æµ‹è¯• API:**
```bash
# æ‰“å¼€ç»ˆç«¯ï¼Œè¿è¡Œæµ‹è¯•
curl http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "ä½ å¥½"}],
    "max_tokens": 100
  }'
```

å®Œæˆï¼ğŸ‰

---

## ğŸ“± ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹ 1: ç®€å•å¯¹è¯

```
ä½ : ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ

åŠ©æ‰‹: æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„å±‚æ¬¡åŒ–è¡¨ç¤ºï¼Œä»è€Œå®ç°å¯¹å¤æ‚æ¨¡å¼çš„è¯†åˆ«å’Œé¢„æµ‹ã€‚

âš¡ 23 tokens | 0.52s | 44.2 tokens/s
```

### ç¤ºä¾‹ 2: ä»£ç ç”Ÿæˆ

```
ä½ : å†™ä¸€ä¸ª Python å¿«é€Ÿæ’åºç®—æ³•

åŠ©æ‰‹: [ç”Ÿæˆå®Œæ•´çš„å¿«é€Ÿæ’åºä»£ç ï¼ŒåŒ…å«æ³¨é‡Šå’Œç¤ºä¾‹]

âš¡ 156 tokens | 3.4s | 45.9 tokens/s
```

### ç¤ºä¾‹ 3: æ¨ç†ä»»åŠ¡

```
ä½ : å¦‚æœæˆ‘æœ‰ 5 ä¸ªè‹¹æœï¼Œç»™äº† 2 ä¸ªç»™æœ‹å‹ï¼Œåˆä¹°äº† 3 ä¸ªï¼Œç°åœ¨æœ‰å¤šå°‘ä¸ªï¼Ÿ

åŠ©æ‰‹: <think>
åˆå§‹: 5 ä¸ªè‹¹æœ
ç»™å‡º: -2 ä¸ª
è´­ä¹°: +3 ä¸ª
è®¡ç®—: 5 - 2 + 3 = 6
</think>

ä½ ç°åœ¨æœ‰ 6 ä¸ªè‹¹æœã€‚

âš¡ 45 tokens | 1.0s | 45.0 tokens/s
```

---

## ğŸ”Œ é…ç½® OpenClaw

### å¿«é€Ÿé…ç½® (3 ä¸ªå‘½ä»¤)

```bash
# 1. è®¾ç½® API ç«¯ç‚¹
export OPENAI_API_BASE="http://localhost:1234/v1"
export OPENAI_API_KEY="lm-studio"

# 2. æµ‹è¯•è¿æ¥
openclaw models list

# 3. å¼€å§‹ä½¿ç”¨
openclaw chat "è¯·ä»‹ç»é‡å­è®¡ç®—"
```

### æ°¸ä¹…é…ç½®

ç¼–è¾‘ `~/.openclaw/config.yaml`:

```yaml
llm:
  provider: openai
  base_url: http://localhost:1234/v1
  api_key: lm-studio
  model: minimax-m2.1
  temperature: 0.7
  max_tokens: 4000
```

ç„¶åé‡å¯ OpenClaw:

```bash
openclaw restart
```

---

## âš™ï¸ å¸¸ç”¨è®¾ç½®

### æ¨èé…ç½® (LM Studio ç•Œé¢)

**GPU è®¾ç½®:**
```
GPU Offload: Max
GPU Layers: -1 (å…¨éƒ¨)
Metal: âœ… Enabled
```

**æœåŠ¡å™¨è®¾ç½®:**
```
Port: 1234
CORS: âœ… Enabled
Auto-start: âœ… (å¯é€‰)
Require Auth: âŒ (æœ¬åœ°ä¸éœ€è¦)
```

**ç”Ÿæˆå‚æ•°:**
```
Temperature: 0.7
Top P: 0.95
Max Tokens: 4000
Context Length: 32768
```

---

## ğŸ¯ å¿«æ·é”®

| å¿«æ·é”® | åŠŸèƒ½ |
|-------|------|
| `Cmd+K` | æ–°å¯¹è¯ |
| `Cmd+,` | è®¾ç½® |
| `Cmd+Shift+L` | åˆ‡æ¢æ¨¡å‹ |
| `Cmd+Enter` | å‘é€æ¶ˆæ¯ |
| `Esc` | åœæ­¢ç”Ÿæˆ |

---

## ğŸ”§ æ•…éšœæ’é™¤

### Q: æ¨¡å‹ä¸‹è½½å¾ˆæ…¢ï¼Ÿ

**A:**
- è€å¿ƒç­‰å¾…ï¼Œ120GB éœ€è¦æ—¶é—´
- å¯ä»¥æš‚åœåç»§ç»­
- æ£€æŸ¥ç½‘ç»œè¿æ¥
- è€ƒè™‘ä½¿ç”¨ VPN æˆ–é•œåƒ

### Q: æ¨¡å‹ä¸æ˜¾ç¤ºï¼Ÿ

**A:**
```bash
# 1. æ£€æŸ¥ä¸‹è½½å®Œæˆ
ls ~/.lmstudio/models/

# 2. é‡å¯ LM Studio
killall "LM Studio"
open -a "LM Studio"

# 3. æ£€æŸ¥ç£ç›˜ç©ºé—´
df -h
```

### Q: ç”Ÿæˆé€Ÿåº¦æ…¢ï¼Ÿ

**A:**
1. ç¡®ä¿ GPU Offload = Max
2. å…³é—­å…¶ä»–åº”ç”¨ (Chrome, Docker)
3. ä½¿ç”¨ 4-bit æ¨¡å‹ (æœ€å¿«)
4. æ£€æŸ¥å†…å­˜å‹åŠ›:
   ```bash
   vm_stat | grep "Pages free"
   ```

### Q: API è¿æ¥å¤±è´¥ï¼Ÿ

**A:**
```bash
# 1. ç¡®è®¤æœåŠ¡å™¨è¿è¡Œ
curl http://localhost:1234/v1/models

# 2. æ£€æŸ¥ç«¯å£
lsof -i :1234

# 3. é‡å¯æœåŠ¡å™¨
# LM Studio -> Local Server -> Stop -> Start
```

### Q: å†…å­˜ä¸è¶³ï¼Ÿ

**A:**
- ä½¿ç”¨ 4-bit æ¨¡å‹ (~135GB)
- å…³é—­å…¶ä»–åº”ç”¨
- é‡å¯ Mac é‡Šæ”¾å†…å­˜
- æ£€æŸ¥: `Activity Monitor -> Memory`

---

## ğŸ“Š æ€§èƒ½å‚è€ƒ

### M3 Ultra 512GB é¢„æœŸæ€§èƒ½

| æ¨¡å‹ç‰ˆæœ¬ | å†…å­˜ | TPS | TTFT | æ¨èç”¨é€” |
|---------|------|-----|------|---------|
| MLX 4-bit | 135GB | 45.7 | 67ms | â­ æ—¥å¸¸å¯¹è¯/ç¼–ç¨‹ |
| MLX 8-bit | 252GB | 33.0 | 95ms | ğŸ“ æ–‡æ¡£ç”Ÿæˆ |
| GGUF Q4 | 140GB | ~40 | ~80ms | ğŸ”„ é€šç”¨ |
| GGUF Q8 | 250GB | ~30 | ~100ms | ğŸ¯ é«˜è´¨é‡è¾“å‡º |

**æœ¯è¯­è§£é‡Š:**
- **TPS**: Tokens Per Second (æ¯ç§’ç”Ÿæˆçš„ token æ•°)
- **TTFT**: Time To First Token (é¦–ä¸ª token å»¶è¿Ÿ)
- **å†…å­˜**: è¿è¡Œæ—¶å ç”¨çš„ç³»ç»Ÿå†…å­˜

---

## ğŸš€ è¿›é˜¶ä½¿ç”¨

### Python è„šæœ¬è°ƒç”¨

```python
import openai

# é…ç½® LM Studio
openai.api_base = "http://localhost:1234/v1"
openai.api_key = "lm-studio"

# å‘é€è¯·æ±‚
response = openai.ChatCompletion.create(
    model="minimax-m2.1",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹"},
        {"role": "user", "content": "è§£é‡Šé‡å­çº ç¼ "}
    ],
    max_tokens=500,
    temperature=0.7
)

print(response.choices[0].message.content)
```

### å‘½ä»¤è¡Œå¿«é€Ÿæµ‹è¯•

```bash
# æµ‹è¯•è„šæœ¬
cat > test_lmstudio.sh << 'EOF'
#!/bin/bash
curl -s http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d "{
    \"messages\": [{\"role\": \"user\", \"content\": \"$1\"}],
    \"max_tokens\": 500
  }" | python3 -c "import sys, json; print(json.load(sys.stdin)['choices'][0]['message']['content'])"
EOF

chmod +x test_lmstudio.sh

# ä½¿ç”¨
./test_lmstudio.sh "ä»€ä¹ˆæ˜¯é‡å­è®¡ç®—ï¼Ÿ"
```

### æ‰¹é‡å¤„ç†

```python
# batch_process.py
import openai
openai.api_base = "http://localhost:1234/v1"

questions = [
    "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
    "æ·±åº¦å­¦ä¹ çš„åº”ç”¨æœ‰å“ªäº›ï¼Ÿ",
    "å¦‚ä½•å¼€å§‹å­¦ä¹  AIï¼Ÿ"
]

for q in questions:
    response = openai.ChatCompletion.create(
        model="minimax-m2.1",
        messages=[{"role": "user", "content": q}],
        max_tokens=200
    )
    print(f"Q: {q}")
    print(f"A: {response.choices[0].message.content}\n")
```

---

## ğŸ“š æ›´å¤šèµ„æº

**æ–‡æ¡£:**
- å®Œæ•´è®¾ç½®æŒ‡å—: [docs/lm-studio-setup.md](docs/lm-studio-setup.md)
- OpenClaw é›†æˆ: [docs/openclaw-setup.md](docs/openclaw-setup.md)
- æ€§èƒ½æµ‹è¯•ç»“æœ: [docs/benchmark-results.md](docs/benchmark-results.md)

**é“¾æ¥:**
- LM Studio å®˜ç½‘: https://lmstudio.ai
- å®˜æ–¹æ–‡æ¡£: https://lmstudio.ai/docs
- Discord ç¤¾åŒº: https://discord.gg/lmstudio
- æ¨¡å‹ä»“åº“: https://huggingface.co/mlx-community

**å¤‡é€‰æ–¹æ¡ˆ:**
- å¦‚æœä½ åå¥½å‘½ä»¤è¡Œ: [MLX æ–¹å¼](QUICKSTART.md)
- é«˜çº§æµ‹è¯•å’Œä¼˜åŒ–: [Test Plan](docs/test-plan.md)

---

## âœ… æ£€æŸ¥æ¸…å•

å®Œæˆä»¥ä¸‹æ­¥éª¤ç¡®ä¿ä¸€åˆ‡æ­£å¸¸ï¼š

- [ ] LM Studio å·²å®‰è£…å¹¶å¯ä»¥æ‰“å¼€
- [ ] æ¨¡å‹å·²ä¸‹è½½å®Œæˆ (æ£€æŸ¥ ~/.lmstudio/models/)
- [ ] æ¨¡å‹å¯ä»¥åœ¨ Chat ç•Œé¢åŠ è½½
- [ ] å¯ä»¥ä¸æ¨¡å‹å¯¹è¯å¹¶æ”¶åˆ°å›å¤
- [ ] API æœåŠ¡å™¨å¯ä»¥å¯åŠ¨ (http://localhost:1234)
- [ ] curl æµ‹è¯•æˆåŠŸè¿”å›å“åº”
- [ ] OpenClaw é…ç½®å®Œæˆ (å¯é€‰)
- [ ] OpenClaw å¯ä»¥è°ƒç”¨æœ¬åœ°æ¨¡å‹ (å¯é€‰)

---

## ğŸ“‹ LMS CLI å¿«é€Ÿå‚è€ƒ

```bash
# æ¨¡å‹æ“ä½œ
lms models list                    # åˆ—å‡ºæœ¬åœ°æ¨¡å‹
lms models search minimax          # æœç´¢æ¨¡å‹
lms download <model-id>            # ä¸‹è½½æ¨¡å‹
lms models delete <model-id>       # åˆ é™¤æ¨¡å‹

# æœåŠ¡å™¨æ“ä½œ
lms server start                   # å¯åŠ¨ (ä½¿ç”¨æœ€è¿‘çš„æ¨¡å‹)
lms server start <model> --port 1234  # æŒ‡å®šæ¨¡å‹å’Œç«¯å£
lms server start --detach          # åå°è¿è¡Œ
lms server stop                    # åœæ­¢æœåŠ¡å™¨
lms server status                  # æŸ¥çœ‹çŠ¶æ€
lms server logs                    # æŸ¥çœ‹æ—¥å¿—
lms server restart                 # é‡å¯

# é…ç½®
lms config list                    # åˆ—å‡ºé…ç½®
lms config set server.port 1234    # è®¾ç½®ç«¯å£
lms config set gpu.layers -1       # è®¾ç½® GPU layers

# å·¥å…·
lms version                        # ç‰ˆæœ¬ä¿¡æ¯
lms doctor                         # è¯Šæ–­é—®é¢˜
```

---

**å‡†å¤‡å¥½äº†å—ï¼Ÿ**

**CLI ç”¨æˆ·:**
```bash
brew install --cask lm-studio && lms download mlx-community/MiniMax-M2.1-4bit
```

**GUI ç”¨æˆ·:**
```bash
open https://lmstudio.ai/download
```

ğŸ‰ å¼€å§‹ä½ çš„æœ¬åœ° AI ä¹‹æ—…ï¼
