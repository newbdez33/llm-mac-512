# OpenClaw + MLX æœ¬åœ°é…ç½®æŒ‡å—

> åœ¨æœ¬åœ°ä½¿ç”¨MLX MiniMax M2.1ä¸ºOpenClawæä¾›AIèƒ½åŠ›

## ğŸ“‹ ç›®å½•

- [æ¦‚è¿°](#æ¦‚è¿°)
- [å‰ç½®è¦æ±‚](#å‰ç½®è¦æ±‚)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [è¯¦ç»†é…ç½®](#è¯¦ç»†é…ç½®)
- [æµ‹è¯•éªŒè¯](#æµ‹è¯•éªŒè¯)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

---

## æ¦‚è¿°

è¿™ä¸ªæŒ‡å—å°†å¸®ä½ ï¼š
1. å¯åŠ¨æœ¬åœ°MLX APIæœåŠ¡å™¨
2. é…ç½®OpenClawä½¿ç”¨æœ¬åœ°API
3. éªŒè¯é…ç½®æ˜¯å¦æ­£å¸¸å·¥ä½œ

**æ¶æ„ï¼š**
```
OpenClaw â†’ æœ¬åœ°APIæœåŠ¡å™¨ (127.0.0.1:8000) â†’ MLX MiniMax M2.1
```

---

## å‰ç½®è¦æ±‚

### 1. MLXç¯å¢ƒå·²é…ç½®

```bash
# æ£€æŸ¥MLXæ˜¯å¦å·²å®‰è£…
source venv/bin/activate
python -c "import mlx_lm; print('MLX OK')"
```

å¦‚æœæœªå®‰è£…ï¼Œå‚è€ƒï¼š[docs/mlx-local-setup.md](./mlx-local-setup.md)

### 2. å®‰è£…APIæœåŠ¡å™¨ä¾èµ–

```bash
# æ¿€æ´»ç¯å¢ƒ
source venv/bin/activate

# å®‰è£…Flaskï¼ˆAPIæœåŠ¡å™¨ï¼‰
pip install flask flask-cors

# éªŒè¯å®‰è£…
python -c "import flask; print('Flask installed')"
```

### 3. OpenClawå·²å®‰è£…

å‚è€ƒï¼šhttps://openclaw.ai æˆ– https://github.com/openclaw/openclaw

---

## å¿«é€Ÿå¼€å§‹

### ç¬¬1æ­¥ï¼šå¯åŠ¨APIæœåŠ¡å™¨

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd ~/projects/llm-mac-512
source venv/bin/activate

# å¯åŠ¨APIæœåŠ¡å™¨ï¼ˆä½¿ç”¨4-bitæ¨¡å‹ï¼Œæœ€å¿«ï¼‰
python scripts/api_server.py
```

**é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡å‹ï¼ˆ~120GBï¼‰ï¼Œè¯·è€å¿ƒç­‰å¾…ï¼**

æœåŠ¡å™¨å¯åŠ¨åä¼šæ˜¾ç¤ºï¼š
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         MLX MiniMax M2.1 API Server                      â•‘
â•‘         OpenAI-Compatible API                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ“ æ¨¡å‹åŠ è½½å®Œæˆï¼ç”¨æ—¶ 21.25 ç§’

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
API æœåŠ¡å™¨é…ç½®
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
æ¨¡å‹: mlx-community/MiniMax-M2.1-4bit
åœ°å€: http://127.0.0.1:8000
ç«¯ç‚¹:
  â€¢ Chat: http://127.0.0.1:8000/v1/chat/completions
  â€¢ Completions: http://127.0.0.1:8000/v1/completions
  â€¢ Models: http://127.0.0.1:8000/v1/models
  â€¢ Health: http://127.0.0.1:8000/health

æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### ç¬¬2æ­¥ï¼šéªŒè¯APIå·¥ä½œ

æ‰“å¼€**æ–°ç»ˆç«¯**ï¼Œæµ‹è¯•APIï¼š

```bash
# æµ‹è¯•å¥åº·æ£€æŸ¥
curl http://127.0.0.1:8000/health

# æµ‹è¯•chat API
curl http://127.0.0.1:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mlx-community/MiniMax-M2.1-4bit",
    "messages": [{"role": "user", "content": "ä½ å¥½"}],
    "max_tokens": 100
  }'
```

### ç¬¬3æ­¥ï¼šé…ç½®OpenClaw

#### æ–¹å¼Aï¼šç¯å¢ƒå˜é‡ï¼ˆæ¨èï¼‰

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export OPENAI_API_BASE="http://127.0.0.1:8000/v1"
export OPENAI_API_KEY="sk-dummy"  # æœ¬åœ°APIä¸éœ€è¦çœŸå®key

# å¯åŠ¨OpenClaw
openclaw
```

#### æ–¹å¼Bï¼šé…ç½®æ–‡ä»¶

ç¼–è¾‘OpenClawé…ç½®æ–‡ä»¶ï¼ˆé€šå¸¸åœ¨ `~/.openclaw/config.yaml` æˆ–ç±»ä¼¼ä½ç½®ï¼‰ï¼š

```yaml
llm:
  provider: openai
  base_url: http://127.0.0.1:8000/v1
  api_key: sk-dummy  # æœ¬åœ°APIä¸éœ€è¦çœŸå®key
  model: mlx-community/MiniMax-M2.1-4bit
```

#### æ–¹å¼Cï¼šå‘½ä»¤è¡Œå‚æ•°

```bash
openclaw \
  --llm-provider openai \
  --llm-base-url http://127.0.0.1:8000/v1 \
  --llm-api-key sk-dummy \
  --llm-model mlx-community/MiniMax-M2.1-4bit
```

### ç¬¬4æ­¥ï¼šæµ‹è¯•OpenClaw

åœ¨OpenClawä¸­æµ‹è¯•ï¼š

```
> ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±
> å†™ä¸€ä¸ªPythonå¿«é€Ÿæ’åºç®—æ³•
> å¸®æˆ‘åˆ†æä¸€ä¸‹å½“å‰ç›®å½•çš„æ–‡ä»¶
```

---

## è¯¦ç»†é…ç½®

### APIæœåŠ¡å™¨é€‰é¡¹

```bash
# ä½¿ç”¨ä¸åŒçš„æ¨¡å‹
python scripts/api_server.py --model mlx-community/MiniMax-M2.1-8bit

# æ›´æ”¹ç«¯å£
python scripts/api_server.py --port 8080

# å…è®¸å¤–éƒ¨è®¿é—®ï¼ˆè°¨æ…ä½¿ç”¨ï¼ï¼‰
python scripts/api_server.py --host 0.0.0.0 --port 8000

# å®Œæ•´ç¤ºä¾‹
python scripts/api_server.py \
  --model mlx-community/MiniMax-M2.1-4bit \
  --host 127.0.0.1 \
  --port 8000
```

### OpenClawé…ç½®ç¤ºä¾‹

**å®Œæ•´çš„config.yamlç¤ºä¾‹ï¼š**

```yaml
# ~/.openclaw/config.yaml

# LLMé…ç½®
llm:
  provider: openai
  base_url: http://127.0.0.1:8000/v1
  api_key: sk-dummy
  model: mlx-community/MiniMax-M2.1-4bit
  temperature: 0.7
  max_tokens: 2000

# OpenClawå…¶ä»–é…ç½®
agent:
  name: "MiniMaxåŠ©æ‰‹"
  personality: "helpful and concise"

# å·¥å…·é…ç½®
tools:
  enabled:
    - shell
    - file_system
    - web_search
```

---

## æµ‹è¯•éªŒè¯

### 1. APIå¥åº·æ£€æŸ¥

```bash
curl http://127.0.0.1:8000/health
```

æœŸæœ›è¾“å‡ºï¼š
```json
{
  "status": "ok",
  "model": "mlx-community/MiniMax-M2.1-4bit",
  "model_loaded": true
}
```

### 2. æµ‹è¯•Chat API

```bash
curl http://127.0.0.1:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mlx-community/MiniMax-M2.1-4bit",
    "messages": [
      {"role": "user", "content": "è¯·ç”¨ä¸€å¥è¯è§£é‡Šé‡å­è®¡ç®—"}
    ],
    "max_tokens": 100,
    "temperature": 0.7
  }'
```

### 3. æµ‹è¯•Completions API

```bash
curl http://127.0.0.1:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mlx-community/MiniMax-M2.1-4bit",
    "prompt": "å†™ä¸€ä¸ªPythonå†’æ³¡æ’åºï¼š",
    "max_tokens": 300
  }'
```

### 4. åˆ—å‡ºæ¨¡å‹

```bash
curl http://127.0.0.1:8000/v1/models
```

### 5. Pythonæµ‹è¯•è„šæœ¬

åˆ›å»º `test_api.py`ï¼š

```python
import requests

BASE_URL = "http://127.0.0.1:8000/v1"

# æµ‹è¯•chat
response = requests.post(
    f"{BASE_URL}/chat/completions",
    json={
        "model": "mlx-community/MiniMax-M2.1-4bit",
        "messages": [
            {"role": "user", "content": "ä½ å¥½"}
        ],
        "max_tokens": 100
    }
)

print("Status:", response.status_code)
print("Response:", response.json())
```

è¿è¡Œï¼š
```bash
python test_api.py
```

---

## æ€§èƒ½ä¼˜åŒ–

### 1. é€‰æ‹©åˆé€‚çš„æ¨¡å‹

```bash
# é€Ÿåº¦ä¼˜å…ˆï¼ˆæ¨èOpenClawä½¿ç”¨ï¼‰
python scripts/api_server.py --model mlx-community/MiniMax-M2.1-4bit

# è´¨é‡ä¼˜å…ˆ
python scripts/api_server.py --model mlx-community/MiniMax-M2.1-8bit

# å¹³è¡¡
python scripts/api_server.py --model mlx-community/MiniMax-M2.1-6bit
```

**æ¨èï¼š** å¯¹äºOpenClawï¼Œä½¿ç”¨4-bitæ¨¡å‹ï¼ˆ45 TPSï¼‰ï¼Œå“åº”å¿«é€Ÿã€‚

### 2. ä¼˜åŒ–VRAMï¼ˆå¯é€‰ï¼‰

```bash
# å¢åŠ GPUå¯ç”¨å†…å­˜
sudo sysctl iogpu.wired_limit_mb=458752

# å¯åŠ¨APIæœåŠ¡å™¨
python scripts/api_server.py
```

### 3. è°ƒæ•´OpenClawå‚æ•°

```yaml
llm:
  temperature: 0.7      # é»˜è®¤ï¼Œå¹³è¡¡
  # temperature: 0.5   # æ›´ç¡®å®šæ€§
  # temperature: 0.9   # æ›´æœ‰åˆ›æ„

  max_tokens: 1000      # é€‚ä¸­
  # max_tokens: 2000   # é•¿å›å¤
  # max_tokens: 500    # å¿«é€Ÿå›å¤
```

---

## æ•…éšœæ’é™¤

### é—®é¢˜1ï¼šAPIæœåŠ¡å™¨æ— æ³•å¯åŠ¨

**ç—‡çŠ¶ï¼š** `ModuleNotFoundError: No module named 'flask'`

**è§£å†³ï¼š**
```bash
source venv/bin/activate
pip install flask flask-cors
```

### é—®é¢˜2ï¼šOpenClawè¿æ¥å¤±è´¥

**ç—‡çŠ¶ï¼š** `Connection refused` æˆ– `Connection timeout`

**æ£€æŸ¥æ¸…å•ï¼š**
```bash
# 1. ç¡®è®¤APIæœåŠ¡å™¨æ­£åœ¨è¿è¡Œ
curl http://127.0.0.1:8000/health

# 2. æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨
lsof -i :8000

# 3. æŸ¥çœ‹æœåŠ¡å™¨æ—¥å¿—
# åœ¨è¿è¡Œapi_server.pyçš„ç»ˆç«¯æŸ¥çœ‹é”™è¯¯ä¿¡æ¯

# 4. éªŒè¯OpenClawé…ç½®
cat ~/.openclaw/config.yaml
```

### é—®é¢˜3ï¼šå“åº”å¾ˆæ…¢

**ç—‡çŠ¶ï¼š** å“åº”æ—¶é—´ > 10ç§’

**è§£å†³ï¼š**
```bash
# 1. ä½¿ç”¨4-bitæ¨¡å‹
python scripts/api_server.py --model mlx-community/MiniMax-M2.1-4bit

# 2. å‡å°‘max_tokens
# åœ¨OpenClawé…ç½®ä¸­è®¾ç½® max_tokens: 500

# 3. æ£€æŸ¥ç³»ç»Ÿèµ„æº
# å…³é—­å…¶ä»–å ç”¨å†…å­˜çš„åº”ç”¨

# 4. ä¼˜åŒ–VRAM
sudo sysctl iogpu.wired_limit_mb=458752
```

### é—®é¢˜4ï¼šæ¨¡å‹è¾“å‡ºè´¨é‡å·®

**ç—‡çŠ¶ï¼š** å›å¤ä¸è¿è´¯æˆ–æ— æ„ä¹‰

**è§£å†³ï¼š**

1. è°ƒæ•´temperatureï¼š
```yaml
llm:
  temperature: 0.7  # å°è¯•0.5-0.9ä¹‹é—´
```

2. ä½¿ç”¨æ›´é«˜bitçš„æ¨¡å‹ï¼š
```bash
python scripts/api_server.py --model mlx-community/MiniMax-M2.1-8bit
```

### é—®é¢˜5ï¼šAPIè¿”å›401é”™è¯¯

**ç—‡çŠ¶ï¼š** `Unauthorized` æˆ– `Invalid API key`

**è§£å†³ï¼š**
æœ¬åœ°APIä¸éœ€è¦çœŸå®keyï¼Œä½¿ç”¨ä»»æ„å€¼å³å¯ï¼š
```bash
export OPENAI_API_KEY="sk-dummy"
```

æˆ–åœ¨OpenClawé…ç½®ä¸­ï¼š
```yaml
llm:
  api_key: sk-anything-works
```

---

## é«˜çº§ä½¿ç”¨

### 1. åå°è¿è¡ŒAPIæœåŠ¡å™¨

```bash
# ä½¿ç”¨nohupåå°è¿è¡Œ
nohup python scripts/api_server.py > api_server.log 2>&1 &

# æŸ¥çœ‹æ—¥å¿—
tail -f api_server.log

# åœæ­¢æœåŠ¡å™¨
pkill -f api_server.py
```

### 2. ä½¿ç”¨systemdæœåŠ¡ï¼ˆmacOSä½¿ç”¨launchdï¼‰

åˆ›å»º `~/Library/LaunchAgents/com.mlx.api.plist`ï¼š

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
    <key>Label</key>
    <string>com.mlx.api</string>
    <key>ProgramArguments</key>
    <array>
        <string>/Users/jacky/projects/llm-mac-512/venv/bin/python</string>
        <string>/Users/jacky/projects/llm-mac-512/scripts/api_server.py</string>
    </array>
    <key>RunAtLoad</key>
    <true/>
    <key>KeepAlive</key>
    <true/>
    <key>StandardOutPath</key>
    <string>/tmp/mlx-api.log</string>
    <key>StandardErrorPath</key>
    <string>/tmp/mlx-api.error.log</string>
</dict>
</plist>
```

åŠ è½½æœåŠ¡ï¼š
```bash
launchctl load ~/Library/LaunchAgents/com.mlx.api.plist
```

### 3. ç›‘æ§APIæ€§èƒ½

```bash
# æŸ¥çœ‹è¯·æ±‚æ—¥å¿—
# APIæœåŠ¡å™¨ä¼šåœ¨ç»ˆç«¯æ˜¾ç¤ºæ¯ä¸ªè¯·æ±‚

# ç›‘æ§ç³»ç»Ÿèµ„æº
watch -n 1 'ps aux | grep api_server'

# æŸ¥çœ‹å†…å­˜ä½¿ç”¨
memory_pressure
```

---

## æ€§èƒ½å‚è€ƒ

**ä½ çš„ç³»ç»Ÿï¼ˆM3 Ultra 512GBï¼‰+ MLX 4-bitï¼š**

| æŒ‡æ ‡ | æ€§èƒ½ |
|------|------|
| æ¨¡å‹åŠ è½½æ—¶é—´ | ~21ç§’ |
| TTFT (é¦–token) | 67ms |
| ç”Ÿæˆé€Ÿåº¦ | 45.73 tokens/sec |
| å†…å­˜å ç”¨ | 135 GB |
| å¹¶å‘èƒ½åŠ› | 1-4ä¸ªè¯·æ±‚ |

**OpenClawä½¿ç”¨ä½“éªŒï¼š**
- å“åº”è¿…é€Ÿï¼Œæ¥è¿‘äº‘APIä½“éªŒ
- æ— ç½‘ç»œå»¶è¿Ÿ
- å®Œå…¨ç§å¯†ï¼Œæ•°æ®ä¸å‡ºæœ¬åœ°
- æ— APIè´¹ç”¨

---

## å¸¸è§é—®é¢˜ FAQ

### Q1: å¯ä»¥åŒæ—¶è¿æ¥å¤šä¸ªOpenClawå®ä¾‹å—ï¼Ÿ

**A:** å¯ä»¥ï¼ŒAPIæœåŠ¡å™¨æ”¯æŒå¹¶å‘è¯·æ±‚ã€‚ä½†æ€§èƒ½ä¼šéšå¹¶å‘æ•°ä¸‹é™ã€‚

### Q2: èƒ½å¦ä½¿ç”¨å…¶ä»–ç«¯å£ï¼Ÿ

**A:** å¯ä»¥ï¼š
```bash
python scripts/api_server.py --port 8080
```
ç„¶ååœ¨OpenClawä¸­é…ç½® `base_url: http://127.0.0.1:8080/v1`

### Q3: æ˜¯å¦æ”¯æŒæµå¼å“åº”ï¼Ÿ

**A:** APIåŒ…å«åŸºç¡€çš„æµå¼æ”¯æŒï¼Œä½†å¯èƒ½éœ€è¦æ ¹æ®OpenClawçš„å…·ä½“éœ€æ±‚è°ƒæ•´ã€‚

### Q4: å¦‚ä½•æŸ¥çœ‹APIæ—¥å¿—ï¼Ÿ

**A:** APIæœåŠ¡å™¨ä¼šåœ¨ç»ˆç«¯å®æ—¶æ˜¾ç¤ºè¯·æ±‚æ—¥å¿—ã€‚

### Q5: å¯ä»¥è¿œç¨‹è®¿é—®å—ï¼Ÿ

**A:** å¯ä»¥ï¼Œä½†**ä¸æ¨è**ï¼ˆå®‰å…¨é£é™©ï¼‰ã€‚å¦‚éœ€è¦ï¼š
```bash
python scripts/api_server.py --host 0.0.0.0
```
ç„¶åé…ç½®é˜²ç«å¢™å’Œè®¤è¯ã€‚

---

## ä¸‹ä¸€æ­¥

1. âœ… å¯åŠ¨APIæœåŠ¡å™¨
2. âœ… é…ç½®OpenClaw
3. âœ… æµ‹è¯•åŸºæœ¬åŠŸèƒ½
4. ğŸ“Š ç›‘æ§æ€§èƒ½å’Œä¼˜åŒ–
5. ğŸš€ äº«å—æœ¬åœ°AIåŠ©æ‰‹ï¼

---

## ç›¸å…³èµ„æº

- **APIæœåŠ¡å™¨è„šæœ¬ï¼š** `scripts/api_server.py`
- **MLXè®¾ç½®æŒ‡å—ï¼š** `docs/mlx-local-setup.md`
- **æ€§èƒ½æµ‹è¯•ç»“æœï¼š** `docs/benchmark-results.md`
- **OpenClawå®˜ç½‘ï¼š** https://openclaw.ai
- **OpenClawæ–‡æ¡£ï¼š** https://docs.openclaw.ai

---

**ç¥ä½¿ç”¨æ„‰å¿«ï¼å¦‚æœ‰é—®é¢˜ï¼ŒæŸ¥çœ‹æ•…éšœæ’é™¤éƒ¨åˆ†æˆ–æ£€æŸ¥æ—¥å¿—ã€‚**
