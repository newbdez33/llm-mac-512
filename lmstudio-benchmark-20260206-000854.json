{
  "timestamp": "2026-02-06T00:08:54.083758",
  "api_base": "http://localhost:1234/v1",
  "model": "qwen3-coder-next",
  "summary": {
    "avg_tps": 33.91636745903371,
    "max_tps": 36.81690327083504,
    "min_tps": 27.27751448901148,
    "total_tokens": 3321,
    "total_time": 92.21513986587524
  },
  "results": [
    {
      "test_name": "short",
      "name": "短文本生成",
      "prompt": "请用一句话解释量子计算",
      "max_tokens": 100,
      "prompt_tokens": 14,
      "completion_tokens": 64,
      "total_time": 2.346254825592041,
      "tps": 27.27751448901148,
      "content": "量子计算是一种利用量子力学原理（如叠加态和纠缠）来编码、处理和传输信息的新型计算范式，其基本单元是量子比特（qubit），可同时表示0和1的叠加态，从而在特定问题上实现远超经典计算机的并行计算能力。",
      "success": true
    },
    {
      "test_name": "medium",
      "name": "代码生成",
      "prompt": "写一个Python快速排序算法，包含完整注释",
      "max_tokens": 500,
      "prompt_tokens": 19,
      "completion_tokens": 500,
      "total_time": 13.894958972930908,
      "tps": 35.98427321549215,
      "content": "以下是一个带有详细注释的 Python 快速排序（Quick Sort）实现。该实现采用 **原地（in-place）、递归、Lomuto 分区方案**，时间复杂度平均为 O(n log n)，最坏情况为 O(n²)。\n\n```python\ndef quick_sort(arr, low=0, high=None):\n    \"\"\"\n    快速排序主函数：对列表 arr 的 [low, high] 区间进行原地排序（升序）\n    \n    参数：\n        arr: 待排序的可变序列（如 list），将被原地修改\n        low: 当前分区的起始索引（默认 0）\n        high: 当前分区的结束索引（默认 None，表示为 len(arr) - 1）\n    \n    返回值：无（直接修改原列表）\n    \n    时间复杂度：\n        平均：O(n log n)\n        最坏：O(n²)（例如：每次选到极端枢轴，如已排序数组且总选首/尾元素为枢轴）\n    空间复杂度：O(log n)（递归栈深度平均为 O(log n)，最坏为 O(n)）\n    \n    注意：\n        - 本实现使用 Lomuto 分区方案（以末尾元素为枢轴）\n        - 不稳定排序（相同元素的相对顺序可能改变）\n        - 适用于原地排序场景；若需稳定排序，建议用归并排序\n    \"\"\"\n    \n    # 若 high 未指定，则初始化为最后一个索引\n    if high is None:\n        high = len(arr) - 1\n    \n    # 基本情况：区间长度 ≤ 1 时已有序（递归终止条件）\n    if low < high:\n        # 【分区操作】将 arr[low..high] 划分为两部分：\n        #   左半部分：所有元素 < 枢轴\n        #   右半部分：所有元素 ≥ 枢轴\n        # 并返回枢轴的最终位置索引\n        pivot_index = partition(arr, low, high)\n        \n        # 【递归排序】分别对枢轴左右两个子区间进行快速排序\n        quick_sort(arr, low, pivot_index - 1)   # 排序左半部分：[low, pivot_index-",
      "success": true
    },
    {
      "test_name": "long",
      "name": "长文本生成",
      "prompt": "详细解释深度学习的反向传播算法，包括数学推导过程",
      "max_tokens": 2000,
      "prompt_tokens": 23,
      "completion_tokens": 2000,
      "total_time": 54.32287406921387,
      "tps": 36.81690327083504,
      "content": "深度学习中的**反向传播（Backpropagation）**是训练神经网络的核心算法，用于高效计算损失函数对各参数（权重和偏置）的梯度，从而通过梯度下降等优化方法更新参数。其本质是**链式法则（Chain Rule）**在计算图上的应用。\n\n下面我将从直观理解、数学推导、具体示例三部分详细展开：\n\n---\n\n## 一、核心思想与背景\n\n### 1. 问题设定\n考虑一个前馈神经网络（如多层感知机 MLP），输入为向量 \\( \\mathbf{x} \\in \\mathbb{R}^n \\)，输出为 \\( \\hat{\\mathbf{y}} = f(\\mathbf{x}; \\theta) \\)，其中 \\( \\theta \\) 是所有可学习参数（权重矩阵 \\( W^{(l)} \\)、偏置向量 \\( \\mathbf{b}^{(l)} \\)）的集合。\n\n给定损失函数 \\( L(\\hat{\\mathbf{y}}, \\mathbf{y}) \\)，目标是计算：\n\\[\n\\frac{\\partial L}{\\partial W_{ij}^{(l)}}, \\quad \\frac{\\partial L}{\\partial b_i^{(l)}} \\quad \\text{for all layers } l\n\\]\n\n直接展开计算复杂度为 \\( O(|\\theta| \\cdot d) \\)，其中 \\( d \\) 是前向传播的计算步骤数；而反向传播可在 **一次前向 + 一次反向** 中以线性复杂度 \\( O(d) \\) 同时得到所有梯度（忽略常数因子）。\n\n---\n\n## 二、数学推导：从单样本出发\n\n### 1. 网络结构定义（第 \\( l \\) 层）\n设网络有 \\( L \\) 层（输出层为第 \\( L \\) 层），则：\n\n- 输入层：\\( \\mathbf{a}^{(0)} = \\mathbf{x} \\)\n- 隐层激活前的线性组合：\n  \\[\n  \\mathbf{z}^{(l)} = W^{(l)} \\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)}, \\quad l = 1,2,\\dots,L\n  \\]\n- 激活后输出：\n  \\[\n  \\mathbf{a}^{(l)} = g^{(l)}(\\mathbf{z}^{(l)})\n  \\]\n  其中 \\( g^{(l)} \\) 是第 \\( l \\) 层的激活函数（如 ReLU、sigmoid），逐元素作用。\n\n最终输出为 \\( \\hat{\\mathbf{y}} = \\mathbf{a}^{(L)} \\)，损失为 \\( L = \\ell(\\mathbf{a}^{(L)}, \\mathbf{y}) \\)。\n\n---\n\n### 2. 链式法则的递归应用\n\n目标：求各层的梯度。定义**误差项（error term）**：\n\\[\n\\boldsymbol{\\delta}^{(l)} \\triangleq \\frac{\\partial L}{\\partial \\mathbf{z}^{(l)}} \\in \\mathbb{R}^{n_l}\n\\]\n即损失对第 \\( l \\) 层线性输出的梯度（向量）。\n\n#### 关键递推关系：\n我们分两步推导：\n\n---\n\n#### **Step 1：输出层误差 \\( \\boldsymbol{\\delta}^{(L)} \\)**\n\n由链式法则：\n\\[\n\\boldsymbol{\\delta}^{(L)} = \\frac{\\partial L}{\\partial \\mathbf{z}^{(L)}} \n= \\frac{\\partial L}{\\partial \\mathbf{a}^{(L)}} \\odot \\frac{\\partial \\mathbf{a}^{(L)}}{\\partial \\mathbf{z}^{(L)}}\n\\]\n其中：\n- \\( \\frac{\\partial L}{\\partial \\mathbf{a}^{(L)}} \\) 是损失对输出的梯度（取决于损失函数）\n- \\( \\frac{\\partial \\mathbf{a}^{(L)}}{\\partial \\mathbf{z}^{(L)}} = \\mathrm{diag}\\left( (g^{(L)})'(\\mathbf{z}^{(L)}) \\right) \\)，即激活函数导数的对角矩阵\n\n因此：\n\\[\n\\boxed{\n\\boldsymbol{\\delta}^{(L)} = \\nabla_{\\mathbf{a}} L \\odot (g^{(L)})'(\\mathbf{z}^{(L)})\n}\n\\]\n\n**常见例子：**\n- 若 \\( L = \\frac{1}{2}\\|\\mathbf{a}^{(L)} - \\mathbf{y}\\|^2 \\)（MSE），则 \\( \\nabla_{\\mathbf{a}} L = \\mathbf{a}^{(L)} - \\mathbf{y} \\)\n- 若输出层激活为 sigmoid，\\( g'(z) = g(z)(1-g(z)) = a(1-a) \\)\n\n---\n\n#### **Step 2：隐层误差 \\( \\boldsymbol{\\delta}^{(l)} \\)（从后向前递推）**\n\n考虑第 \\( l \\) 层（\\( l < L \\)），注意：\n\\[\n\\mathbf{z}^{(l+1)} = W^{(l+1)} \\mathbf{a}^{(l)} + \\mathbf{b}^{(l+1)}, \\quad\n\\mathbf{a}^{(l)} = g^{(l)}(\\mathbf{z}^{(l)})\n\\]\n\n由链式法则：\n\\[\n\\boldsymbol{\\delta}^{(l)} = \\frac{\\partial L}{\\partial \\mathbf{z}^{(l)}} \n= \\frac{\\partial L}{\\partial \\mathbf{a}^{(l)}} \\odot \\frac{\\partial \\mathbf{a}^{(l)}}{\\partial \\mathbf{z}^{(l)}}\n\\]\n\n而 \\( \\frac{\\partial L}{\\partial \\mathbf{a}^{(l)}} \\) 可通过下一层反向传播得到：\n\\[\n\\frac{\\partial L}{\\partial \\mathbf{a}^{(l)}} \n= \\frac{\\partial \\mathbf{z}^{(l+1)}}{\\partial \\mathbf{a}^{(l)}}^\\top \\frac{\\partial L}{\\partial \\mathbf{z}^{(l+1)}}\n= \\left(W^{(l+1)}\\right)^\\top \\boldsymbol{\\delta}^{(l+1)}\n\\]\n\n因为 \\( \\mathbf{z}^{(l+1)} = W^{(l+1)}\\mathbf{a}^{(l)} + \\cdots \\)，所以雅可比矩阵为 \\( \\frac{\\partial \\mathbf{z}^{(l+1)}}{\\partial \\mathbf{a}^{(l)}} = W^{(l+1)} \\)。\n\n因此：\n\\[\n\\boxed{\n\\boldsymbol{\\delta}^{(l)} = \\left( (W^{(l+1)})^\\top \\boldsymbol{\\delta}^{(l+1)} \\right) \\odot (g^{(l)})'(\\mathbf{z}^{(l)})\n}\n\\]\n\n---\n\n### 3. 梯度计算：从误差项到参数梯度\n\n现在我们有 \\( \\boldsymbol{\\delta}^{(l)} = \\frac{\\partial L}{\\partial \\mathbf{z}^{(l)}} \\)，而：\n\\[\n\\mathbf{z}^{(l)} = W^{(l)} \\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)}\n\\]\n\n因此：\n\\[\n\\frac{\\partial L}{\\partial W_{ij}^{(l)}} \n= \\sum_k \\frac{\\partial L}{\\partial z_i^{(l)}} \\cdot \\frac{\\partial z_i^{(l)}}{\\partial W_{ij}^{(l)}}\n= \\delta_i^{(l)} \\cdot a_j^{(l-1)}\n\\]\n\n即矩阵形式：\n\\[\n\\boxed{\n\\frac{\\partial L}{\\partial W^{(l)}} = \\boldsymbol{\\delta}^{(l)} (\\mathbf{a}^{(l-1)})^\\top\n}\n\\]\n\\[\n\\boxed{\n\\frac{\\partial L}{\\partial \\mathbf{b}^{(l)}} = \\boldsymbol{\\delta}^{(l)}\n}\n\\]\n\n> ✅ 注意：这里对单个样本。若批量训练（batch size \\( m \\)），则梯度为所有样本的平均或求和：\n> \\[\n> \\frac{\\partial L}{\\partial W^{(l)}} = \\frac{1}{m} \\sum_{k=1}^m \\boldsymbol{\\delta}^{(l)}^{(k)} (\\mathbf{a}^{(l-1)}^{(k)})^\\top\n> \\]\n\n---\n\n## 三、完整反",
      "success": true
    },
    {
      "test_name": "reasoning",
      "name": "推理能力",
      "prompt": "有5个人排队，已知：A不在第一位，B在C前面，D紧挨着E，A在D后面。请推理出他们的排列顺序。",
      "max_tokens": 500,
      "prompt_tokens": 43,
      "completion_tokens": 500,
      "total_time": 14.103589057922363,
      "tps": 35.45196885321447,
      "content": "我们来逐步推理这道逻辑题。\n\n题目条件：\n\n1. A 不在第一位；  \n2. B 在 C 前面（即 B 的位置编号 < C 的位置编号）；  \n3. D 紧挨着 E（即 DE 或 ED 相邻）；  \n4. A 在 D 后面（即 A 的位置编号 > D 的位置编号）。\n\n我们有 5 个人：A、B、C、D、E，排成一列，位置编号为 1~5（从左到右）。\n\n---\n\n### 第一步：分析条件3：D 和 E 必须相邻  \n所有可能的相邻对位置（D/E 或 E/D）有：\n- (1,2)\n- (2,1)\n- (2,3)\n- (3,2)\n- (3,4)\n- (4,3)\n- (4,5)\n- (5,4)\n\n即 D 和 E 占据的可能位置组合为：{1&2, 2&3, 3&4, 4&5}（不考虑顺序时），每组有两种排列。\n\n### 第二步：条件4：A 在 D 后面 ⇒ pos(A) > pos(D)\n\n所以 A 的位置必须比 D 大，即 D 不可能在第5位（否则 A 没地方站）；  \n同样，若 D 在第4位，则 A 只能在第5位；  \nD 在第3位 → A 可在 4 或 5；  \nD 在第2位 → A 可在 3、4、5；  \nD 在第1位 → A 可在 2~5。\n\n但结合条件1：A ≠ 第1位（已满足，只要 A 不在1即可）。\n\n### 第三步：尝试枚举 D/E 的可能相邻位置，并结合 A > D。\n\n我们来逐个测试可能的 (D, E) 相邻组合，并尝试安排 A、B、C。\n\n#### 情况1：D 和 E 在 (1,2)\n\n- 子情况1a：D=1, E=2  \n  → A > 1 ⇒ A ∈ {3,4,5}  \n  剩余位置：3,4,5 给 A、B、C  \n",
      "success": true
    },
    {
      "test_name": "instruction",
      "name": "指令跟随",
      "prompt": "请完成以下任务：\n1. 列出3种常见的排序算法\n2. 为每种算法写出时间复杂度\n3. 用一句话总结它们的适用场景",
      "max_tokens": 400,
      "prompt_tokens": 46,
      "completion_tokens": 257,
      "total_time": 7.5474629402160645,
      "tps": 34.05117746661539,
      "content": "以下是3种常见排序算法及其相关信息：\n\n1. **冒泡排序（Bubble Sort）**  \n   - **时间复杂度**：平均/最坏 O(n²)，最好 O(n)（已有序时）  \n   - **适用场景**：适用于小规模数据或基本有序的数据，常用于教学演示排序原理。\n\n2. **快速排序（Quick Sort）**  \n   - **时间复杂度**：平均 O(n log n)，最坏 O(n²)（如基准选择不当），空间复杂度 O(log n)（递归栈）  \n   - **适用场景**：适用于大多数通用场景，尤其是大规模无序数据；是实际工程中最常用的高效排序算法之一（常配合随机化或三数取中优化）。\n\n3. **归并排序（Merge Sort）**  \n   - **时间复杂度**：所有情况均为 O(n log n)，稳定排序；空间复杂度 O(n)  \n   - **适用场景**：适用于对稳定性有要求的场景，以及外部排序（如处理超大数据集需分块读写磁盘）。\n\n> ✅ 补充说明：若需进一步比较堆排序、计数排序等其他算法，也可提供。",
      "success": true
    }
  ]
}