# Mac 512GB å¤§æ¨¡å‹æ€§èƒ½æµ‹è¯•è®¡åˆ’ v2

> åˆ›å»ºæ—¥æœŸ: 2026-02-04
> æµ‹è¯•å¹³å°: LM Studio (ç»Ÿä¸€æµ‹è¯•æ¡†æ¶)

## æµ‹è¯•ç›®æ ‡

åœ¨ Mac (512GB ç»Ÿä¸€å†…å­˜) ä¸Šå…¨é¢æµ‹è¯•ä¸¤æ¬¾å¤§å‹ MoE æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚

## æµ‹è¯•æ¨¡å‹

### 1. MiniMax M2.1
- **å‚æ•°**: 230B æ€»å‚æ•° / 10B æ¿€æ´»å‚æ•° (MoE)
- **å‘å¸ƒ**: 2025å¹´12æœˆ23æ—¥
- **ç‰¹ç‚¹**: ä»£ç ç”Ÿæˆã€å·¥å…·ä½¿ç”¨ã€æŒ‡ä»¤è·Ÿéšã€é•¿æœŸè§„åˆ’
- **Context**: 196,608 tokens

### 2. Qwen3-Coder-Next
- **å‚æ•°**: 80B æ€»å‚æ•° / 3B æ¿€æ´»å‚æ•° (MoE)
- **å‘å¸ƒ**: 2026å¹´2æœˆ3æ—¥
- **ç‰¹ç‚¹**: ä¸“ä¸ºç¼–ç¨‹ä»£ç†å’Œæœ¬åœ°å¼€å‘è®¾è®¡
- **Context**: 256,000 tokens
- **æ¶æ„**: 512 experts, 10+1 active per token

## æµ‹è¯•æ¡†æ¶

**ğŸ¯ ç»Ÿä¸€æµ‹è¯•å¹³å°: LM Studio (å…¬å¹³å¯¹æ¯”)**

| åç«¯æ¡†æ¶ | æ¨¡å‹æ ¼å¼ | åŠ è½½æ–¹å¼ | æµ‹è¯•è„šæœ¬ | è¯´æ˜ |
|---------|----------|----------|----------|------|
| **MLX Backend** | MLX æ ¼å¼ | LM Studio | benchmark_lmstudio.py | Apple Silicon åŸç”Ÿ |
| **llama.cpp Backend** | GGUF æ ¼å¼ | LM Studio | benchmark_lmstudio.py | é€šç”¨é‡åŒ–æ ¼å¼ |

**å…³é”®**: éƒ½é€šè¿‡ LM Studio API æµ‹è¯•ï¼Œç¡®ä¿å…¬å¹³å¯¹æ¯”

**æµ‹è¯•å·®å¼‚**:
- âœ… æµ‹è¯•çš„æ˜¯ï¼šMLX backend vs llama.cpp backend (éƒ½åœ¨ LM Studio å†…)
- âŒ ä¸æ˜¯ï¼šnative mlx-lm vs LM Studio (è¿™æ ·ä¸å…¬å¹³ï¼ŒAPI æœ‰å¼€é”€)

**ä¸ºä»€ä¹ˆè¿™æ ·å…¬å¹³**:
- ç›¸åŒçš„ API æ¥å£å¼€é”€
- ç›¸åŒçš„è¯·æ±‚/å“åº”å¤„ç†
- æ’é™¤æ¡†æ¶å¤–éƒ¨å› ç´ 
- çœŸå®ç”Ÿäº§åœºæ™¯ (éƒ½æ˜¯é€šè¿‡ API è°ƒç”¨)

## æµ‹è¯•çŸ©é˜µ

### MiniMax M2.1

#### Phase 1A: MLX ç‰ˆæœ¬æµ‹è¯• (åŸç”Ÿæ¡†æ¶)

| ç‰ˆæœ¬ | å¤§å° | å†…å­˜å ç”¨ | ä¼˜å…ˆçº§ | çŠ¶æ€ | åŸºå‡†æ€§èƒ½ | æµ‹è¯•æ–¹æ³• |
|------|------|----------|--------|------|----------|----------|
| **mlx-4bit** | ~120GB | ~135GB | ğŸ”¥ 1 | âœ… å·²æµ‹ | 45.73 TPS | mlx-lm |
| **mlx-6bit** | ~180GB | ~198GB | ğŸ”¥ 2 | âœ… å·²æµ‹ | 39.01 TPS | mlx-lm |
| **mlx-8bit** | ~240GB | ~252GB | ğŸ”¥ 3 | âœ… å·²æµ‹ | 33.04 TPS | mlx-lm |
| **mlx-bf16** | ~460GB | ~478GB | 4 | âŒ ä¸å¯ç”¨ | - | æ— å®˜æ–¹ç‰ˆæœ¬ |

**æ³¨**: å·²æœ‰å½’æ¡£æ•°æ®ï¼Œå¯ç›´æ¥ä½¿ç”¨æˆ–é‡æ–°æµ‹è¯•éªŒè¯

#### Phase 1B: llama.cpp ç‰ˆæœ¬æµ‹è¯• (GGUF via LM Studio)

| ç‰ˆæœ¬ | å¤§å° | å†…å­˜å ç”¨ | ä¼˜å…ˆçº§ | çŠ¶æ€ | å¯¹æ¯”MLX | æµ‹è¯•æ–¹æ³• |
|------|------|----------|--------|------|---------|----------|
| **Q4_K_S** | 130GB | ~135GB | ğŸ”¥ 1 | ğŸ”„ è¿›è¡Œä¸­ | vs mlx-4bit | LM Studio |
| **Q4_K_M** | 138GB | ~143GB | ğŸ”¥ 2 | â³ å¾…æµ‹ | vs mlx-4bit | LM Studio |
| **Q6_K** | 188GB | ~193GB | ğŸ”¥ 3 | â³ å¾…æµ‹ | vs mlx-6bit | LM Studio |
| **Q8_0** | 243GB | ~248GB | ğŸ”¥ 4 | â³ å¾…æµ‹ | vs mlx-8bit | LM Studio |
| **BF16** | 457GB | ~462GB | 5 | âŒ å¤±è´¥ | vs mlx-bf16 | OOM (å·²æµ‹) |

**å¯¹æ¯”é‡ç‚¹**: ç›¸åŒé‡åŒ–çº§åˆ«ä¸‹ MLX vs llama.cpp çš„æ€§èƒ½å·®å¼‚

### Qwen3-Coder-Next

#### Phase 2A: MLX ç‰ˆæœ¬æµ‹è¯• (å¾…ç¡®è®¤)

| ç‰ˆæœ¬ | å¤§å° | å†…å­˜å ç”¨ | ä¼˜å…ˆçº§ | çŠ¶æ€ | è¯´æ˜ |
|------|------|----------|--------|------|------|
| **mlx-4bit** | ~45GB | ~50GB | ğŸ”¥ 1 | ğŸ” å¾…æŸ¥æ‰¾ | æŸ¥æ‰¾ mlx-community ç‰ˆæœ¬ |
| **mlx-6bit** | ~68GB | ~73GB | 2 | ğŸ” å¾…æŸ¥æ‰¾ | å¦‚æœå­˜åœ¨ |
| **mlx-8bit** | ~90GB | ~95GB | 3 | ğŸ” å¾…æŸ¥æ‰¾ | å¦‚æœå­˜åœ¨ |

**æ³¨**: éœ€è¦ç¡®è®¤æ˜¯å¦æœ‰ mlx-community è½¬æ¢çš„ Qwen3-Coder-Nextï¼Œæˆ–ä½¿ç”¨ mlx-lm æ‰‹åŠ¨è½¬æ¢

#### Phase 2B: llama.cpp ç‰ˆæœ¬æµ‹è¯• (GGUF via LM Studio)

| ç‰ˆæœ¬ | å¤§å° | å†…å­˜å ç”¨ | ä¼˜å…ˆçº§ | çŠ¶æ€ | å¯¹æ¯”MLX | æµ‹è¯•æ–¹æ³• |
|------|------|----------|--------|------|---------|----------|
| **Q4_K_M** | 48.5GB | ~53GB | ğŸ”¥ 1 | â³ å¾…æµ‹ | vs mlx-4bit | LM Studio |
| **Q6_K** | 65.5GB | ~70GB | ğŸ”¥ 2 | â³ å¾…æµ‹ | vs mlx-6bit | LM Studio |
| **Q8_0** | 84.8GB | ~90GB | ğŸ”¥ 3 | â³ å¾…æµ‹ | vs mlx-8bit | LM Studio |
| **Q4_0** | 45.3GB | ~50GB | 4 | â³ å¾…æµ‹ | å¿«é€Ÿç‰ˆæœ¬ | LM Studio |
| **Q2_K** | 29.2GB | ~34GB | 5 | â³ å¾…æµ‹ | æœ€å°ç‰ˆæœ¬ | LM Studio |
| **BF16** | 159GB | ~164GB | 6 | â³ å¾…æµ‹ | å®Œæ•´ç²¾åº¦ | LM Studio |

**å¯¹æ¯”é‡ç‚¹**: å¦‚æœæœ‰ MLX ç‰ˆæœ¬ï¼Œå¯¹æ¯”ä¸¤ä¸ªæ¡†æ¶æ€§èƒ½ï¼›å¦åˆ™ä»…æµ‹è¯• GGUF ç‰ˆæœ¬

## æµ‹è¯•é¡ºåºè§„åˆ’

### Week 1: MiniMax M2.1 - åŒåç«¯å¯¹æ¯” (é€šè¿‡ LM Studio)

**ç­–ç•¥**: äº¤æ›¿æµ‹è¯• MLX å’Œ GGUF ç›¸åŒé‡åŒ–çº§åˆ«

```
Day 1:
â”œâ”€â”€ MLX 4-bit (via LM Studio) â†’ benchmark_lmstudio.py
â””â”€â”€ GGUF Q4_K_S (via LM Studio) â†’ benchmark_lmstudio.py
    â†’ ç”Ÿæˆå¯¹æ¯”è¡¨: 4-bit MLX vs Q4_K_S

Day 2:
â”œâ”€â”€ GGUF Q4_K_M (via LM Studio) â†’ benchmark_lmstudio.py
â””â”€â”€ å¯¹æ¯”åˆ†æ: Q4_K_S vs Q4_K_M

Day 3:
â”œâ”€â”€ MLX 6-bit (via LM Studio) â†’ benchmark_lmstudio.py
â””â”€â”€ GGUF Q6_K (via LM Studio) â†’ benchmark_lmstudio.py
    â†’ ç”Ÿæˆå¯¹æ¯”è¡¨: 6-bit MLX vs Q6_K

Day 4:
â”œâ”€â”€ MLX 8-bit (via LM Studio) â†’ benchmark_lmstudio.py
â””â”€â”€ GGUF Q8_0 (via LM Studio) â†’ benchmark_lmstudio.py
    â†’ ç”Ÿæˆå¯¹æ¯”è¡¨: 8-bit MLX vs Q8_0

Day 5:
â””â”€â”€ ç»¼åˆåˆ†æ: MiniMax M2.1 å®Œæ•´å¯¹æ¯”æŠ¥å‘Š
```

**è¾“å‡º**:
- MiniMax M2.1 MLX vs llama.cpp æ€§èƒ½å¯¹æ¯”è¡¨ (3ä¸ªé‡åŒ–çº§åˆ«)
- æ¡†æ¶æ¨èå»ºè®®

---

### Week 2: Qwen3-Coder-Next - åŒåç«¯æµ‹è¯•

```
Day 1:
â””â”€â”€ æŸ¥æ‰¾/ä¸‹è½½ MLX å’Œ GGUF æ¨¡å‹

Day 2-3:
â”œâ”€â”€ MLX 4-bit (via LM Studio) â†’ benchmark_lmstudio.py
â”œâ”€â”€ GGUF Q4_K_M (via LM Studio) â†’ benchmark_lmstudio.py
â”œâ”€â”€ MLX 6-bit (via LM Studio) â†’ benchmark_lmstudio.py
â””â”€â”€ GGUF Q6_K (via LM Studio) â†’ benchmark_lmstudio.py

Day 4:
â”œâ”€â”€ MLX 8-bit (via LM Studio) â†’ benchmark_lmstudio.py
â”œâ”€â”€ GGUF Q8_0 (via LM Studio) â†’ benchmark_lmstudio.py
â””â”€â”€ (å¯é€‰) Q4_0, Q2_K å¿«é€Ÿæµ‹è¯•

Day 5:
â””â”€â”€ ç»¼åˆåˆ†æ: Qwen3-Coder-Next å®Œæ•´å¯¹æ¯”æŠ¥å‘Š
```

**è¾“å‡º**:
- Qwen3-Coder-Next MLX vs llama.cpp æ€§èƒ½å¯¹æ¯”è¡¨
- ä¸ MiniMax M2.1 çš„æ¨ªå‘å¯¹æ¯”

---

### Week 3: ç»¼åˆåˆ†æä¸æŠ¥å‘Š

```
Day 1: æ¡†æ¶å¯¹æ¯”
â””â”€â”€ MLX backend vs llama.cpp backend (åœ¨ LM Studio å†…)
    - æ€§èƒ½å·®å¼‚åˆ†æ
    - å†…å­˜æ•ˆç‡å¯¹æ¯”
    - ç¨³å®šæ€§è¯„ä¼°

Day 2: æ¨¡å‹å¯¹æ¯”
â””â”€â”€ MiniMax M2.1 (230B/10B) vs Qwen3-Coder-Next (80B/3B)
    - ä»£ç ç”Ÿæˆè´¨é‡
    - TPS per GB æ•ˆç‡
    - Context åˆ©ç”¨ç‡

Day 3: é‡åŒ–çº§åˆ«å¯¹æ¯”
â””â”€â”€ 4-bit vs 6-bit vs 8-bit
    - è´¨é‡/æ€§èƒ½æƒè¡¡
    - å†…å­˜/é€Ÿåº¦æƒè¡¡
    - æœ€ä½³é€‰æ‹©å»ºè®®

Day 4-5: æ–‡æ¡£æ•´ç†
â”œâ”€â”€ framework-comparison.md
â”œâ”€â”€ model-comparison.md
â”œâ”€â”€ best-practices.md
â””â”€â”€ benchmark-results.md (æ›´æ–°)
```

**æœ€ç»ˆè¾“å‡º**:
- å®Œæ•´æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š
- 512GB Mac éƒ¨ç½²å»ºè®®
- é€‰å‹å†³ç­–æ ‘

## æµ‹è¯•æŒ‡æ ‡

### æ ¸å¿ƒæŒ‡æ ‡

| æŒ‡æ ‡ | è¯´æ˜ | æµ‹é‡æ–¹æ³• | é‡è¦æ€§ |
|------|------|----------|--------|
| **TPS** | Tokens per Second | æ€»tokens/ç”Ÿæˆæ—¶é—´ | â­â­â­â­â­ |
| **TTFT** | Time to First Token | é¦–tokenå»¶è¿Ÿ | â­â­â­â­ |
| **Peak Memory** | å³°å€¼å†…å­˜å ç”¨ | ç³»ç»Ÿç›‘æ§ | â­â­â­â­â­ |
| **Load Time** | æ¨¡å‹åŠ è½½æ—¶é—´ | åˆå§‹åŒ–è®¡æ—¶ | â­â­â­ |
| **Quality** | è¾“å‡ºè´¨é‡ | ç›¸åŒpromptå¯¹æ¯” | â­â­â­â­ |

### é¢å¤–æŒ‡æ ‡

- **Memory Efficiency**: TPS per GB (åå/å†…å­˜æ¯”)
- **Context Utilization**: å®é™…å¯ç”¨context vs ç†è®ºå€¼
- **Stability**: å¤šæ¬¡è¿è¡Œä¸€è‡´æ€§

## æµ‹è¯•ç”¨ä¾‹

### æ ‡å‡†æµ‹è¯•é›† (5ä¸ªåœºæ™¯)

```json
{
  "short": {
    "prompt": "è¯·ç”¨ä¸€å¥è¯è§£é‡Šé‡å­è®¡ç®—",
    "max_tokens": 100,
    "category": "ç®€çŸ­é—®ç­”"
  },
  "medium": {
    "prompt": "å†™ä¸€ä¸ªPythonå¿«é€Ÿæ’åºç®—æ³•ï¼ŒåŒ…å«æ³¨é‡Š",
    "max_tokens": 500,
    "category": "ä»£ç ç”Ÿæˆ"
  },
  "long": {
    "prompt": "è¯¦ç»†è§£é‡Šæ·±åº¦å­¦ä¹ çš„åå‘ä¼ æ’­ç®—æ³•ï¼ŒåŒ…å«æ•°å­¦æ¨å¯¼",
    "max_tokens": 2000,
    "category": "é•¿æ–‡æœ¬ç”Ÿæˆ"
  },
  "reasoning": {
    "prompt": "ä¸‰ä¸ªç›’å­ï¼Œçº¢ç›’è£…è“çƒï¼Œè“ç›’è£…çº¢çƒï¼Œæ ‡ç­¾å…¨é”™ã€‚æœ€å°‘å–å‡ æ¬¡ç¡®å®šå†…å®¹ï¼Ÿ",
    "max_tokens": 500,
    "category": "é€»è¾‘æ¨ç†"
  },
  "instruction": {
    "prompt": "ä½œä¸ºPythonä¸“å®¶ï¼Œå®¡æŸ¥ä»¥ä¸‹ä»£ç å¹¶æå‡ºæ”¹è¿›å»ºè®®ï¼š[ç¤ºä¾‹ä»£ç ]",
    "max_tokens": 400,
    "category": "æŒ‡ä»¤è·Ÿéš"
  }
}
```

### Qwen3-Coder-Next ç‰¹æ®Šæµ‹è¯•

```json
{
  "code_agent": {
    "prompt": "åˆ†æè¿™ä¸ªrepoç»“æ„ï¼Œå»ºè®®é‡æ„æ–¹æ¡ˆ",
    "max_tokens": 1000,
    "category": "ä»£ç ä»£ç†"
  },
  "multi_file": {
    "prompt": "é‡æ„é¡¹ç›®ï¼šå°†å•æ–‡ä»¶æ‹†åˆ†ä¸ºæ¨¡å—åŒ–ç»“æ„",
    "max_tokens": 1500,
    "category": "å¤šæ–‡ä»¶æ“ä½œ"
  }
}
```

## æµ‹è¯•æµç¨‹

### ç»Ÿä¸€æµ‹è¯•æµç¨‹ (MLX å’Œ GGUF éƒ½é€šè¿‡ LM Studio)

#### Step 1: åœ¨ LM Studio ä¸­åŠ è½½æ¨¡å‹

**MLX æ¨¡å‹åŠ è½½**:
```bash
# æ–¹æ³•1: LM Studio GUI
# æœç´¢: mlx-community/MiniMax-M2.1-4bit
# ç‚¹å‡»ä¸‹è½½å¹¶åŠ è½½

# æ–¹æ³•2: CLI (å¦‚æœæ”¯æŒ)
lms download mlx-community/MiniMax-M2.1-4bit
lms load mlx-community/MiniMax-M2.1-4bit
```

**GGUF æ¨¡å‹åŠ è½½**:
```bash
# æ–¹æ³•1: LM Studio GUI
# æœç´¢: unsloth/MiniMax-M2.1-GGUF
# é€‰æ‹© Q4_K_S é‡åŒ–ç‰ˆæœ¬ä¸‹è½½å¹¶åŠ è½½

# æ–¹æ³•2: CLI
lms download unsloth/MiniMax-M2.1-GGUF:Q4_K_S
lms load unsloth/MiniMax-M2.1-GGUF:Q4_K_S
```

**å¯åŠ¨ API Server**:
```bash
# ç¡®ä¿æœåŠ¡å™¨è¿è¡Œåœ¨ port 1234
lms server start --port 1234
```

#### Step 2: ç¡®è®¤æœåŠ¡å™¨å’Œåç«¯
```bash
# æ£€æŸ¥ API
curl http://localhost:1234/v1/models

# æµ‹è¯•å“åº”
curl http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"model-name","messages":[{"role":"user","content":"hi"}],"max_tokens":10}'

# åœ¨ LM Studio GUI ä¸­ç¡®è®¤å½“å‰ä½¿ç”¨çš„åç«¯
# MLX æ¨¡å‹ â†’ æ˜¾ç¤º "MLX" æ ‡ç­¾
# GGUF æ¨¡å‹ â†’ æ˜¾ç¤º "llama.cpp" æ ‡ç­¾
```

#### Step 3: è¿è¡Œç»Ÿä¸€æµ‹è¯•è„šæœ¬
```bash
cd /Users/jacky/projects/llm-mac-512

# æ— è®º MLX è¿˜æ˜¯ GGUFï¼Œéƒ½ç”¨åŒä¸€ä¸ªè„šæœ¬æµ‹è¯•
python scripts/benchmark_lmstudio.py

# ç»“æœè‡ªåŠ¨ä¿å­˜å¹¶æ ‡æ³¨åç«¯ç±»å‹
# docs/test-results/mlx-minimax-m2-1-4bit-{timestamp}.json
# docs/test-results/gguf-minimax-m2-1-q4ks-{timestamp}.json
```

#### Step 4: åˆ‡æ¢æ¨¡å‹ç»§ç»­æµ‹è¯•
```bash
# åœ¨ LM Studio GUI ä¸­ï¼š
# 1. Unload å½“å‰æ¨¡å‹
# 2. Load ä¸‹ä¸€ä¸ªæ¨¡å‹ (MLX æˆ– GGUF)
# 3. é‡å¤ Step 3
```

**æµ‹è¯•é¡ºåºå»ºè®®**:
```
Week 1: MiniMax M2.1
â”œâ”€â”€ MLX 4-bit â†’ benchmark_lmstudio.py
â”œâ”€â”€ GGUF Q4_K_S â†’ benchmark_lmstudio.py (å¯¹æ¯”)
â”œâ”€â”€ MLX 6-bit â†’ benchmark_lmstudio.py
â”œâ”€â”€ GGUF Q6_K â†’ benchmark_lmstudio.py (å¯¹æ¯”)
â”œâ”€â”€ MLX 8-bit â†’ benchmark_lmstudio.py
â””â”€â”€ GGUF Q8_0 â†’ benchmark_lmstudio.py (å¯¹æ¯”)
```

### 3. ç»“æœè®°å½•

æ¯æ¬¡æµ‹è¯•ç”Ÿæˆï¼š
- `{model}-{version}-{timestamp}.json` - åŸå§‹æ•°æ®
- `{model}-{version}-{timestamp}.md` - MarkdownæŠ¥å‘Š

### 4. æµ‹è¯•åæ¸…ç†

```bash
# å¸è½½æ¨¡å‹ (é‡Šæ”¾å†…å­˜)
# åœ¨ LM Studio GUI ä¸­ unload model

# åˆ é™¤æ¨¡å‹ (é‡Šæ”¾ç£ç›˜)
# ä»…åœ¨ç¡®è®¤æµ‹è¯•å®Œæˆååˆ é™¤
```

## å¯¹æ¯”åˆ†æç»´åº¦

### 1. ğŸ”¥ æ¡†æ¶å¯¹æ¯” (MLX vs llama.cpp) - æ ¸å¿ƒé‡ç‚¹

**MiniMax M2.1:**
```
4-bit: mlx-4bit (45.73 TPS) vs Q4_K_S/Q4_K_M (å¾…æµ‹)
6-bit: mlx-6bit (39.01 TPS) vs Q6_K (å¾…æµ‹)
8-bit: mlx-8bit (33.04 TPS) vs Q8_0 (å¾…æµ‹)
```

**Qwen3-Coder-Next:**
```
4-bit: mlx-4bit (å¾…æµ‹) vs Q4_K_M (å¾…æµ‹)
6-bit: mlx-6bit (å¾…æµ‹) vs Q6_K (å¾…æµ‹)
8-bit: mlx-8bit (å¾…æµ‹) vs Q8_0 (å¾…æµ‹)
```

**å¯¹æ¯”æŒ‡æ ‡:**
- TPS (ç”Ÿæˆé€Ÿåº¦)
- TTFT (é¦–tokenå»¶è¿Ÿ)
- å†…å­˜ä½¿ç”¨æ•ˆç‡
- åŠ è½½æ—¶é—´
- ç¨³å®šæ€§

**é¢„æœŸé—®é¢˜:**
- MLX åœ¨ Apple Silicon ä¸Šæ˜¯å¦æ›´å¿«ï¼Ÿ
- llama.cpp é‡åŒ–æ˜¯å¦æ›´èŠ‚çœå†…å­˜ï¼Ÿ
- å“ªä¸ªæ¡†æ¶æ›´é€‚åˆç”Ÿäº§ç¯å¢ƒï¼Ÿ

### 2. æ¨¡å‹é—´å¯¹æ¯”

```
MiniMax M2.1 (230B/10B) vs Qwen3-Coder-Next (80B/3B):
- ä»£ç ç”Ÿæˆè´¨é‡
- æ¨ç†èƒ½åŠ›
- æ€§èƒ½/å†…å­˜æ•ˆç‡ (TPS per GB)
- Context åˆ©ç”¨ç‡ (196K vs 256K)
- å¯åŠ¨é€Ÿåº¦
```

### 3. é‡åŒ–çº§åˆ«å¯¹æ¯”

```
4-bit vs 6-bit vs 8-bit:
- è´¨é‡ä¸‹é™ç¨‹åº¦
- æ€§èƒ½æå‡å¹…åº¦ (TPS å¢åŠ )
- å†…å­˜èŠ‚çœæ¯”ä¾‹
- æœ€ä½³æ€§ä»·æ¯”é€‰æ‹©
```

### 4. æ¡†æ¶ç‰¹æ€§å¯¹æ¯”

| ç‰¹æ€§ | MLX | llama.cpp (GGUF) |
|------|-----|------------------|
| Apple ä¼˜åŒ– | âœ… åŸç”Ÿ | âš ï¸ Metal åç«¯ |
| é€šç”¨æ€§ | âŒ Mac only | âœ… è·¨å¹³å° |
| ç”Ÿæ€ç³»ç»Ÿ | mlx-lm | LM Studio, Ollama |
| æ˜“ç”¨æ€§ | Python API | CLI + API |
| ç¤¾åŒºæ”¯æŒ | ğŸ”¥ Apple | ğŸ”¥ğŸ”¥ æœ€å¹¿æ³› |

## æµ‹è¯•å¯¹æ¯”æ€»è§ˆ

### å®Œæ•´æµ‹è¯•çŸ©é˜µ

| æ¨¡å‹ | MLX 4bit | MLX 6bit | MLX 8bit | GGUF Q4 | GGUF Q6 | GGUF Q8 |
|------|----------|----------|----------|---------|---------|---------|
| **MiniMax M2.1** | âœ… 45.73 TPS | âœ… 39.01 TPS | âœ… 33.04 TPS | ğŸ”„ æµ‹è¯•ä¸­ | â³ å¾…æµ‹ | â³ å¾…æµ‹ |
| **Qwen3-Coder** | ğŸ” å¾…æŸ¥æ‰¾ | ğŸ” å¾…æŸ¥æ‰¾ | ğŸ” å¾…æŸ¥æ‰¾ | â³ å¾…æµ‹ | â³ å¾…æµ‹ | â³ å¾…æµ‹ |

### æ ¸å¿ƒå¯¹æ¯”é—®é¢˜

**æ¡†æ¶å¯¹æ¯”:**
1. ç›¸åŒé‡åŒ–ä¸‹ï¼ŒMLX vs llama.cpp è°æ›´å¿«ï¼Ÿ
2. å†…å­˜ä½¿ç”¨æ•ˆç‡å·®å¼‚å¤šå¤§ï¼Ÿ
3. å“ªä¸ªæ¡†æ¶æ›´é€‚åˆç”Ÿäº§ç¯å¢ƒï¼Ÿ

**æ¨¡å‹å¯¹æ¯”:**
1. MiniMax M2.1 vs Qwen3-Coder-Next ä»£ç èƒ½åŠ›ï¼Ÿ
2. 230B/10B vs 80B/3B çš„æ€§èƒ½/è´¨é‡æƒè¡¡ï¼Ÿ
3. 256K context æ˜¯å¦æ¯” 196K æ›´å®ç”¨ï¼Ÿ

**é‡åŒ–å¯¹æ¯”:**
1. 4bit vs 6bit vs 8bit è´¨é‡ä¸‹é™å¤šå°‘ï¼Ÿ
2. æ€§èƒ½æå‡æ˜¯å¦å€¼å¾—é¢å¤–å†…å­˜ï¼Ÿ
3. æœ€ä½³æ€§ä»·æ¯”é€‰æ‹©æ˜¯ä»€ä¹ˆï¼Ÿ

## é¢„æœŸæˆæœ

### æµ‹è¯•æŠ¥å‘Š

1. **benchmark-results.md** - æ±‡æ€»æ‰€æœ‰æµ‹è¯•æ•°æ®
   - MLX æµ‹è¯•ç»“æœæ±‡æ€»
   - GGUF æµ‹è¯•ç»“æœæ±‡æ€»
   - æ¡†æ¶å¯¹æ¯”è¡¨æ ¼

2. **framework-comparison.md** - MLX vs llama.cpp æ·±åº¦å¯¹æ¯”
   - æ€§èƒ½å¯¹æ¯” (TPS, TTFT)
   - å†…å­˜æ•ˆç‡å¯¹æ¯”
   - ç¨³å®šæ€§å’Œæ˜“ç”¨æ€§

3. **model-comparison.md** - MiniMax vs Qwen3 å¯¹æ¯”
   - ä»£ç ç”Ÿæˆè´¨é‡
   - æ¨ç†èƒ½åŠ›
   - é€‚ç”¨åœºæ™¯

4. **best-practices.md** - 512GB Mac ä½¿ç”¨å»ºè®®
   - æ¨¡å‹é€‰å‹å»ºè®®
   - é‡åŒ–çº§åˆ«æ¨è
   - éƒ¨ç½²æœ€ä½³å®è·µ

## å‚è€ƒèµ„æº

### MiniMax M2.1
- [å®˜æ–¹æ–°é—»](https://www.minimax.io/news/minimax-m21)
- [MLXéƒ¨ç½²æŒ‡å—](https://github.com/MiniMax-AI/MiniMax-M2.1/blob/main/docs/mlx_deploy_guide.md)
- [Unsloth GGUFç‰ˆæœ¬](https://huggingface.co/unsloth/MiniMax-M2.1-GGUF)

### Qwen3-Coder-Next
- [å®˜æ–¹åšå®¢](https://qwen.ai/blog?id=qwen3-coder-next)
- [Hugging Faceä¸»é¡µ](https://huggingface.co/Qwen/Qwen3-Coder-Next)
- [Unsloth GGUFç‰ˆæœ¬](https://huggingface.co/unsloth/Qwen3-Coder-Next-GGUF)
- [Unslothæ–‡æ¡£](https://unsloth.ai/docs/models/qwen3-coder-next)

### å·¥å…·
- [LM Studio](https://lmstudio.ai/download)
- [OpenClawæ–‡æ¡£](https://docs.openclaw.ai/)

## é¡¹ç›®æ–‡ä»¶ç»“æ„

```
/Users/jacky/projects/llm-mac-512/
â”œâ”€â”€ README.md
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ test-plan-v2.md (æœ¬æ–‡æ¡£)
â”‚   â”œâ”€â”€ test-plan.md (v1 - å·²å½’æ¡£)
â”‚   â”œâ”€â”€ benchmark-results.md (æ±‡æ€»ç»“æœ)
â”‚   â”œâ”€â”€ model-comparison.md (å¾…åˆ›å»º)
â”‚   â”œâ”€â”€ best-practices.md (å¾…åˆ›å»º)
â”‚   â”œâ”€â”€ lmstudio-openclaw-troubleshooting.md
â”‚   â””â”€â”€ test-results/
â”‚       â”œâ”€â”€ archive/ (MLXåŸå§‹æµ‹è¯•)
â”‚       â”œâ”€â”€ minimax-*.json/md (MiniMaxæµ‹è¯•)
â”‚       â””â”€â”€ qwen3-*.json/md (Qwen3æµ‹è¯•)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ benchmark_lmstudio.py (ç»Ÿä¸€æµ‹è¯•è„šæœ¬)
â”‚   â””â”€â”€ utils.py
â””â”€â”€ prompts/
    â””â”€â”€ test_prompts.json
```

## æ—¶é—´ä¼°ç®—

| é˜¶æ®µ | é¢„è®¡æ—¶é—´ | è¯´æ˜ |
|------|----------|------|
| MiniMax GGUFæµ‹è¯• | 5å¤© | 4ä¸ªç‰ˆæœ¬ + åˆ†æ |
| Qwen3-Coderæµ‹è¯• | 5å¤© | 5-6ä¸ªç‰ˆæœ¬ + åˆ†æ |
| å¯¹æ¯”åˆ†æ | 3å¤© | æŠ¥å‘Šç¼–å†™ |
| **æ€»è®¡** | **2-3å‘¨** | åŒ…å«æ–‡æ¡£æ•´ç† |

## æ³¨æ„äº‹é¡¹

### æµ‹è¯•å‰æ£€æŸ¥

- [ ] LM Studio å·²å®‰è£…å¹¶æ›´æ–°åˆ°æœ€æ–°ç‰ˆ
- [ ] Context Length è®¾ç½®ä¸º 131,072+
- [ ] å…³é—­å…¶ä»–å¤§å‹åº”ç”¨é‡Šæ”¾å†…å­˜
- [ ] å‡†å¤‡è¶³å¤Ÿç£ç›˜ç©ºé—´ (æ¯ä¸ªæ¨¡å‹ä¸‹è½½åæµ‹è¯•)

### æµ‹è¯•ä¸­ç›‘æ§

- [ ] å†…å­˜ä½¿ç”¨ (Activity Monitor)
- [ ] æ¸©åº¦/é£æ‰‡ (é¿å…è¿‡çƒ­)
- [ ] ç£ç›˜ç©ºé—´ (åŠæ—¶æ¸…ç†)

### æµ‹è¯•åæ¸…ç†

- [ ] ä¿å­˜æµ‹è¯•ç»“æœ
- [ ] å¸è½½æ¨¡å‹é‡Šæ”¾å†…å­˜
- [ ] å½’æ¡£åˆ° Git
- [ ] (å¯é€‰) åˆ é™¤æ¨¡å‹æ–‡ä»¶é‡Šæ”¾ç£ç›˜

## ä¸‹ä¸€æ­¥è¡ŒåŠ¨

**å½“å‰çŠ¶æ€**: MiniMax M2.1 Q4_K_S (GGUF) å·²åŠ è½½åœ¨ LM Studio

### ğŸš¨ é‡è¦æ›´æ–°ï¼šå…¬å¹³æµ‹è¯•æ–¹æ³•

**æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡ LM Studio**:
- âœ… MLX æ¨¡å‹ â†’ LM Studio (MLX backend) â†’ API â†’ benchmark_lmstudio.py
- âœ… GGUF æ¨¡å‹ â†’ LM Studio (llama.cpp backend) â†’ API â†’ benchmark_lmstudio.py

**ä¸ºä»€ä¹ˆ**: ç¡®ä¿å…¬å¹³å¯¹æ¯”ï¼Œæ’é™¤ API å¼€é”€å·®å¼‚

---

### ç«‹å³æ‰§è¡Œ (Day 1: 4-bit å¯¹æ¯”)

#### Test 1: MLX 4-bit (é€šè¿‡ LM Studio)

```bash
# Step 1: åœ¨ LM Studio GUI ä¸­
# - Unload å½“å‰çš„ GGUF æ¨¡å‹
# - æœç´¢å¹¶ä¸‹è½½: mlx-community/MiniMax-M2.1-4bit
# - Load è¯¥æ¨¡å‹
# - ç¡®è®¤æ˜¾ç¤º "MLX" åç«¯æ ‡ç­¾

# Step 2: å¯åŠ¨ API server (å¦‚æœæœªè¿è¡Œ)
lms server start --port 1234

# Step 3: è¿è¡Œæµ‹è¯•
cd /Users/jacky/projects/llm-mac-512
python scripts/benchmark_lmstudio.py

# ç»“æœä¿å­˜ä¸º: docs/test-results/mlx-minimax-m2-1-4bit-{timestamp}.json
```

#### Test 2: GGUF Q4_K_S (é€šè¿‡ LM Studio)

```bash
# Step 1: åœ¨ LM Studio GUI ä¸­
# - Unload MLX æ¨¡å‹
# - Load: unsloth/MiniMax-M2.1-GGUF Q4_K_S (å½“å‰å·²æœ‰)
# - ç¡®è®¤æ˜¾ç¤º "llama.cpp" åç«¯æ ‡ç­¾

# Step 2: è¿è¡Œæµ‹è¯•
python scripts/benchmark_lmstudio.py

# ç»“æœä¿å­˜ä¸º: docs/test-results/gguf-minimax-m2-1-q4ks-{timestamp}.json
```

#### Test 3: ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š

```bash
# å¯¹æ¯”ä¸¤ä¸ªç»“æœ
python scripts/compare_results.py \
  docs/test-results/mlx-minimax-m2-1-4bit-{timestamp}.json \
  docs/test-results/gguf-minimax-m2-1-q4ks-{timestamp}.json

# è¾“å‡º: MLX 4-bit vs GGUF Q4_K_S å¯¹æ¯”è¡¨
```

---

### æœ¬å‘¨ç›®æ ‡ (Week 1)

**Day 1**: 4-bit å¯¹æ¯” âœ…
- [ ] MLX 4-bit (via LM Studio)
- [ ] GGUF Q4_K_S (via LM Studio)
- [ ] å¯¹æ¯”åˆ†æ

**Day 2**: Q4_K_M æµ‹è¯•
- [ ] GGUF Q4_K_M (via LM Studio)
- [ ] vs Q4_K_S å¯¹æ¯”

**Day 3**: 6-bit å¯¹æ¯”
- [ ] MLX 6-bit (via LM Studio)
- [ ] GGUF Q6_K (via LM Studio)
- [ ] å¯¹æ¯”åˆ†æ

**Day 4**: 8-bit å¯¹æ¯”
- [ ] MLX 8-bit (via LM Studio)
- [ ] GGUF Q8_0 (via LM Studio)
- [ ] å¯¹æ¯”åˆ†æ

**Day 5**: Week 1 æ€»ç»“
- [ ] ç”Ÿæˆ MiniMax M2.1 å®Œæ•´å¯¹æ¯”æŠ¥å‘Š
- [ ] MLX vs llama.cpp æ¡†æ¶åˆ†æ

---

### æµ‹è¯•å‰æ£€æŸ¥æ¸…å•

#### LM Studio é…ç½®
- [ ] LM Studio å·²å®‰è£…å¹¶æ›´æ–°
- [ ] Server è¿è¡Œåœ¨ port 1234
- [ ] Context Length = 131,072 (åœ¨ Settings ä¸­ç¡®è®¤)
- [ ] å¯ä»¥é€šè¿‡ GUI çœ‹åˆ°å½“å‰åç«¯ç±»å‹ (MLX/llama.cpp)

#### æµ‹è¯•è„šæœ¬
- [ ] `scripts/benchmark_lmstudio.py` å­˜åœ¨
- [ ] è„šæœ¬ä½¿ç”¨ configs/gguf_standard.json é…ç½®
- [ ] è„šæœ¬è®°å½•åç«¯ç±»å‹åˆ°ç»“æœæ–‡ä»¶

#### ç³»ç»Ÿèµ„æº
- [ ] å…³é—­å…¶ä»–å¤§å‹åº”ç”¨
- [ ] è‡³å°‘ 150GB+ å¯ç”¨å†…å­˜
- [ ] è¶³å¤Ÿç£ç›˜ç©ºé—´ä¿å­˜ç»“æœ

---

### æ³¨æ„äº‹é¡¹

1. **åç«¯ç¡®è®¤**: æ¯æ¬¡åŠ è½½æ¨¡å‹åï¼Œåœ¨ LM Studio GUI ç¡®è®¤åç«¯ç±»å‹
   - MLX æ¨¡å‹ â†’ åº”æ˜¾ç¤º "MLX" æ ‡ç­¾
   - GGUF æ¨¡å‹ â†’ åº”æ˜¾ç¤º "llama.cpp" æ ‡ç­¾

2. **ç»“æœå‘½å**: ç¡®ä¿ç»“æœæ–‡ä»¶ååŒºåˆ†åç«¯
   - MLX: `mlx-minimax-m2-1-4bit-{timestamp}.json`
   - GGUF: `gguf-minimax-m2-1-q4ks-{timestamp}.json`

3. **API ä¸€è‡´æ€§**: æ‰€æœ‰æµ‹è¯•é€šè¿‡ç›¸åŒçš„ API endpoint
   - `http://localhost:1234/v1/chat/completions`

4. **å‚æ•°ä¸€è‡´æ€§**: ç¡®è®¤ä¸¤ä¸ªåç«¯ä½¿ç”¨ç›¸åŒå‚æ•°
   - Context: 131,072
   - Temperature: 0.7
   - Top-p: 0.9
   - Seed: 42
