# MLX æœ¬åœ°è¿è¡ŒæŒ‡å—

> åœ¨Macä¸Šæœ¬åœ°è¿è¡ŒMiniMax M2.1æ¨¡å‹çš„å®Œæ•´æŒ‡å—

## ğŸ“‹ ç›®å½•

- [ç³»ç»Ÿè¦æ±‚](#ç³»ç»Ÿè¦æ±‚)
- [ç¯å¢ƒé…ç½®](#ç¯å¢ƒé…ç½®)
- [å®‰è£…MLX](#å®‰è£…mlx)
- [è¿è¡Œæ¨¡å‹](#è¿è¡Œæ¨¡å‹)
- [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

---

## ç³»ç»Ÿè¦æ±‚

### æœ€ä½è¦æ±‚

| é¡¹ç›® | è¦æ±‚ |
|------|------|
| **ç¡¬ä»¶** | Apple Silicon Mac (M1/M2/M3ç³»åˆ—) |
| **macOS** | 13.3+ (æ¨è 14.0+) |
| **å†…å­˜** | 16GB+ (æ¨è64GB+) |
| **ç£ç›˜ç©ºé—´** | 150GB+ å¯ç”¨ç©ºé—´ |
| **Python** | 3.9+ (æ¨è 3.12) |

### æ¨èé…ç½®ï¼ˆMiniMax M2.1ï¼‰

| æ¨¡å‹ç‰ˆæœ¬ | æ¨èå†…å­˜ | ç£ç›˜ç©ºé—´ | æ€§èƒ½é¢„æœŸ |
|---------|---------|---------|---------|
| 4-bit | 32GB+ | 120GB | æœ€å¿« (~45 TPS) |
| 6-bit | 64GB+ | 180GB | å¹³è¡¡ (~40 TPS) |
| 8-bit | 128GB+ | 240GB | é«˜è´¨é‡ (~33 TPS) |
| bf16 | 512GB | 460GB | å…¨ç²¾åº¦ (ä¸æ¨è) |

> **æ³¨æ„ï¼š** ä½ çš„Mac Studio (512GB) å¯ä»¥è¿è¡Œæ‰€æœ‰ç‰ˆæœ¬ï¼

---

## ç¯å¢ƒé…ç½®

### 1. æ£€æŸ¥ç³»ç»Ÿä¿¡æ¯

```bash
# æŸ¥çœ‹ç³»ç»Ÿä¿¡æ¯
system_profiler SPHardwareDataType | grep -E "Model Name|Model Identifier|Chip|Memory"

# æŸ¥çœ‹macOSç‰ˆæœ¬
sw_vers

# æŸ¥çœ‹å¯ç”¨ç£ç›˜ç©ºé—´
df -h
```

**ä½ çš„é…ç½®ï¼š**
- Mac Studio (Mac15,14)
- Apple M3 Ultra
- 512 GB ç»Ÿä¸€å†…å­˜
- macOS 26.2

âœ… æ»¡è¶³æ‰€æœ‰è¦æ±‚ï¼

### 2. å®‰è£…Homebrewï¼ˆå¦‚æœæœªå®‰è£…ï¼‰

```bash
# æ£€æŸ¥æ˜¯å¦å·²å®‰è£…
which brew

# å¦‚æœæœªå®‰è£…ï¼Œè¿è¡Œï¼š
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
```

### 3. å®‰è£…Python 3.12

```bash
# å®‰è£…Python 3.12
brew install python@3.12

# éªŒè¯å®‰è£…
python3.12 --version
```

---

## å®‰è£…MLX

### æ–¹å¼ä¸€ï¼šä½¿ç”¨ç°æœ‰é¡¹ç›®ï¼ˆæ¨èï¼‰

ä½ å·²ç»æœ‰è¿™ä¸ªé¡¹ç›®äº†ï¼Œç›´æ¥æ¿€æ´»ç¯å¢ƒï¼š

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd ~/projects/llm-mac-512  # æˆ–ä½ çš„é¡¹ç›®å®é™…è·¯å¾„

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate

# éªŒè¯MLXå®‰è£…
python -c "import mlx.core as mx; print(f'MLX version: {mx.__version__}')"
python -c "import mlx_lm; print('mlx-lm installed')"
```

å¦‚æœå‡ºç°é”™è¯¯ï¼Œé‡æ–°å®‰è£…ï¼š

```bash
# ç¡®ä¿è™šæ‹Ÿç¯å¢ƒæ¿€æ´»
source venv/bin/activate

# æ›´æ–°MLX
pip install --upgrade mlx mlx-lm

# éªŒè¯
python -c "import mlx_lm; print('Success!')"
```

### æ–¹å¼äºŒï¼šä»é›¶å¼€å§‹ï¼ˆæ–°é¡¹ç›®ï¼‰

```bash
# åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir -p ~/mlx-minimax
cd ~/mlx-minimax

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3.12 -m venv venv

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate

# å‡çº§pip
pip install --upgrade pip

# å®‰è£…MLXå’Œmlx-lm
pip install mlx mlx-lm

# å®‰è£…å…¶ä»–å·¥å…·
pip install huggingface-hub psutil

# éªŒè¯å®‰è£…
python -c "import mlx.core as mx; print('MLX installed successfully!')"
```

---

## è¿è¡Œæ¨¡å‹

### å¿«é€Ÿå¼€å§‹ï¼šå‘½ä»¤è¡Œè¿è¡Œ

#### 1. æœ€ç®€å•çš„æ–¹å¼ï¼ˆ4-bitï¼Œæ¨èé¦–æ¬¡æµ‹è¯•ï¼‰

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•å¹¶æ¿€æ´»ç¯å¢ƒ
cd ~/projects/llm-mac-512  # æˆ–ä½ çš„é¡¹ç›®å®é™…è·¯å¾„
source venv/bin/activate

# è¿è¡Œæ¨¡å‹ï¼ˆä¼šè‡ªåŠ¨ä¸‹è½½ï¼‰
mlx_lm.generate --model mlx-community/MiniMax-M2.1-4bit \
  --prompt "è¯·ç”¨ä¸€å¥è¯è§£é‡Šé‡å­è®¡ç®—" \
  --max-tokens 100
```

**é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡å‹ï¼ˆ~120GBï¼‰ï¼Œè¯·è€å¿ƒç­‰å¾…ï¼**

#### 2. äº¤äº’å¼å¯¹è¯

```bash
# å¯åŠ¨äº¤äº’æ¨¡å¼
mlx_lm.generate --model mlx-community/MiniMax-M2.1-4bit \
  --prompt "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±" \
  --max-tokens 500
```

#### 3. ä½¿ç”¨ä¸åŒçš„æ¨¡å‹ç‰ˆæœ¬

```bash
# 6-bitï¼ˆæ›´å¥½çš„è´¨é‡ï¼‰
mlx_lm.generate --model mlx-community/MiniMax-M2.1-6bit \
  --prompt "å†™ä¸€ä¸ªPythonå¿«é€Ÿæ’åºç®—æ³•" \
  --max-tokens 500

# 8-bitï¼ˆæœ€ä½³è´¨é‡ï¼‰
mlx_lm.generate --model mlx-community/MiniMax-M2.1-8bit \
  --prompt "è¯¦ç»†è§£é‡Šæ·±åº¦å­¦ä¹ çš„åå‘ä¼ æ’­ç®—æ³•" \
  --max-tokens 2000
```

---

### Pythonè„šæœ¬è¿è¡Œ

#### åŸºç¡€ç¤ºä¾‹

åˆ›å»ºæ–‡ä»¶ `test_mlx.py`ï¼š

```python
#!/usr/bin/env python3
"""
MLX MiniMax M2.1 åŸºç¡€æµ‹è¯•
"""

from mlx_lm import load, generate

# åŠ è½½æ¨¡å‹ï¼ˆé¦–æ¬¡ä¼šä¸‹è½½ï¼‰
print("Loading model...")
model, tokenizer = load("mlx-community/MiniMax-M2.1-4bit")
print("Model loaded!")

# å‡†å¤‡prompt
prompt = "è¯·ç”¨ä¸€å¥è¯è§£é‡Šé‡å­è®¡ç®—"

# ç”Ÿæˆå›ç­”
print(f"\nPrompt: {prompt}\n")
print("Generating...")

response = generate(
    model,
    tokenizer,
    prompt=prompt,
    max_tokens=100,
    verbose=True  # æ˜¾ç¤ºç”Ÿæˆè¿‡ç¨‹
)

print(f"\nResponse:\n{response}")
```

è¿è¡Œï¼š

```bash
python test_mlx.py
```

#### é«˜çº§ç¤ºä¾‹ï¼šå¸¦å‚æ•°æ§åˆ¶

åˆ›å»ºæ–‡ä»¶ `chat_mlx.py`ï¼š

```python
#!/usr/bin/env python3
"""
MLX MiniMax M2.1 å¯¹è¯ç¤ºä¾‹
"""

from mlx_lm import load, generate
from mlx_lm.sample_utils import make_sampler
import time

def chat(model_name="mlx-community/MiniMax-M2.1-4bit"):
    """äº¤äº’å¼å¯¹è¯"""

    # åŠ è½½æ¨¡å‹
    print("Loading model...")
    start_time = time.time()
    model, tokenizer = load(model_name)
    load_time = time.time() - start_time
    print(f"Model loaded in {load_time:.2f} seconds\n")

    print("=" * 60)
    print("MiniMax M2.1 æœ¬åœ°å¯¹è¯")
    print("=" * 60)
    print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º\n")

    while True:
        # è·å–ç”¨æˆ·è¾“å…¥
        user_input = input("You: ").strip()

        if user_input.lower() in ['quit', 'exit', 'q']:
            print("å†è§ï¼")
            break

        if not user_input:
            continue

        # åº”ç”¨chatæ¨¡æ¿ï¼ˆå¦‚æœæœ‰ï¼‰
        if hasattr(tokenizer, 'apply_chat_template'):
            messages = [{"role": "user", "content": user_input}]
            prompt = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        else:
            prompt = user_input

        # ç”Ÿæˆå›ç­”
        print("\nAssistant: ", end="", flush=True)
        start_gen = time.time()

        # åˆ›å»ºé‡‡æ ·å™¨
        sampler = make_sampler(temp=0.7)

        response = generate(
            model,
            tokenizer,
            prompt=prompt,
            max_tokens=500,
            sampler=sampler,
            verbose=False
        )

        gen_time = time.time() - start_gen
        tokens = len(tokenizer.encode(response))
        tps = tokens / gen_time if gen_time > 0 else 0

        print(response)
        print(f"\n[ç”Ÿæˆ {tokens} tokensï¼Œç”¨æ—¶ {gen_time:.2f}sï¼Œé€Ÿåº¦ {tps:.1f} tokens/s]\n")

if __name__ == "__main__":
    # å¯ä»¥ä¿®æ”¹æ¨¡å‹ç‰ˆæœ¬
    # chat("mlx-community/MiniMax-M2.1-4bit")  # æœ€å¿«
    # chat("mlx-community/MiniMax-M2.1-6bit")  # å¹³è¡¡
    # chat("mlx-community/MiniMax-M2.1-8bit")  # æœ€ä½³è´¨é‡

    chat()  # é»˜è®¤4-bit
```

è¿è¡Œï¼š

```bash
python chat_mlx.py
```

#### æ‰¹é‡æµ‹è¯•ç¤ºä¾‹

åˆ›å»ºæ–‡ä»¶ `batch_test.py`ï¼š

```python
#!/usr/bin/env python3
"""
æ‰¹é‡æµ‹è¯•å¤šä¸ªprompts
"""

from mlx_lm import load, generate
from mlx_lm.sample_utils import make_sampler
import time

# æµ‹è¯•prompts
test_prompts = [
    "è¯·ç”¨ä¸€å¥è¯è§£é‡Šé‡å­è®¡ç®—",
    "å†™ä¸€ä¸ªPythonå¿«é€Ÿæ’åºç®—æ³•",
    "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
    "è§£é‡Šä¸€ä¸‹åŒºå—é“¾æŠ€æœ¯",
    "ç»™æˆ‘è®²ä¸ªç¬‘è¯"
]

def batch_test(model_name="mlx-community/MiniMax-M2.1-4bit"):
    """æ‰¹é‡æµ‹è¯•"""

    print("Loading model...")
    model, tokenizer = load(model_name)
    print("Model loaded!\n")

    sampler = make_sampler(temp=0.7)

    for i, prompt in enumerate(test_prompts, 1):
        print(f"\n{'='*60}")
        print(f"Test {i}/{len(test_prompts)}")
        print(f"{'='*60}")
        print(f"Prompt: {prompt}\n")

        start_time = time.time()
        response = generate(
            model,
            tokenizer,
            prompt=prompt,
            max_tokens=200,
            sampler=sampler,
            verbose=False
        )
        gen_time = time.time() - start_time

        tokens = len(tokenizer.encode(response))
        tps = tokens / gen_time if gen_time > 0 else 0

        print(f"Response:\n{response}\n")
        print(f"Stats: {tokens} tokens in {gen_time:.2f}s ({tps:.1f} TPS)")

if __name__ == "__main__":
    batch_test()
```

è¿è¡Œï¼š

```bash
python batch_test.py
```

---

## ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šä»£ç ç”Ÿæˆ

```bash
mlx_lm.generate \
  --model mlx-community/MiniMax-M2.1-4bit \
  --prompt "å†™ä¸€ä¸ªPythonå‡½æ•°ï¼Œè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬né¡¹" \
  --max-tokens 300
```

### ç¤ºä¾‹2ï¼šæ–‡æœ¬ç¿»è¯‘

```bash
mlx_lm.generate \
  --model mlx-community/MiniMax-M2.1-4bit \
  --prompt "å°†ä»¥ä¸‹è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡ï¼šMachine learning is a subset of artificial intelligence" \
  --max-tokens 100
```

### ç¤ºä¾‹3ï¼šé—®ç­”

```bash
mlx_lm.generate \
  --model mlx-community/MiniMax-M2.1-4bit \
  --prompt "ä»€ä¹ˆæ˜¯æ¢¯åº¦ä¸‹é™ï¼Ÿè¯·ç”¨ç®€å•çš„è¯­è¨€è§£é‡Š" \
  --max-tokens 300
```

### ç¤ºä¾‹4ï¼šé•¿æ–‡æœ¬ç”Ÿæˆ

```bash
mlx_lm.generate \
  --model mlx-community/MiniMax-M2.1-4bit \
  --prompt "å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½æœªæ¥å‘å±•çš„æ–‡ç« " \
  --max-tokens 2000 \
  --temp 0.8
```

---

## æ€§èƒ½ä¼˜åŒ–

### 1. é€‰æ‹©åˆé€‚çš„æ¨¡å‹ç‰ˆæœ¬

æ ¹æ®ä½ çš„éœ€æ±‚é€‰æ‹©ï¼š

```bash
# é€Ÿåº¦ä¼˜å…ˆï¼ˆæ¨èæ—¥å¸¸ä½¿ç”¨ï¼‰
mlx_lm.generate --model mlx-community/MiniMax-M2.1-4bit ...

# è´¨é‡ä¼˜å…ˆï¼ˆé‡è¦ä»»åŠ¡ï¼‰
mlx_lm.generate --model mlx-community/MiniMax-M2.1-8bit ...

# å¹³è¡¡ï¼ˆæŠ˜ä¸­æ–¹æ¡ˆï¼‰
mlx_lm.generate --model mlx-community/MiniMax-M2.1-6bit ...
```

### 2. è°ƒæ•´ç”Ÿæˆå‚æ•°

```python
from mlx_lm.sample_utils import make_sampler

# æ›´å¿«ä½†å¯èƒ½è´¨é‡ç•¥ä½
sampler = make_sampler(temp=0.5, top_p=0.9)

# æ›´æœ‰åˆ›æ„ä½†å¯èƒ½ä¸å¤Ÿç²¾ç¡®
sampler = make_sampler(temp=0.9, top_p=0.95)

# å¹³è¡¡ï¼ˆæ¨èï¼‰
sampler = make_sampler(temp=0.7, top_p=0.9)
```

### 3. ä¼˜åŒ–VRAMï¼ˆå¯é€‰ï¼Œé«˜çº§ï¼‰

```bash
# æ£€æŸ¥å½“å‰VRAMé™åˆ¶
sysctl iogpu.wired_limit_mb

# å¢åŠ VRAMé™åˆ¶åˆ°448GBï¼ˆæ¨èï¼‰
sudo sysctl iogpu.wired_limit_mb=458752

# è¿è¡Œæ¨¡å‹
mlx_lm.generate --model mlx-community/MiniMax-M2.1-4bit ...
```

### 4. é¢„åŠ è½½æ¨¡å‹ï¼ˆé¿å…é‡å¤åŠ è½½ï¼‰

å¯¹äºé¢‘ç¹ä½¿ç”¨ï¼Œä½¿ç”¨Pythonè„šæœ¬ä¿æŒæ¨¡å‹åœ¨å†…å­˜ä¸­ï¼š

```python
# ä¸€æ¬¡åŠ è½½ï¼Œå¤šæ¬¡ä½¿ç”¨
model, tokenizer = load("mlx-community/MiniMax-M2.1-4bit")

# å¤šæ¬¡ç”Ÿæˆä¸éœ€è¦é‡æ–°åŠ è½½
for prompt in prompts:
    response = generate(model, tokenizer, prompt=prompt)
```

---

## æ•…éšœæ’é™¤

### é—®é¢˜1ï¼šæ¨¡å‹ä¸‹è½½å¤±è´¥

**ç—‡çŠ¶ï¼š** `Connection timeout` æˆ–ä¸‹è½½ä¸­æ–­

**è§£å†³æ–¹æ¡ˆï¼š**

```bash
# æ–¹æ³•1ï¼šä½¿ç”¨é•œåƒï¼ˆå¦‚æœåœ¨ä¸­å›½ï¼‰
export HF_ENDPOINT=https://hf-mirror.com
mlx_lm.generate --model mlx-community/MiniMax-M2.1-4bit ...

# æ–¹æ³•2ï¼šæ‰‹åŠ¨ä¸‹è½½
pip install huggingface-hub
huggingface-cli download mlx-community/MiniMax-M2.1-4bit \
  --local-dir ~/models/MiniMax-M2.1-4bit \
  --resume-download

# ç„¶åä½¿ç”¨æœ¬åœ°è·¯å¾„
mlx_lm.generate --model ~/models/MiniMax-M2.1-4bit ...
```

### é—®é¢˜2ï¼šå†…å­˜ä¸è¶³

**ç—‡çŠ¶ï¼š** `Out of memory` æˆ–ç³»ç»Ÿå¡æ­»

**è§£å†³æ–¹æ¡ˆï¼š**

```bash
# 1. ä½¿ç”¨æ›´å°çš„æ¨¡å‹
mlx_lm.generate --model mlx-community/MiniMax-M2.1-4bit ...  # è€Œä¸æ˜¯8-bit

# 2. å‡å°‘max_tokens
mlx_lm.generate --model ... --max-tokens 100  # è€Œä¸æ˜¯2000

# 3. å…³é—­å…¶ä»–åº”ç”¨ç¨‹åº
# åœ¨Activity Monitorä¸­å…³é—­ä¸éœ€è¦çš„åº”ç”¨

# 4. é‡å¯Macæ¸…ç†å†…å­˜
sudo reboot
```

### é—®é¢˜3ï¼šç”Ÿæˆé€Ÿåº¦æ…¢

**ç—‡çŠ¶ï¼š** TPS < 10

**è¯Šæ–­ï¼š**

```bash
# æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†Metal GPU
python -c "import mlx.core as mx; print(mx.metal.is_available())"

# æ£€æŸ¥å†…å­˜å‹åŠ›
memory_pressure

# ç›‘æ§GPUä½¿ç”¨
sudo powermetrics --samplers gpu_power -i 1000
```

**è§£å†³æ–¹æ¡ˆï¼š**

1. ç¡®ä¿MLXä½¿ç”¨Metalï¼šåº”è¯¥æ˜¯è‡ªåŠ¨çš„
2. ä¼˜åŒ–VRAMé™åˆ¶ï¼ˆè§ä¸Šæ–‡ï¼‰
3. ä½¿ç”¨4-bitæ¨¡å‹
4. é‡å¯Mac

### é—®é¢˜4ï¼šç”Ÿæˆç»“æœè´¨é‡å·®

**ç—‡çŠ¶ï¼š** è¾“å‡ºä¸è¿è´¯æˆ–é‡å¤

**è§£å†³æ–¹æ¡ˆï¼š**

```bash
# è°ƒæ•´temperature
mlx_lm.generate --model ... --temp 0.7  # é»˜è®¤

# æ›´ç¡®å®šæ€§ï¼ˆè´¨é‡æ›´ç¨³å®šï¼‰
mlx_lm.generate --model ... --temp 0.5

# æ›´æœ‰åˆ›æ„ï¼ˆä½†å¯èƒ½ä¸ç¨³å®šï¼‰
mlx_lm.generate --model ... --temp 0.9
```

æˆ–åœ¨Pythonä¸­ï¼š

```python
from mlx_lm.sample_utils import make_sampler

# æ›´ç¨³å®šçš„è¾“å‡º
sampler = make_sampler(temp=0.5, top_p=0.9)

# æˆ–è€…ä½¿ç”¨ä¸åŒçš„é‡‡æ ·ç­–ç•¥
sampler = make_sampler(temp=0.7, top_p=0.95, top_k=50)
```

### é—®é¢˜5ï¼šImportError

**ç—‡çŠ¶ï¼š** `ModuleNotFoundError: No module named 'mlx'`

**è§£å†³æ–¹æ¡ˆï¼š**

```bash
# ç¡®ä¿è™šæ‹Ÿç¯å¢ƒæ¿€æ´»
source venv/bin/activate

# é‡æ–°å®‰è£…MLX
pip install --upgrade mlx mlx-lm

# éªŒè¯
python -c "import mlx; import mlx_lm; print('OK')"
```

### é—®é¢˜6ï¼šæ¨¡å‹æ–‡ä»¶æŸå

**ç—‡çŠ¶ï¼š** `Error loading model` æˆ– `Invalid file`

**è§£å†³æ–¹æ¡ˆï¼š**

```bash
# æ¸…ç†ç¼“å­˜
rm -rf ~/.cache/huggingface/hub/models--mlx-community--MiniMax-M2.1-4bit

# é‡æ–°ä¸‹è½½
mlx_lm.generate --model mlx-community/MiniMax-M2.1-4bit ...
```

---

## å¸¸è§é—®é¢˜ FAQ

### Q1: é¦–æ¬¡ä¸‹è½½éœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿ

**A:** å–å†³äºç½‘é€Ÿå’Œæ¨¡å‹å¤§å°ï¼š
- 4-bit (~120GB): 1-3å°æ—¶ï¼ˆ100Mbpsç½‘ç»œï¼‰
- 8-bit (~240GB): 2-6å°æ—¶

### Q2: æ¨¡å‹å­˜å‚¨åœ¨å“ªé‡Œï¼Ÿ

**A:** é»˜è®¤ä½ç½®ï¼š
```bash
~/.cache/huggingface/hub/
```

æŸ¥çœ‹å ç”¨ç©ºé—´ï¼š
```bash
du -sh ~/.cache/huggingface/hub/
```

æ¸…ç†ç¼“å­˜ï¼š
```bash
rm -rf ~/.cache/huggingface/hub/*
```

### Q3: å¯ä»¥åŒæ—¶è¿è¡Œå¤šä¸ªæ¨¡å‹å—ï¼Ÿ

**A:** å¯ä»¥ï¼Œä½†å–å†³äºå†…å­˜ï¼š
- 512GBç³»ç»Ÿï¼šå¯ä»¥åŒæ—¶åŠ è½½4-bit (135GB) + 8-bit (252GB)
- å»ºè®®ï¼šä¸€æ¬¡è¿è¡Œä¸€ä¸ªæ¨¡å‹ï¼Œé¿å…å†…å­˜å‹åŠ›

### Q4: å¦‚ä½•ç¦»çº¿ä½¿ç”¨ï¼Ÿ

**A:** ä¸‹è½½åå³å¯ç¦»çº¿ä½¿ç”¨ï¼š

```bash
# åœ¨çº¿æ—¶ä¸‹è½½
mlx_lm.generate --model mlx-community/MiniMax-M2.1-4bit --prompt "test"

# ä¹‹åå¯ä»¥æ–­ç½‘ä½¿ç”¨
# æ¨¡å‹å·²ç¼“å­˜åœ¨ ~/.cache/huggingface/hub/
```

### Q5: å¦‚ä½•æ›´æ–°MLXï¼Ÿ

**A:**
```bash
source venv/bin/activate
pip install --upgrade mlx mlx-lm
```

### Q6: æ”¯æŒå“ªäº›è¯­è¨€ï¼Ÿ

**A:** MiniMax M2.1ä¸»è¦ä¼˜åŒ–ä¸­æ–‡å’Œè‹±æ–‡ï¼Œä½†ä¹Ÿæ”¯æŒå…¶ä»–è¯­è¨€ã€‚

---

## æ€§èƒ½å‚è€ƒ

**ä½ çš„ç³»ç»Ÿï¼ˆM3 Ultra 512GBï¼‰é¢„æœŸæ€§èƒ½ï¼š**

| æ¨¡å‹ | åŠ è½½æ—¶é—´ | å†…å­˜å ç”¨ | TPS | TTFT |
|------|---------|---------|-----|------|
| 4-bit | ~21ç§’ | 135 GB | 45.73 | 67ms |
| 6-bit | ~30ç§’ | 192 GB | 41.83 | 75ms |
| 8-bit | ~28ç§’ | 252 GB | 33.04 | 95ms |

---

## ä¸‹ä¸€æ­¥

1. âœ… **å¼€å§‹ä½¿ç”¨**ï¼šè¿è¡Œ4-bitæ¨¡å‹æµ‹è¯•
2. ğŸ“Š **æ€§èƒ½æµ‹è¯•**ï¼šä½¿ç”¨benchmarkè„šæœ¬
3. ğŸ”§ **ä¼˜åŒ–é…ç½®**ï¼šè°ƒæ•´VRAMå’Œå‚æ•°
4. ğŸš€ **ç”Ÿäº§éƒ¨ç½²**ï¼šæ„å»ºåº”ç”¨æˆ–API

---

## ç›¸å…³èµ„æº

- **é¡¹ç›®æ ¹ç›®å½•ï¼š** æœ¬é¡¹ç›®æ‰€åœ¨ç›®å½•
- **æµ‹è¯•è„šæœ¬ï¼š** `scripts/benchmark_mlx.py`
- **æ€§èƒ½ç»“æœï¼š** `docs/benchmark-results.md`
- **æµ‹è¯•è®¡åˆ’ï¼š** `docs/test-plan.md`

---

## è·å–å¸®åŠ©

å¦‚æœé‡åˆ°é—®é¢˜ï¼š

1. æŸ¥çœ‹æœ¬æ–‡æ¡£çš„"æ•…éšœæ’é™¤"éƒ¨åˆ†
2. æŸ¥çœ‹ `docs/benchmark-results.md` çš„å·²çŸ¥é—®é¢˜
3. æ£€æŸ¥ MLX GitHub Issues: https://github.com/ml-explore/mlx
4. æ£€æŸ¥ mlx-lm GitHub: https://github.com/ml-explore/mlx-lm

---

**ç¥ä½¿ç”¨æ„‰å¿«ï¼ğŸš€**
