# LM Studio å®Œæ•´è®¾ç½®æŒ‡å—

> ä½¿ç”¨ LM Studio åœ¨ Mac ä¸Šæœ¬åœ°è¿è¡Œ MiniMax M2.1 æ¨¡å‹

## ç›®å½•

- [ä¸ºä»€ä¹ˆé€‰æ‹© LM Studio](#ä¸ºä»€ä¹ˆé€‰æ‹©-lm-studio)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [è¯¦ç»†å®‰è£…æ­¥éª¤](#è¯¦ç»†å®‰è£…æ­¥éª¤)
- [æ¨¡å‹ä¸‹è½½ä¸åŠ è½½](#æ¨¡å‹ä¸‹è½½ä¸åŠ è½½)
- [å¯åŠ¨ API æœåŠ¡å™¨](#å¯åŠ¨-api-æœåŠ¡å™¨)
- [OpenClaw é›†æˆ](#openclaw-é›†æˆ)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

## ä¸ºä»€ä¹ˆé€‰æ‹© LM Studio

âœ… **ä¼˜åŠ¿**
- ğŸ–¥ï¸ **å›¾å½¢ç•Œé¢**: æ— éœ€å‘½ä»¤è¡Œï¼Œæ˜“äºä½¿ç”¨
- ğŸš€ **å¼€ç®±å³ç”¨**: è‡ªåŠ¨å¤„ç†ä¾èµ–å’Œé…ç½®
- ğŸ”Œ **OpenAI å…¼å®¹**: åŸç”Ÿæä¾› OpenAI API
- ğŸ’ª **é«˜æ€§èƒ½**: é’ˆå¯¹ Apple Silicon ä¼˜åŒ–
- ğŸ“Š **å®æ—¶ç›‘æ§**: GPU/CPU/å†…å­˜ä½¿ç”¨å¯è§†åŒ–
- ğŸ”„ **æ¨¡å‹ç®¡ç†**: è½»æ¾åˆ‡æ¢ä¸åŒæ¨¡å‹

ğŸ“‰ **å¯¹æ¯” MLX**
- LM Studio: GUI æ“ä½œï¼Œæ›´å‹å¥½
- MLX: å‘½ä»¤è¡Œæ“ä½œï¼Œéœ€è¦ Python ç¯å¢ƒ

## å¿«é€Ÿå¼€å§‹

### 3 æ­¥è¿è¡Œèµ·æ¥ (GUI æ–¹å¼)

```bash
# 1. ä¸‹è½½å¹¶å®‰è£… LM Studio
open https://lmstudio.ai

# 2. åœ¨ LM Studio ä¸­ä¸‹è½½æ¨¡å‹
# æœç´¢: mlx-community/MiniMax-M2.1-8bit
# æˆ–ä½¿ç”¨: unsloth/MiniMax-M2.1-GGUF (Q4_K_M)

# 3. å¯åŠ¨æœ¬åœ°æœåŠ¡å™¨
# åœ¨ LM Studio: Local Server -> Start Server
```

### 3 æ­¥è¿è¡Œèµ·æ¥ (CLI æ–¹å¼) â­ æ¨è

```bash
# 1. å®‰è£… LM Studio (å¦‚æœæœªå®‰è£…)
brew install --cask lm-studio

# 2. ä¸‹è½½æ¨¡å‹
lms download mlx-community/MiniMax-M2.1-4bit

# 3. å¯åŠ¨ API æœåŠ¡å™¨
lms server start mlx-community/MiniMax-M2.1-4bit --port 1234
```

å®Œæˆï¼ç°åœ¨å¯ä»¥åœ¨ `http://localhost:1234` ä½¿ç”¨ APIã€‚

## è¯¦ç»†å®‰è£…æ­¥éª¤

### 1. å®‰è£… LM Studio

**ä¸‹è½½:**
- å®˜ç½‘: https://lmstudio.ai
- æˆ–ç›´æ¥ä¸‹è½½: https://lmstudio.ai/download

**å®‰è£…:**
```bash
# ä¸‹è½½ .dmg æ–‡ä»¶å
1. æ‰“å¼€ LMStudio.dmg
2. æ‹–æ‹½åˆ° Applications æ–‡ä»¶å¤¹
3. æ‰“å¼€ LM Studio
4. å…è®¸å¿…è¦çš„ç³»ç»Ÿæƒé™
```

**é¦–æ¬¡å¯åŠ¨:**
- åŒæ„è®¸å¯åè®®
- å¯é€‰ï¼šç™»å½•è´¦å·ï¼ˆç”¨äºåŒæ­¥è®¾ç½®ï¼‰
- å®Œæˆåˆå§‹è®¾ç½®å‘å¯¼

### 2. é…ç½® LM Studio

**GUI æ–¹å¼:**

1. **æ‰“å¼€è®¾ç½® (âš™ï¸)**
   - `File > Preferences` æˆ– `Cmd+,`

2. **GPU è®¾ç½®**
   ```
   GPU Offload: Auto (æ¨è)
   GPU Layers: Max (æˆ–æ ¹æ®å†…å­˜è°ƒæ•´)
   Context Length: 32768 (æˆ–æ›´é«˜)
   ```

3. **API æœåŠ¡å™¨è®¾ç½®**
   ```
   Port: 1234 (é»˜è®¤)
   CORS: Enabled (å¦‚æœéœ€è¦ç½‘é¡µè®¿é—®)
   API Key: å¯é€‰ï¼ˆæœ¬åœ°ä½¿ç”¨ä¸éœ€è¦ï¼‰
   ```

**CLI æ–¹å¼:**

```bash
# æŸ¥çœ‹å½“å‰é…ç½®
lms config list

# è®¾ç½® GPU layers
lms config set gpu.layers -1  # -1 = å…¨éƒ¨

# è®¾ç½®é»˜è®¤ç«¯å£
lms config set server.port 1234

# å¯ç”¨ CORS
lms config set server.cors true
```

## æ¨¡å‹ä¸‹è½½ä¸åŠ è½½

### æ¨èæ¨¡å‹

#### é€‰é¡¹ 1: MLX æ ¼å¼ (æ¨è for Mac)

```
mlx-community/MiniMax-M2.1-4bit  (~120GB) â­ æ¨è
mlx-community/MiniMax-M2.1-8bit  (~240GB) - è´¨é‡æ›´å¥½
```

#### é€‰é¡¹ 2: GGUF æ ¼å¼

```
unsloth/MiniMax-M2.1-GGUF:Q4_K_M  (~138GB)
unsloth/MiniMax-M2.1-GGUF:Q8_0    (~243GB)
```

### åœ¨ LM Studio ä¸­ä¸‹è½½æ¨¡å‹

**æ–¹æ³• 1: CLI ä¸‹è½½** â­ æ¨è

```bash
# ä¸‹è½½ 4-bit æ¨¡å‹ (æœ€å¿«)
lms download mlx-community/MiniMax-M2.1-4bit

# ä¸‹è½½ 8-bit æ¨¡å‹ (è´¨é‡æ›´å¥½)
lms download mlx-community/MiniMax-M2.1-8bit

# ä¸‹è½½ GGUF ç‰ˆæœ¬
lms download unsloth/MiniMax-M2.1-GGUF:Q4_K_M

# æŸ¥çœ‹ä¸‹è½½è¿›åº¦
lms download list

# åˆ—å‡ºå·²ä¸‹è½½çš„æ¨¡å‹
lms models list
```

**æ–¹æ³• 2: GUI æœç´¢ä¸‹è½½**

1. ç‚¹å‡»å·¦ä¾§ **ğŸ” Search** æ ‡ç­¾
2. æœç´¢æ è¾“å…¥: `MiniMax-M2.1`
3. æ‰¾åˆ°æ¨¡å‹:
   - `mlx-community/MiniMax-M2.1-4bit` (Mac æ¨è)
   - `unsloth/MiniMax-M2.1-GGUF` (é€šç”¨)
4. ç‚¹å‡» **Download**
5. ç­‰å¾…ä¸‹è½½å®Œæˆï¼ˆ120-240GBï¼Œéœ€è¦ 30-120 åˆ†é’Ÿï¼‰

**æ–¹æ³• 3: ä½¿ç”¨å·²ä¸‹è½½çš„æ¨¡å‹**

å¦‚æœä½ å·²ç»é€šè¿‡å…¶ä»–æ–¹å¼ä¸‹è½½äº†æ¨¡å‹ï¼ˆå¦‚ MLX æˆ– Hugging Face CLIï¼‰ï¼Œå¯ä»¥åˆ›å»ºç¬¦å·é“¾æ¥ï¼š

```bash
# MLX æ¨¡å‹ -> LM Studio
ln -s ~/.cache/huggingface/hub/models--mlx-community--MiniMax-M2.1-8bit/snapshots/* \
      ~/.lmstudio/models/mlx-community/MiniMax-M2.1-8bit/

# GGUF æ¨¡å‹ -> LM Studio
ln -s /path/to/MiniMax-M2.1-Q4_K_M.gguf \
      ~/.lmstudio/models/unsloth/MiniMax-M2.1-GGUF/
```

é‡å¯ LM Studio åï¼Œæ¨¡å‹ä¼šå‡ºç°åœ¨åˆ—è¡¨ä¸­ã€‚

### åŠ è½½æ¨¡å‹

1. ç‚¹å‡»å·¦ä¾§ **ğŸ’¬ Chat** æ ‡ç­¾
2. ç‚¹å‡»é¡¶éƒ¨æ¨¡å‹é€‰æ‹©å™¨
3. é€‰æ‹© `MiniMax-M2.1-4bit` (æˆ–å…¶ä»–ç‰ˆæœ¬)
4. ç­‰å¾…æ¨¡å‹åŠ è½½ï¼ˆé¦–æ¬¡åŠ è½½éœ€è¦ 20-30 ç§’ï¼‰

**åŠ è½½æˆåŠŸæ ‡å¿—:**
- åº•éƒ¨æ˜¾ç¤º âœ… "Model loaded"
- GPU/CPU ä½¿ç”¨ç‡å‡ºç°åœ¨çŠ¶æ€æ 
- å¯ä»¥åœ¨èŠå¤©æ¡†è¾“å…¥æ¶ˆæ¯

## å¯åŠ¨ API æœåŠ¡å™¨

### CLI æ–¹å¼ â­ æ¨è

```bash
# å¯åŠ¨æœåŠ¡å™¨ (è‡ªåŠ¨é€‰æ‹©æœ€è¿‘ä½¿ç”¨çš„æ¨¡å‹)
lms server start

# å¯åŠ¨æœåŠ¡å™¨å¹¶æŒ‡å®šæ¨¡å‹
lms server start mlx-community/MiniMax-M2.1-4bit

# è‡ªå®šä¹‰ç«¯å£
lms server start --port 8080

# åå°è¿è¡Œ
lms server start --detach

# æŸ¥çœ‹æœåŠ¡å™¨çŠ¶æ€
lms server status

# åœæ­¢æœåŠ¡å™¨
lms server stop

# æŸ¥çœ‹æ—¥å¿—
lms server logs
```

**é«˜çº§é€‰é¡¹:**

```bash
# å®Œæ•´å‘½ä»¤
lms server start \
  --model mlx-community/MiniMax-M2.1-4bit \
  --port 1234 \
  --host 0.0.0.0 \
  --cors true \
  --gpu-layers -1 \
  --ctx-size 32768
```

### GUI æ–¹å¼

1. ç‚¹å‡»å·¦ä¾§ **âš¡ Local Server** æ ‡ç­¾
2. é€‰æ‹©å·²åŠ è½½çš„æ¨¡å‹
3. é…ç½®æœåŠ¡å™¨:
   ```
   Port: 1234
   CORS: âœ… Enabled
   Auto-start: âœ… (å¯é€‰)
   ```
4. ç‚¹å‡» **Start Server** ğŸš€

**æœåŠ¡å™¨å¯åŠ¨å:**
```
âœ… Server running on http://localhost:1234
ğŸ“¡ Endpoints available:
   â€¢ /v1/models
   â€¢ /v1/chat/completions
   â€¢ /v1/completions
```

### å‘½ä»¤è¡Œæµ‹è¯•

```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:1234/v1/models

# æµ‹è¯•å¯¹è¯
curl http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "minimax-m2.1",
    "messages": [{"role": "user", "content": "ä½ å¥½"}],
    "max_tokens": 100,
    "temperature": 0.7
  }'
```

### Python æµ‹è¯•

```python
import openai

openai.api_base = "http://localhost:1234/v1"
openai.api_key = "lm-studio"  # ä»»æ„å€¼

response = openai.ChatCompletion.create(
    model="minimax-m2.1",
    messages=[{"role": "user", "content": "Hello"}],
    max_tokens=100
)

print(response.choices[0].message.content)
```

## OpenClaw é›†æˆ

### é…ç½® OpenClaw

**æ–¹æ³• 1: ç¯å¢ƒå˜é‡**

```bash
export OPENAI_API_BASE="http://localhost:1234/v1"
export OPENAI_API_KEY="lm-studio"
```

**æ–¹æ³• 2: é…ç½®æ–‡ä»¶** (æ¨è)

ç¼–è¾‘ `~/.openclaw/openclaw.json`:

```json
{
  "models": {
    "providers": {
      "lmstudio": {
        "baseUrl": "http://localhost:1234/v1",
        "api": "openai-completions",
        "apiKey": "lm-studio",
        "models": [
          {
            "id": "minimax-m2.1",
            "name": "MiniMax M2.1 (LM Studio)",
            "reasoning": true,
            "contextWindow": 200000,
            "maxTokens": 8192,
            "inputPricePerToken": 0,
            "outputPricePerToken": 0
          }
        ]
      }
    }
  },
  "agents": {
    "defaults": {
      "model": {
        "primary": "lmstudio/minimax-m2.1"
      }
    }
  }
}
```

**æ–¹æ³• 3: config.yaml**

å¦‚æœä½¿ç”¨ YAML é…ç½®:

```yaml
llm:
  provider: openai
  base_url: http://localhost:1234/v1
  api_key: lm-studio
  model: minimax-m2.1
  temperature: 0.7
  max_tokens: 4000
```

### é‡å¯ OpenClaw

```bash
# é‡å¯ gateway
openclaw gateway restart

# æˆ–é‡å¯æ•´ä¸ª OpenClaw
openclaw restart
```

### æµ‹è¯•é›†æˆ

```bash
# ä½¿ç”¨ OpenClaw CLI
openclaw chat "è¯·ä»‹ç»ä¸€ä¸‹é‡å­è®¡ç®—"

# æ£€æŸ¥æ¨¡å‹çŠ¶æ€
openclaw models list
```

## æ€§èƒ½ä¼˜åŒ–

### GPU é…ç½®

**æœ€å¤§åŒ– GPU ä½¿ç”¨:**

1. LM Studio è®¾ç½®:
   ```
   GPU Offload: Max
   GPU Layers: -1 (å…¨éƒ¨)
   Metal: Enabled
   ```

2. å…³é—­å…¶ä»–åº”ç”¨:
   - é‡Šæ”¾å†…å­˜å’Œ GPU
   - æå‡æ¨ç†é€Ÿåº¦

### å†…å­˜ä¼˜åŒ–

**512GB Mac æ¨èé…ç½®:**

| æ¨¡å‹ç‰ˆæœ¬ | å†…å­˜å ç”¨ | æ¨è GPU Layers | é¢„æœŸ TPS |
|---------|---------|----------------|---------|
| 4-bit   | ~135GB  | Max (-1)       | 40-45   |
| 8-bit   | ~240GB  | Max (-1)       | 30-35   |
| Q4_K_M  | ~140GB  | Max (-1)       | 35-40   |
| Q8_0    | ~250GB  | Max (-1)       | 28-33   |

**è°ƒæ•´ç³»ç»Ÿ VRAM é™åˆ¶:**

```bash
# æŸ¥çœ‹å½“å‰é™åˆ¶
sysctl iogpu.wired_limit_mb

# å¢åŠ åˆ° 448GB (å¦‚æœéœ€è¦)
sudo sysctl iogpu.wired_limit_mb=458752
```

### æ¨ç†å‚æ•°è°ƒä¼˜

**å“åº”é€Ÿåº¦ä¼˜å…ˆ:**
```json
{
  "temperature": 0.3,
  "top_p": 0.9,
  "max_tokens": 1000,
  "stream": true
}
```

**è´¨é‡ä¼˜å…ˆ:**
```json
{
  "temperature": 0.7,
  "top_p": 0.95,
  "max_tokens": 4000,
  "stream": true
}
```

## æ•…éšœæ’é™¤

### æ¨¡å‹æœªæ˜¾ç¤º

**é—®é¢˜:** ä¸‹è½½çš„æ¨¡å‹ä¸åœ¨åˆ—è¡¨ä¸­

**è§£å†³:**
```bash
# 1. æ£€æŸ¥æ¨¡å‹è·¯å¾„
ls -la ~/.lmstudio/models/

# 2. æ£€æŸ¥æƒé™
chmod -R 755 ~/.lmstudio/models/

# 3. é‡å¯ LM Studio
killall "LM Studio" && open -a "LM Studio"
```

### æœåŠ¡å™¨å¯åŠ¨å¤±è´¥

**é—®é¢˜:** "Port 1234 already in use"

**è§£å†³:**
```bash
# æŸ¥æ‰¾å ç”¨ç«¯å£çš„è¿›ç¨‹
lsof -i :1234

# æ€æ­»è¿›ç¨‹
kill -9 <PID>

# æˆ–ä½¿ç”¨å…¶ä»–ç«¯å£
# LM Studio -> Settings -> Server Port: 8080
```

### å†…å­˜ä¸è¶³ (OOM)

**é—®é¢˜:** "Out of memory" æˆ–æ¨¡å‹åŠ è½½å¤±è´¥

**è§£å†³:**
1. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ (4-bit è€Œé 8-bit)
2. å‡å°‘ GPU Layers
3. å…³é—­å…¶ä»–åº”ç”¨
4. é‡å¯ Mac æ¸…ç†å†…å­˜

```bash
# æ£€æŸ¥å†…å­˜ä½¿ç”¨
vm_stat

# æ¸…ç†å†…å­˜ (é‡å¯)
sudo purge
```

### å“åº”é€Ÿåº¦æ…¢ (<10 TPS)

**é—®é¢˜:** ç”Ÿæˆé€Ÿåº¦å¾ˆæ…¢

**è¯Šæ–­:**
```bash
# 1. æ£€æŸ¥ GPU ä½¿ç”¨
# åœ¨ LM Studio åº•éƒ¨æŸ¥çœ‹ GPU ä½¿ç”¨ç‡

# 2. æ£€æŸ¥ç³»ç»Ÿèµ„æº
Activity Monitor -> GPU -> % Use
```

**è§£å†³:**
1. ç¡®ä¿ GPU Offload = Max
2. å…³é—­åå°åº”ç”¨ (Chrome, Docker, etc.)
3. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ (4-bit)
4. å‡å°‘ context length

### OpenClaw è¿æ¥å¤±è´¥

**é—®é¢˜:** OpenClaw æ— æ³•è¿æ¥åˆ° LM Studio

**æ£€æŸ¥:**
```bash
# 1. æœåŠ¡å™¨æ˜¯å¦è¿è¡Œ
curl http://localhost:1234/v1/models

# 2. ç«¯å£æ˜¯å¦æ­£ç¡®
cat ~/.openclaw/openclaw.json | grep baseUrl

# 3. é˜²ç«å¢™è®¾ç½®
# System Settings > Network > Firewall
```

**è§£å†³:**
1. ç¡®ä¿ LM Studio æœåŠ¡å™¨åœ¨è¿è¡Œ
2. æ£€æŸ¥ OpenClaw é…ç½®ä¸­çš„ç«¯å£
3. é‡å¯ä¸¤ä¸ªæœåŠ¡

## LMS CLI å‘½ä»¤å‚è€ƒ

### å¸¸ç”¨å‘½ä»¤

```bash
# æ¨¡å‹ç®¡ç†
lms models list              # åˆ—å‡ºæœ¬åœ°æ¨¡å‹
lms models search minimax    # æœç´¢æ¨¡å‹
lms download <model-id>      # ä¸‹è½½æ¨¡å‹
lms models delete <model-id> # åˆ é™¤æ¨¡å‹

# æœåŠ¡å™¨ç®¡ç†
lms server start             # å¯åŠ¨æœåŠ¡å™¨
lms server start --detach    # åå°è¿è¡Œ
lms server stop              # åœæ­¢æœåŠ¡å™¨
lms server status            # æŸ¥çœ‹çŠ¶æ€
lms server logs              # æŸ¥çœ‹æ—¥å¿—
lms server restart           # é‡å¯æœåŠ¡å™¨

# é…ç½®ç®¡ç†
lms config list              # åˆ—å‡ºæ‰€æœ‰é…ç½®
lms config get <key>         # è·å–é…ç½®å€¼
lms config set <key> <value> # è®¾ç½®é…ç½®
lms config reset             # é‡ç½®ä¸ºé»˜è®¤

# å·¥å…·å‘½ä»¤
lms version                  # æŸ¥çœ‹ç‰ˆæœ¬
lms update                   # æ›´æ–° LM Studio
lms doctor                   # è¯Šæ–­é—®é¢˜
```

### å®Œæ•´ç¤ºä¾‹å·¥ä½œæµ

```bash
# 1. æœç´¢å¹¶ä¸‹è½½æ¨¡å‹
lms models search "MiniMax"
lms download mlx-community/MiniMax-M2.1-4bit

# 2. é…ç½®æœåŠ¡å™¨
lms config set server.port 1234
lms config set server.cors true
lms config set gpu.layers -1

# 3. å¯åŠ¨æœåŠ¡å™¨ (åå°)
lms server start mlx-community/MiniMax-M2.1-4bit --detach

# 4. æµ‹è¯•è¿æ¥
curl http://localhost:1234/v1/models

# 5. æŸ¥çœ‹æ—¥å¿—
lms server logs --tail 50

# 6. åœæ­¢æœåŠ¡å™¨
lms server stop
```

### ç¯å¢ƒå˜é‡

```bash
# LMS CLI é…ç½®
export LMS_HOME="$HOME/.lmstudio"
export LMS_SERVER_PORT="1234"
export LMS_GPU_LAYERS="-1"

# æ·»åŠ åˆ° ~/.bashrc æˆ– ~/.zshrc
echo 'export LMS_SERVER_PORT="1234"' >> ~/.zshrc
```

### è‡ªåŠ¨åŒ–è„šæœ¬

åˆ›å»º `start_lmstudio.sh`:

```bash
#!/bin/bash
# LM Studio è‡ªåŠ¨å¯åŠ¨è„šæœ¬

MODEL="mlx-community/MiniMax-M2.1-4bit"
PORT=1234

echo "ğŸš€ å¯åŠ¨ LM Studio æœåŠ¡å™¨..."

# æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½
if ! lms models list | grep -q "$MODEL"; then
    echo "ğŸ“¥ ä¸‹è½½æ¨¡å‹: $MODEL"
    lms download "$MODEL"
fi

# å¯åŠ¨æœåŠ¡å™¨
lms server start "$MODEL" \
    --port "$PORT" \
    --host 0.0.0.0 \
    --cors true \
    --gpu-layers -1 \
    --detach

echo "âœ… æœåŠ¡å™¨è¿è¡Œåœ¨ http://localhost:$PORT"
echo "ğŸ“Š æŸ¥çœ‹æ—¥å¿—: lms server logs"
echo "ğŸ›‘ åœæ­¢æœåŠ¡å™¨: lms server stop"
```

ä½¿ç”¨:
```bash
chmod +x start_lmstudio.sh
./start_lmstudio.sh
```

## é«˜çº§åŠŸèƒ½

### è‡ªå®šä¹‰ Chat Template

MiniMax M2.1 ä½¿ç”¨ç‰¹æ®Šçš„æ€è€ƒæ ¼å¼ `<think>...</think>`ã€‚å¯ä»¥åœ¨ LM Studio ä¸­é…ç½®ï¼š

1. æ‰“å¼€æ¨¡å‹è®¾ç½® (âš™ï¸)
2. æ‰¾åˆ° "Chat Template"
3. æ·»åŠ è‡ªå®šä¹‰æ¨¡æ¿ï¼ˆå¯é€‰ï¼‰

### API Key ä¿æŠ¤

å¦‚æœéœ€è¦åœ¨ç½‘ç»œä¸Šæš´éœ² APIï¼š

1. LM Studio Settings:
   ```
   API Key: your-secret-key
   Require Authentication: âœ…
   ```

2. OpenClaw é…ç½®:
   ```json
   {
     "apiKey": "your-secret-key"
   }
   ```

### å¤šæ¨¡å‹åˆ‡æ¢

LM Studio æ”¯æŒåŠ è½½å¤šä¸ªæ¨¡å‹å¹¶å¿«é€Ÿåˆ‡æ¢ï¼š

1. ä¸‹è½½å¤šä¸ªæ¨¡å‹ç‰ˆæœ¬
2. åœ¨ Chat æˆ– Server ç•Œé¢åˆ‡æ¢
3. OpenClaw ä¼šè‡ªåŠ¨é€‚åº”å½“å‰æ¨¡å‹

## æ€§èƒ½åŸºå‡†

### åœ¨ M3 Ultra 512GB ä¸Šçš„è¡¨ç°

| æ¨¡å‹ | åŠ è½½æ—¶é—´ | å†…å­˜ä½¿ç”¨ | TPS | TTFT |
|------|---------|---------|-----|------|
| MLX 4-bit | 21s | 135GB | 45.7 | 67ms |
| MLX 8-bit | 28s | 252GB | 33.0 | 95ms |
| GGUF Q4_K_M | ~25s | 140GB | ~40 | ~80ms |
| GGUF Q8_0 | ~30s | 250GB | ~30 | ~100ms |

### æ¨èé…ç½®

**äº¤äº’å¼ä½¿ç”¨ (å¯¹è¯/ç¼–ç¨‹):**
- æ¨¡å‹: `mlx-community/MiniMax-M2.1-4bit`
- ç†ç”±: æœ€å¿«é€Ÿåº¦ (45 TPS)ï¼Œä½å»¶è¿Ÿ (67ms)

**æ‰¹é‡å¤„ç† (æ–‡æ¡£ç”Ÿæˆ/åˆ†æ):**
- æ¨¡å‹: `mlx-community/MiniMax-M2.1-8bit`
- ç†ç”±: æ›´é«˜è´¨é‡ï¼Œå†…å­˜è¶³å¤Ÿ

## èµ„æºé“¾æ¥

- **LM Studio å®˜ç½‘**: https://lmstudio.ai
- **æ–‡æ¡£**: https://lmstudio.ai/docs
- **Discord ç¤¾åŒº**: https://discord.gg/lmstudio
- **æ¨¡å‹ä»“åº“**: https://huggingface.co/mlx-community
- **GGUF æ¨¡å‹**: https://huggingface.co/unsloth/MiniMax-M2.1-GGUF

## ä¸‹ä¸€æ­¥

1. âœ… å®‰è£… LM Studio
2. âœ… ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹
3. âœ… å¯åŠ¨ API æœåŠ¡å™¨
4. âœ… é…ç½® OpenClaw
5. ğŸ“Š è¿è¡Œæ€§èƒ½æµ‹è¯•
6. ğŸš€ å¼€å§‹ä½¿ç”¨ï¼

---

## å¤‡é€‰æ–¹æ¡ˆ

å¦‚æœä½ æƒ³ä½¿ç”¨å‘½ä»¤è¡Œæˆ–éœ€è¦æ›´å¤šæ§åˆ¶ï¼Œå¯ä»¥æŸ¥çœ‹ï¼š
- **MLX æ–¹å¼**: [docs/mlx-local-setup.md](mlx-local-setup.md)
- **llama.cpp æ–¹å¼**: [docs/test-plan.md](test-plan.md)

---

**å‡†å¤‡å¥½äº†å—ï¼Ÿ** ä¸‹è½½ LM Studio å¼€å§‹: https://lmstudio.ai
