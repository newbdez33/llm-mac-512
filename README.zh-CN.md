# MiniMax M2.1 åœ¨ 512GB ç»Ÿä¸€å†…å­˜ Mac ä¸Šçš„æ€§èƒ½æµ‹è¯•

åœ¨ Mac (512GB ç»Ÿä¸€å†…å­˜) ä¸Šå¯¹ MiniMax M2.1 æ¨¡å‹çš„å„ä¸ªå˜ä½“è¿›è¡Œå…¨é¢çš„æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œå¯¹æ¯” MLX å’Œ llama.cpp æ¡†æ¶ã€‚

[ä¸­æ–‡ç‰ˆ](./README.zh-CN.md) | [English](./README.md)

**ğŸš€ [å¿«é€Ÿå¼€å§‹ - 5åˆ†é’Ÿè¿è¡ŒMLX](./QUICKSTART.md)** | **ğŸ“– [å®Œæ•´æœ¬åœ°è¿è¡ŒæŒ‡å—](./docs/mlx-local-setup.md)** | **ğŸ”Œ [OpenClaw APIé…ç½®](./docs/openclaw-setup.md)**

## æ¨¡å‹æ¦‚è¿°

- **MiniMax M2.1**: 230B å‚æ•°çš„ MoE æ¨¡å‹ï¼ˆ10B æ¿€æ´»å‚æ•°ï¼‰
- **å‘å¸ƒæ—¥æœŸ**: 2025å¹´12æœˆ23æ—¥
- **ä¼˜åŒ–æ–¹å‘**: ä»£ç ç”Ÿæˆã€å·¥å…·ä½¿ç”¨ã€æŒ‡ä»¤è·Ÿéšã€é•¿æœŸè§„åˆ’

## æµ‹è¯•æœºå™¨é…ç½®

| è§„æ ¼ | è¯¦æƒ… |
|------|------|
| **å‹å·** | Mac Studio (Mac15,14) |
| **èŠ¯ç‰‡** | Apple M3 Ultra |
| **CPU æ ¸å¿ƒ** | 32æ ¸ï¼ˆ24æ€§èƒ½æ ¸ + 8æ•ˆç‡æ ¸ï¼‰|
| **ç»Ÿä¸€å†…å­˜** | 512 GB |
| **macOS** | 26.2 (Build 25C56) |
| **Python** | 3.12.12 |
| **MLX** | 0.30.4 |
| **mlx-lm** | 0.30.5 |

## ğŸš€ æ€§èƒ½æµ‹è¯•ç»“æœ

### MLX æ€§èƒ½æ€»ç»“

| ç‰ˆæœ¬ | åŠ è½½æ—¶é—´ | å†…å­˜å ç”¨ | å¹³å‡ TPS | TTFT (Prefill) | çŠ¶æ€ |
|------|---------|---------|----------|----------------|------|
| **4-bit** | 21.25ç§’ | 135 GB | **45.73** | 67ms | âœ… æ¨è |
| **6-bit** | 29.85ç§’ | 192 GB | **41.83** | 75ms | âœ… å®Œæˆ |
| **8-bit** | 28.07ç§’ | 252 GB | **33.04** | 95ms | âœ… å®Œæˆ |
| **bf16** | - | ~460 GB | N/A | N/A | âŒ æœªæä¾› |

### llama.cpp æ€§èƒ½æ€»ç»“

| ç‰ˆæœ¬ | åŠ è½½æ—¶é—´ | å†…å­˜å ç”¨ | å¹³å‡ TPS | çŠ¶æ€ |
|------|---------|---------|---------|------|
| **BF16** | - | 426 GB | <0.3 | âŒ å¤±è´¥ï¼ˆè¿è¡Œ6å°æ—¶åOOMï¼‰|
| **Q4_K_M** | - | ~140 GB | å¾…æµ‹ | â³ è®¡åˆ’ä¸­ |
| **Q8_0** | - | ~250 GB | å¾…æµ‹ | â³ è®¡åˆ’ä¸­ |

### å…³é”®å‘ç°

#### âœ… MLX 4-bitï¼ˆæ¨èé…ç½®ï¼‰
- **æ€§èƒ½æœ€ä½³**: 45.73 TPSï¼Œä»…å ç”¨ 135GB å†…å­˜
- **è¶…ä½å»¶è¿Ÿ**: 67ms TTFTï¼ˆprefillé€Ÿåº¦ï¼‰
- **ç¨³å®šç”Ÿæˆ**: é¢„çƒ­åç¨³å®šåœ¨ 48-49 TPS
- **å†…å­˜é«˜æ•ˆ**: ä¸ºå…¶ä»–å·¥ä½œè´Ÿè½½ç•™å‡º 377GB ç©ºé—´

#### âš¡ æ€§èƒ½æ´å¯Ÿ
- **Prefill é€Ÿåº¦**: æ‰€æœ‰é‡åŒ–çº§åˆ«éƒ½åœ¨ 60-95msï¼ˆæ¥è¿‘GPUæ°´å¹³ï¼‰
- **å†…å­˜æ‰©å±•**: ä¸é‡åŒ–ä½æ•°çº¿æ€§ç›¸å…³ï¼ˆ4â†’6â†’8 bitï¼‰
- **é€Ÿåº¦ vs è´¨é‡**: 4-bit å¯¹äº¤äº’å¼ä½¿ç”¨æä¾›æœ€ä½³å¹³è¡¡
- **8-bit æƒè¡¡**: æ…¢ 28%ï¼Œä½†è´¨é‡æ›´å¥½

#### âŒ BF16 ä¸å®ç”¨
- **llama.cpp BF16**: è¿è¡Œ 6+ å°æ—¶åå¤±è´¥ï¼Œç³»ç»Ÿ OOM æ€æ­»è¿›ç¨‹
- **å†…å­˜å‹åŠ›**: 83% ä½¿ç”¨ç‡å¯¼è‡´ä¸¥é‡æ€§èƒ½ä¸‹é™
- **å»ºè®®**: ä»»ä½•å®é™…å·¥ä½œè´Ÿè½½è¯·ä½¿ç”¨ 8-bit æˆ–æ›´ä½

> ğŸ“Š è¯¦ç»†ç»“æœ: [docs/benchmark-results.md](./docs/benchmark-results.md)

## ğŸ“‹ æµ‹è¯•è®¡åˆ’çŠ¶æ€

### âœ… Phase 1-2: å·²å®Œæˆ (50%)
- [x] ç¯å¢ƒæ­å»º
- [x] MLX 4-bit, 6-bit, 8-bit æ€§èƒ½æµ‹è¯•
- [x] llama.cpp BF16 å¤±è´¥åˆ†æ

### â³ Phase 3: llama.cpp é‡åŒ–æµ‹è¯•
- [ ] Q4_K_M (138GB) - å¯¹æ¯” MLX 4-bit
- [ ] Q8_0 (243GB) - å¯¹æ¯” MLX 8-bit

### ğŸ†• Phase 4: MLX Batching ä¸å¹¶å‘æµ‹è¯•
- [ ] vllm-mlx continuous batching æµ‹è¯•
- [ ] å¹¶å‘è¯·æ±‚æ‰©å±•æ€§æµ‹è¯•ï¼ˆ1/2/4/8/16 ç”¨æˆ·ï¼‰
- [ ] èšåˆååé‡æµ‹é‡
- [ ] æ··åˆå·¥ä½œè´Ÿè½½æµ‹è¯•

### ğŸ†• Phase 5: VRAM/å†…å­˜ä¼˜åŒ–
- [ ] ç³»ç»ŸVRAMé™åˆ¶è°ƒæ•´ï¼ˆé»˜è®¤384GB â†’ 448GB/480GBï¼‰
- [ ] llama.cpp Metalåç«¯ä¼˜åŒ–ï¼ˆFORCE_PRIVATEã€DEVICE_INDEXï¼‰
- [ ] æ€§èƒ½å½±å“æµ‹é‡
- [ ] å¤§æ¨¡å‹ä¼˜åŒ–ï¼ˆ8-bitã€bf16ï¼‰

> ğŸ“– å®Œæ•´æµ‹è¯•è®¡åˆ’: [docs/test-plan.md](./docs/test-plan.md)
> ğŸ”§ æ‰§è¡ŒæŒ‡å—: [docs/test-execution-guide.md](./docs/test-execution-guide.md)

## å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒé…ç½®

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv venv
source venv/bin/activate

# å®‰è£…ä¾èµ–
pip install -U mlx-lm psutil

# å®‰è£… llama.cppï¼ˆå¯é€‰ï¼‰
brew install llama.cpp
```

### 2. è¿è¡Œ MLX æµ‹è¯•

```bash
# æµ‹è¯• 4-bit ç‰ˆæœ¬ï¼ˆæ¨èå…ˆæµ‹è¯•ï¼‰
python scripts/benchmark_mlx.py --model mlx-community/MiniMax-M2.1-4bit

# æµ‹è¯• 8-bit ç‰ˆæœ¬
python scripts/benchmark_mlx.py --model mlx-community/MiniMax-M2.1-8bit

# æµ‹è¯•å…¨ç²¾åº¦ç‰ˆæœ¬ï¼ˆéœ€è¦ ~460GB å†…å­˜ï¼‰
python scripts/benchmark_mlx.py --model mlx-community/MiniMax-M2.1-bf16
```

### 3. è¿è¡Œ llama.cpp æµ‹è¯•

```bash
# ä¸‹è½½ GGUF æ¨¡å‹åè¿è¡Œ
python scripts/benchmark_llama.py --model /path/to/MiniMax-M2.1-Q4_K_M.gguf
```

### 4. è¿è¡Œ Batching æµ‹è¯•

```bash
# åŸºçº¿æµ‹è¯•ï¼ˆå•è¯·æ±‚ï¼‰
python scripts/benchmark_batching.py --model mlx-community/MiniMax-M2.1-4bit --concurrent 1

# å¹¶å‘æ‰©å±•æµ‹è¯•
python scripts/benchmark_batching.py --model mlx-community/MiniMax-M2.1-4bit --concurrent 4
python scripts/benchmark_batching.py --model mlx-community/MiniMax-M2.1-4bit --concurrent 8
python scripts/benchmark_batching.py --model mlx-community/MiniMax-M2.1-4bit --concurrent 16

# æ··åˆå·¥ä½œè´Ÿè½½
python scripts/benchmark_batching.py --model mlx-community/MiniMax-M2.1-4bit --concurrent 4 --mixed
```

## æµ‹è¯•çŸ©é˜µ

### MLX ç‰ˆæœ¬ï¼ˆmlx-communityï¼‰

| ç‰ˆæœ¬ | ä¼°è®¡å¤§å° | ä¼˜å…ˆçº§ | çŠ¶æ€ |
|------|---------|---------|------|
| MiniMax-M2.1-4bit | ~120GB | 1ï¼ˆå…ˆæµ‹è¯•ï¼‰| âœ… å®Œæˆ |
| MiniMax-M2.1-6bit | ~180GB | 2 | âœ… å®Œæˆ |
| MiniMax-M2.1-8bit | ~240GB | 3 | âœ… å®Œæˆ |
| MiniMax-M2.1-bf16 | ~460GB | 4ï¼ˆå…¨ç²¾åº¦ï¼‰| âŒ æœªæä¾› |

### GGUF ç‰ˆæœ¬ï¼ˆunsloth/MiniMax-M2.1-GGUFï¼‰

| ç‰ˆæœ¬ | æ–‡ä»¶å¤§å° | ä¼˜å…ˆçº§ | çŠ¶æ€ |
|------|---------|---------|------|
| Q4_K_M | 138GB | 1 | â³ è®¡åˆ’ä¸­ |
| Q6_K | 188GB | 2 | â³ è®¡åˆ’ä¸­ |
| Q8_0 | 243GB | 3 | â³ è®¡åˆ’ä¸­ |
| BF16 | 457GB | 4 | âŒ å¤±è´¥ |

## æµ‹è¯•æŒ‡æ ‡

| æŒ‡æ ‡ | è¯´æ˜ |
|------|------|
| **Load Time** | æ¨¡å‹åŠ è½½åˆ°å†…å­˜çš„æ—¶é—´ |
| **TTFT** | Time to First Tokenï¼ˆé¦–ä¸ªtokenæ—¶é—´ï¼Œå³prefillé€Ÿåº¦ï¼‰|
| **TPS** | Tokens per Secondï¼ˆç”Ÿæˆé€Ÿåº¦ï¼‰|
| **Peak Memory** | æ¨ç†æœŸé—´çš„æœ€å¤§å†…å­˜ä½¿ç”¨é‡ |
| **Aggregate TPS** | å¤šå¹¶å‘åœºæ™¯ä¸‹çš„æ€»ååé‡ |

## é¡¹ç›®ç»“æ„

```
llm-mac-512/
â”œâ”€â”€ README.md               # è‹±æ–‡ç‰ˆ
â”œâ”€â”€ README.zh-CN.md        # ä¸­æ–‡ç‰ˆ
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ test-plan.md           # è¯¦ç»†æµ‹è¯•è®¡åˆ’
â”‚   â”œâ”€â”€ test-execution-guide.md  # åˆ†æ­¥æ‰§è¡ŒæŒ‡å—
â”‚   â”œâ”€â”€ test-design-summary.md   # æµ‹è¯•è®¾è®¡æ¦‚è§ˆ
â”‚   â”œâ”€â”€ benchmark-results.md     # å®Œæ•´ç»“æœ
â”‚   â””â”€â”€ test-results/          # å•ç‹¬çš„æµ‹è¯•è¾“å‡º
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ benchmark_mlx.py       # MLX æµ‹è¯•è„šæœ¬
â”‚   â”œâ”€â”€ benchmark_llama.py     # llama.cpp æµ‹è¯•è„šæœ¬
â”‚   â”œâ”€â”€ benchmark_batching.py  # Batching/å¹¶å‘æµ‹è¯•
â”‚   â””â”€â”€ utils.py               # å·¥å…·å‡½æ•°
â””â”€â”€ prompts/
    â””â”€â”€ test_prompts.json      # æµ‹è¯•ç”¨ä¾‹
```

## å‘½ä»¤è¡Œé€‰é¡¹

### benchmark_mlx.py

```
--model         æ¨¡å‹åç§°ï¼ˆHuggingFace ä»“åº“ï¼‰
--prompts       æµ‹è¯• prompts JSON æ–‡ä»¶è·¯å¾„
--output-dir    ç»“æœè¾“å‡ºç›®å½•
--max-tokens    è¦†ç›–æ‰€æœ‰æµ‹è¯•çš„æœ€å¤§ token æ•°
--temperature   ç”Ÿæˆæ¸©åº¦ï¼ˆé»˜è®¤: 0.7ï¼‰
--tests         æŒ‡å®šè¦è¿è¡Œçš„æµ‹è¯•ï¼ˆä¾‹å¦‚: short mediumï¼‰
--dry-run       æ£€æŸ¥è®¾ç½®ä½†ä¸è¿è¡Œæµ‹è¯•
```

### benchmark_llama.py

```
--model         GGUF æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼ˆå¿…éœ€ï¼‰
--n-gpu-layers  GPU å±‚æ•°ï¼ˆ-1 è¡¨ç¤ºå…¨éƒ¨ï¼‰
--ctx-size      ä¸Šä¸‹æ–‡å¤§å°ï¼ˆé»˜è®¤: 4096ï¼‰
--threads       çº¿ç¨‹æ•°
--llama-cli     llama-cli å¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„
```

### benchmark_batching.py

```
--model         æ¨¡å‹åç§°ï¼ˆHuggingFace æ ¼å¼ï¼‰
--concurrent    å¹¶å‘è¯·æ±‚æ•°ï¼ˆ1, 2, 4, 8, 16ï¼‰
--tokens        æ¯ä¸ªè¯·æ±‚çš„ token æ•°
--mixed         ä½¿ç”¨æ··åˆå·¥ä½œè´Ÿè½½ï¼ˆ100/500/2000 tokensï¼‰
--temperature   ç”Ÿæˆæ¸©åº¦ï¼ˆé»˜è®¤: 0.7ï¼‰
--use-mlx-lm    ä½¿ç”¨ mlx-lm è€Œä¸æ˜¯ vllm-mlx
```

## ä½¿ç”¨åœºæ™¯æ¨è

| ä½¿ç”¨åœºæ™¯ | æ¨èé…ç½® | ç†ç”± |
|---------|---------|------|
| å•ç”¨æˆ·ï¼Œäº¤äº’å¼ | MLX 4-bit | é€Ÿåº¦æœ€å¿«ï¼Œå†…å­˜å ç”¨ä½ |
| å•ç”¨æˆ·ï¼Œè´¨é‡ä¼˜å…ˆ | MLX 6-bit æˆ– 8-bit | è´¨é‡æ›´å¥½ï¼Œé€Ÿåº¦å¯æ¥å— |
| å¤šç”¨æˆ· API (2-4ç”¨æˆ·) | vllm-mlx 4-bit, batching | é«˜æ•ˆæ‰¹å¤„ç† |
| å¤šç”¨æˆ· API (8+ç”¨æˆ·) | vllm-mlx 4-bit, batching | é«˜ååé‡ |
| å…¼å®¹æ€§ï¼ˆGGUFæ ¼å¼ï¼‰| llama.cpp Q4_K_M | æ ‡å‡†æ ¼å¼ |
| å†…å­˜å—é™ | MLX 4-bit | æœ€ä½å†…å­˜ä½¿ç”¨ |

## æ³¨æ„äº‹é¡¹

- bf16 ç‰ˆæœ¬ï¼ˆ~460GBï¼‰æ¥è¿‘ 512GB é™åˆ¶ï¼›æµ‹è¯•å‰è¯·å…³é—­å…¶ä»–åº”ç”¨
- åœ¨ä¸‹è½½ä¸‹ä¸€ä¸ªç‰ˆæœ¬å‰ï¼Œå…ˆæµ‹è¯•å¹¶è®°å½•å½“å‰ç‰ˆæœ¬ç»“æœï¼ˆèŠ‚çœç£ç›˜ç©ºé—´ï¼‰
- æ¨¡å‹åœ¨é¦–æ¬¡è¿è¡Œæ—¶ä¼šè‡ªåŠ¨ä» HuggingFace ä¸‹è½½
- vllm-mlx éœ€è¦å•ç‹¬å®‰è£…ï¼š`pip install vllm-mlx`

## å‚è€ƒèµ„æ–™

- [MLX éƒ¨ç½²æŒ‡å—](https://github.com/MiniMax-AI/MiniMax-M2.1/blob/main/docs/mlx_deploy_guide.md)
- [Unsloth GGUF ç‰ˆæœ¬](https://huggingface.co/unsloth/MiniMax-M2.1-GGUF)
- [MLX Community æ¨¡å‹](https://huggingface.co/mlx-community)
- [MiniMax å®˜æ–¹æ–°é—»](https://www.minimax.io/news/minimax-m21)
- [vllm-mlx GitHub](https://github.com/waybarrios/vllm-mlx)

## è®¸å¯è¯

MIT
